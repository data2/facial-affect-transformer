import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
import timm
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
from PIL import Image
from tqdm import tqdm
import time
import os
from sklearn.utils.class_weight import compute_class_weight
import torch.nn.functional as F
import math
import argparse
import os
from pathlib import Path


# === å‘½ä»¤è¡Œå‚æ•°è§£æ ===
def get_config():
    parser = argparse.ArgumentParser(description='è¡¨æƒ…è¯†åˆ«è®­ç»ƒè„šæœ¬')

    # è®¾å¤‡é…ç½®
    parser.add_argument('--device', type=str, default='auto',
                        choices=['auto', 'cuda', 'cpu', 'cuda:0', 'cuda:1'],
                        help='è®­ç»ƒè®¾å¤‡: auto(è‡ªåŠ¨é€‰æ‹©), cuda, cpu, cuda:0ç­‰')

    # æ¨¡å‹é…ç½®
    parser.add_argument('--model_size', type=str, default='huge',
                        choices=['tiny', 'small', 'base', 'large', 'huge'],
                        help='æ¨¡å‹å¤§å°')
    parser.add_argument('--batch_size', type=int, default=2,
                        help='batchå¤§å°')

    # æƒé‡åŠ è½½é…ç½®
    parser.add_argument('--weights_dir', type=str, default='./vit_weights',
                        help='é¢„è®­ç»ƒæƒé‡ç›®å½•')
    parser.add_argument('--use_pretrained', action='store_true', default=True,
                        help='ä½¿ç”¨é¢„è®­ç»ƒæƒé‡')
    parser.add_argument('--force_download', action='store_true', default=False,
                        help='å¼ºåˆ¶é‡æ–°ä¸‹è½½æƒé‡')

    # å®‰å…¨æ¨¡å¼é…ç½®
    parser.add_argument('--safe_mode', action='store_true', default=False,
                        help='å¯ç”¨å®‰å…¨æ¨¡å¼')
    parser.add_argument('--memory_limit', type=int, default=12,
                        help='æ˜¾å­˜é™åˆ¶(GB)')
    parser.add_argument('--grad_accum_steps', type=int, default=8,
                        help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--check_interval', type=int, default=300,
                        help='å®‰å…¨æ£€æŸ¥é—´éš”(ç§’)')

    # è®­ç»ƒé…ç½®
    parser.add_argument('--epochs', type=int, default=100,
                        help='è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--lr', type=float, default=-1,
                        help='å­¦ä¹ ç‡ï¼Œ-1ä¸ºè‡ªåŠ¨è®¾ç½®')

    args = parser.parse_args()
    return args


# å…¨å±€é…ç½®
config = get_config()

# === GPUä¼˜åŒ–ï¼šæ··åˆç²¾åº¦è®­ç»ƒ ===
try:
    from torch.cuda.amp import autocast, GradScaler

    AMP_AVAILABLE = True
    print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
except ImportError:
    AMP_AVAILABLE = False
    print("âš ï¸ æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨ï¼Œä½¿ç”¨æ™®é€šè®­ç»ƒ")


# === æ™ºèƒ½æƒé‡åŠ è½½å™¨ - ä¿®å¤ç‰ˆæœ¬ ===
class SmartWeightLoader:
    def __init__(self, weights_dir='./vit_weights', use_pretrained=True, force_download=False):
        self.weights_dir = Path(weights_dir)
        self.use_pretrained = use_pretrained
        self.force_download = force_download
        self.weights_dir.mkdir(exist_ok=True, parents=True)

        # å®Œæ•´çš„æ¨¡å‹é…ç½®è¡¨ - ä¿®å¤ç‰ˆæœ¬
        self.model_configs = {
            'tiny': {
                'name': 'vit_tiny_patch16_224',
                'hf_repo': 'google/vit-tiny-patch16-224',
                'hidden_size': 192,
                'intermediate_size': 768,  # 192 * 4
                'num_hidden_layers': 12,
                'num_attention_heads': 3,
                'patch_size': 16,
                'lr': 5e-5
            },
            'small': {
                'name': 'vit_small_patch16_224',
                'hf_repo': 'google/vit-small-patch16-224',
                'hidden_size': 384,
                'intermediate_size': 1536,  # 384 * 4
                'num_hidden_layers': 12,
                'num_attention_heads': 6,
                'patch_size': 16,
                'lr': 3e-5
            },
            'base': {
                'name': 'vit_base_patch16_224',
                'hf_repo': 'google/vit-base-patch16-224',
                'hidden_size': 768,
                'intermediate_size': 3072,  # 768 * 4
                'num_hidden_layers': 12,
                'num_attention_heads': 12,
                'patch_size': 16,
                'lr': 2e-5
            },
            'large': {
                'name': 'vit_large_patch16_224',
                'hf_repo': 'google/vit-large-patch16-224',
                'hidden_size': 1024,
                'intermediate_size': 4096,  # 1024 * 4
                'num_hidden_layers': 24,
                'num_attention_heads': 16,
                'patch_size': 16,
                'lr': 1e-5
            },
            'huge': {
                'name': 'vit_huge_patch14_224',
                'hf_repo': 'google/vit-huge-patch14-224-in21k',
                'hidden_size': 1280,
                'intermediate_size': 5120,  # å…³é”®ä¿®å¤ï¼šViT-Hugeçš„ä¸­é—´å±‚ç»´åº¦æ˜¯5120ï¼Œä¸æ˜¯3072
                'num_hidden_layers': 32,
                'num_attention_heads': 16,
                'patch_size': 14,
                'lr': 5e-6
            }
        }

    def get_local_weight_path(self, model_size):
        """è·å–æœ¬åœ°æƒé‡æ–‡ä»¶è·¯å¾„"""
        hf_files = [
            self.weights_dir / "pytorch_model.bin",
            self.weights_dir / "model.safetensors"
        ]

        for file_path in hf_files:
            if file_path.exists():
                file_size = file_path.stat().st_size / (1024 ** 3)
                print(f"âœ… æ‰¾åˆ°æƒé‡æ–‡ä»¶: {file_path} ({file_size:.2f} GB)")
                return file_path

        print("âŒ æœªæ‰¾åˆ°æœ¬åœ°æƒé‡æ–‡ä»¶")
        return None

    def create_huggingface_model(self, model_size, num_classes=7):
        """åˆ›å»ºHuggingFaceæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬"""
        from transformers import ViTForImageClassification, ViTConfig
        
        if model_size not in self.model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹å¤§å°: {model_size}")
            
        model_config = self.model_configs[model_size]
        hf_repo = model_config['hf_repo']
        
        print(f"ğŸ”„ åˆ›å»ºHuggingFaceæ¨¡å‹: {hf_repo}")
        print(f"ğŸ¯ å…³é”®é…ç½®éªŒè¯:")
        print(f"  éšè—å±‚ç»´åº¦: {model_config['hidden_size']}")
        print(f"  ä¸­é—´å±‚ç»´åº¦: {model_config['intermediate_size']}")  # å…³é”®ä¿®å¤
        print(f"  å±‚æ•°: {model_config['num_hidden_layers']}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {model_config['num_attention_heads']}")
        print(f"  Patchå¤§å°: {model_config['patch_size']}")

        try:
            # æ˜ç¡®æŒ‡å®šæ‰€æœ‰å…³é”®å‚æ•°
            vit_config = ViTConfig(
                image_size=224,
                patch_size=model_config['patch_size'],
                num_channels=3,
                hidden_size=model_config['hidden_size'],
                num_hidden_layers=model_config['num_hidden_layers'],
                num_attention_heads=model_config['num_attention_heads'],
                intermediate_size=model_config['intermediate_size'],  # å…³é”®ä¿®å¤ï¼
                hidden_dropout_prob=0.0,
                attention_probs_dropout_prob=0.0,
                num_labels=num_classes
            )
            
            # é¦–å…ˆå°è¯•ä»æœ¬åœ°åŠ è½½
            local_path = self.get_local_weight_path(model_size)
            if local_path and not self.force_download:
                print(f"ğŸ“ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½: {local_path}")
                model = ViTForImageClassification.from_pretrained(
                    str(local_path.parent),
                    config=vit_config,
                    ignore_mismatched_sizes=True
                )
            else:
                # ä»HuggingFace Hubä¸‹è½½
                print(f"ğŸŒ ä»HuggingFace Hubä¸‹è½½: {hf_repo}")
                model = ViTForImageClassification.from_pretrained(
                    hf_repo,
                    config=vit_config,
                    ignore_mismatched_sizes=True
                )
                
                # ä¿å­˜åˆ°æœ¬åœ°
                if local_path:
                    model.save_pretrained(self.weights_dir)
                    print(f"ğŸ’¾ æƒé‡å·²ä¿å­˜åˆ°: {self.weights_dir}")

            # éªŒè¯æœ€ç»ˆé…ç½®
            actual_config = model.config
            print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            print(f"ğŸ” æœ€ç»ˆé…ç½®éªŒè¯:")
            print(f"  å®é™…éšè—å±‚: {actual_config.hidden_size}")
            print(f"  å®é™…ä¸­é—´å±‚: {actual_config.intermediate_size}")
            print(f"  å®é™…å±‚æ•°: {actual_config.num_hidden_layers}")
            
            # æ£€æŸ¥ç»´åº¦åŒ¹é…
            if (actual_config.hidden_size == model_config['hidden_size'] and 
                actual_config.intermediate_size == model_config['intermediate_size']):
                print("ğŸ‰ æ‰€æœ‰ç»´åº¦åŒ¹é…æˆåŠŸ!")
            else:
                print(f"âš ï¸ è­¦å‘Š: ç»´åº¦ä¸åŒ¹é…!")
                print(f"  æœŸæœ›éšè—å±‚: {model_config['hidden_size']}, å®é™…: {actual_config.hidden_size}")
                print(f"  æœŸæœ›ä¸­é—´å±‚: {model_config['intermediate_size']}, å®é™…: {actual_config.intermediate_size}")

            return model

        except Exception as e:
            print(f"âŒ HuggingFaceæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            # å›é€€æ–¹æ¡ˆï¼šåˆ›å»ºéšæœºåˆå§‹åŒ–çš„æ­£ç¡®æ¨¡å‹
            print("ğŸ”„ åˆ›å»ºéšæœºåˆå§‹åŒ–çš„æ­£ç¡®æ¨¡å‹...")
            model = ViTForImageClassification(vit_config)
            print("âœ… éšæœºåˆå§‹åŒ–æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            return model

    def diagnostic_check(self, model, train_loader, device):
        """è¯Šæ–­æ¨¡å‹å’Œæ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ” æ¨¡å‹è¯Šæ–­æ£€æŸ¥")
        print("="*60)
        
        # 1. æ£€æŸ¥æ¨¡å‹é…ç½®
        print("ğŸ“Š æ¨¡å‹é…ç½®:")
        print(f"  éšè—å±‚ç»´åº¦: {model.config.hidden_size}")
        print(f"  ä¸­é—´å±‚ç»´åº¦: {model.config.intermediate_size}")  # å…³é”®ï¼
        print(f"  å±‚æ•°: {model.config.num_hidden_layers}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {model.config.num_attention_heads}")
        
        # 2. æ£€æŸ¥å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ğŸ“ˆ å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # 3. æ£€æŸ¥ä¸€ä¸ªbatchçš„æ•°æ®
        model.eval()
        with torch.no_grad():
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                
                print(f"ğŸ” è¾“å…¥æ•°æ®æ£€æŸ¥:")
                print(f"  è¾“å…¥å½¢çŠ¶: {images.shape}")
                print(f"  æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
                print(f"  åƒç´ èŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")
                print(f"  å‡å€¼: {images.mean():.3f}, æ ‡å‡†å·®: {images.std():.3f}")
                
                # å‰å‘ä¼ æ’­æµ‹è¯•
                outputs = model(pixel_values=images)
                logits = outputs.logits
                    
                print(f"ğŸ¯ è¾“å‡ºæ£€æŸ¥:")
                print(f"  è¾“å‡ºå½¢çŠ¶: {logits.shape}")
                print(f"  è¾“å‡ºèŒƒå›´: [{logits.min():.3f}, {logits.max():.3f}]")
                
                # è®¡ç®—åˆå§‹å‡†ç¡®ç‡
                _, predicted = torch.max(logits, 1)
                initial_acc = (predicted == labels).float().mean()
                print(f"ğŸ¯ åˆå§‹å‡†ç¡®ç‡: {initial_acc.item()*100:.2f}%")
                
                break
        
        return initial_acc.item()


# === å®‰å…¨æ¨¡å¼æ£€æŸ¥å™¨ ===
class SafeModeChecker:
    def __init__(self, memory_limit_gb=18, check_interval=300):
        self.memory_limit_gb = memory_limit_gb
        self.check_interval = check_interval
        self.last_check_time = 0

    def is_safe_to_train(self):
        """å®‰å…¨æ£€æŸ¥é€»è¾‘"""
        current_time = time.time()
        if current_time - self.last_check_time < self.check_interval:
            return True

        if not torch.cuda.is_available():
            return True

        try:
            used_memory = torch.cuda.memory_allocated() / 1024 ** 3
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
            free_memory = total_memory - used_memory

            if used_memory > self.memory_limit_gb:
                print(f"âš ï¸ GPUæ˜¾å­˜è¶…é™: {used_memory:.1f}GB > {self.memory_limit_gb}GBé™åˆ¶")
                return False

            safety_margin = 2.0
            if free_memory < safety_margin:
                print(f"âš ï¸ GPUæ˜¾å­˜ä¸è¶³: å‰©ä½™ {free_memory:.1f}GB < {safety_margin}GBå®‰å…¨è¾¹é™…")
                return False

            self.last_check_time = current_time
            return True

        except Exception as e:
            print(f"âŒ å®‰å…¨æ£€æŸ¥å¤±è´¥: {e}")
            return True


# === æ—©åœæœºåˆ¶ ===
class AdaptiveEarlyStopping:
    def __init__(self, patience=10, delta=0.001, warmup_epochs=5):
        self.patience = patience
        self.delta = delta
        self.warmup_epochs = warmup_epochs
        self.best_acc = 0
        self.counter = 0
        self.early_stop = False
        self.best_weights = None

    def __call__(self, val_acc, epoch, model):
        if epoch < self.warmup_epochs:
            return False

        if val_acc > self.best_acc + self.delta:
            self.best_acc = val_acc
            self.counter = 0
            self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                if self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                    print("âœ… å·²æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡")
        return self.early_stop


# === æ ‡ç­¾å¹³æ»‘ ===
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        confidence = 1.0 - self.smoothing
        logprobs = F.log_softmax(pred, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()


# === æ¨¡å‹EMA ===
class ModelEMA:
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        self.backup = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


# === å­¦ä¹ ç‡è°ƒåº¦ ===
def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


# === è®¾å¤‡è®¾ç½® ===
def setup_device():
    """è®¾ç½®GPUè®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)

        print(f"âœ… å‘ç° {gpu_count} ä¸ªGPU:")
        for i in range(gpu_count):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024 ** 3:.1f} GB")

        if gpu_count > 1:
            print(f"ğŸ¯ ä½¿ç”¨å¤šGPUè®­ç»ƒ: {gpu_count}ä¸ªGPU")
        else:
            print(f"ğŸ¯ ä½¿ç”¨GPU: {device_name}")

        torch.cuda.empty_cache()
        return device
    else:
        print("âš ï¸ æœªå‘ç°GPUï¼Œä½¿ç”¨CPU")
        return torch.device('cpu')


def set_gpu_memory_limit(limit_gb=12):
    """è®¾ç½®GPUæ˜¾å­˜é™åˆ¶"""
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory
        limit_bytes = int(limit_gb * 1024 ** 3)

        torch.cuda.set_per_process_memory_fraction(limit_bytes / total_memory)
        print(f"âœ… è®¾ç½®æ˜¾å­˜é™åˆ¶: {limit_gb}GB / {total_memory / 1024 ** 3:.1f}GB")


def setup_multi_gpu(model, device):
    """è®¾ç½®å¤šGPUè®­ç»ƒ"""
    if torch.cuda.device_count() > 1:
        print(f"ğŸš€ ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œæ•°æ®å¹¶è¡Œè®­ç»ƒ")
        model = nn.DataParallel(model)

    model = model.to(device)
    return model


def main():
    print("=" * 60)
    print("ğŸ­ è¡¨æƒ…è¯†åˆ«è®­ç»ƒè„šæœ¬ - ä¿®å¤ç‰ˆæœ¬")
    print("=" * 60)

    # æ‰“å°é…ç½®
    print("ğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  æ¨¡å‹å¤§å°: {config.model_size}")
    print(f"  Batch Size: {config.batch_size}")
    print(f"  ä½¿ç”¨é¢„è®­ç»ƒ: {'æ˜¯' if config.use_pretrained else 'å¦'}")

    # === åˆå§‹åŒ–æƒé‡åŠ è½½å™¨ ===
    weight_loader = SmartWeightLoader(
        weights_dir=config.weights_dir,
        use_pretrained=config.use_pretrained,
        force_download=config.force_download
    )

    # === è®¾å¤‡è®¾ç½® ===
    device = setup_device()

    # === è®¾ç½®æ˜¾å­˜é™åˆ¶ ===
    if config.safe_mode and torch.cuda.is_available():
        set_gpu_memory_limit(config.memory_limit)

    # === æ•°æ®é¢„å¤„ç† ===
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ•°æ®é›†
    train_dir = './data/train'
    test_dir = './data/test'

    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    val_dataset = datasets.ImageFolder(test_dir, transform=val_transform)

    num_classes = len(train_dataset.classes)
    print(f"ğŸ¯ æ•°æ®é›†: {num_classes}ä¸ªç±»åˆ«")

    # === åˆ›å»ºæ¨¡å‹ ===
    print(f"\nğŸ”„ åˆ›å»ºæ¨¡å‹: {config.model_size}")
    model = weight_loader.create_huggingface_model(config.model_size, num_classes=num_classes)
    model = model.to(device)

    # === è¯Šæ–­æ£€æŸ¥ ===
    batch_size = config.batch_size
    num_workers = min(4, os.cpu_count())
    train_loader_diag = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                                  num_workers=num_workers, pin_memory=True)
    
    initial_acc = weight_loader.diagnostic_check(model, train_loader_diag, device)
    
    # è¯„ä¼°åˆå§‹å‡†ç¡®ç‡
    print(f"\nğŸ¯ åˆå§‹å‡†ç¡®ç‡è¯„ä¼°:")
    if initial_acc > 0.7:
        print(f"âœ… ä¼˜ç§€! åˆå§‹å‡†ç¡®ç‡: {initial_acc*100:.2f}% (é¢„æœŸèŒƒå›´: 70-85%)")
    elif initial_acc > 0.5:
        print(f"âš ï¸ ä¸€èˆ¬! åˆå§‹å‡†ç¡®ç‡: {initial_acc*100:.2f}% (ä½äºé¢„æœŸ)")
    else:
        print(f"âŒ è¾ƒå·®! åˆå§‹å‡†ç¡®ç‡: {initial_acc*100:.2f}% (å¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜)")

    # === è®¾ç½®å­¦ä¹ ç‡ ===
    learning_rate = config.lr if config.lr > 0 else weight_loader.model_configs[config.model_size]['lr']
    print(f"ğŸ¯ å­¦ä¹ ç‡: {learning_rate:.2e}")

    # === ä¼˜åŒ–å™¨ ===
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=0.01
    )

    # === æŸå¤±å‡½æ•° ===
    simple_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
    ])
    train_dataset_simple = datasets.ImageFolder(train_dir, transform=simple_transform)
    train_labels = [label for _, label in train_dataset_simple]
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)

    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # === æ•°æ®åŠ è½½å™¨ ===
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                             num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                           num_workers=num_workers, pin_memory=True)

    print(f"ğŸ“Š æ•°æ®åŠ è½½å™¨:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    print(f"  Batch Size: {batch_size}")
    print(f"  æ•°æ®åŠ è½½è¿›ç¨‹: {num_workers}")

    # === å­¦ä¹ ç‡è°ƒåº¦ ===
    num_training_steps = len(train_loader) * config.epochs // config.grad_accum_steps
    num_warmup_steps = len(train_loader) * 5 // config.grad_accum_steps
    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

    # === åˆå§‹åŒ–è®­ç»ƒç»„ä»¶ ===
    ema = ModelEMA(model)
    early_stopping = AdaptiveEarlyStopping(patience=10)
    safe_checker = SafeModeChecker() if config.safe_mode else None
    scaler = GradScaler() if AMP_AVAILABLE else None

    # === è®­ç»ƒå‡½æ•° ===
    def train_epoch(model, dataloader, criterion, optimizer, scaler, accumulation_steps=1):
        model.train()
        total_loss = 0
        optimizer.zero_grad()

        for i, (images, labels) in enumerate(tqdm(dataloader, desc="è®­ç»ƒä¸­")):
            # å®‰å…¨æ£€æŸ¥
            if safe_checker and not safe_checker.is_safe_to_train():
                time.sleep(10)
                continue

            images, labels = images.to(device), labels.to(device)

            if AMP_AVAILABLE:
                with autocast():
                    outputs = model(pixel_values=images, labels=labels)
                    loss = outputs.loss / accumulation_steps

                scaler.scale(loss).backward()
            else:
                outputs = model(pixel_values=images, labels=labels)
                loss = outputs.loss / accumulation_steps
                loss.backward()

            total_loss += loss.item() * accumulation_steps

            if (i + 1) % accumulation_steps == 0:
                if AMP_AVAILABLE:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                scheduler.step()
                ema.update()

        return total_loss / len(dataloader)

    def evaluate(model, dataloader):
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in tqdm(dataloader, desc="éªŒè¯ä¸­"):
                images, labels = images.to(device), labels.to(device)

                if AMP_AVAILABLE:
                    with autocast():
                        outputs = model(pixel_values=images)
                        logits = outputs.logits
                else:
                    outputs = model(pixel_values=images)
                    logits = outputs.logits

                _, predicted = torch.max(logits, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        return correct / total

    # === è®­ç»ƒå¾ªç¯ ===
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ")
    best_acc = 0.0
    training_history = {'train_loss': [], 'val_acc': []}

    for epoch in range(config.epochs):
        print(f"\nğŸ“Š Epoch [{epoch+1}/{config.epochs}]")

        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, config.grad_accum_steps)

        # éªŒè¯
        ema.apply_shadow()
        val_acc = evaluate(model, val_loader)
        ema.restore()

        training_history['train_loss'].append(train_loss)
        training_history['val_acc'].append(val_acc)

        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.2f}%")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")

        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        if val_acc > best_acc:
            best_acc = val_acc
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            model_to_save = model.module if hasattr(model, 'module') else model
            torch.save({
                'model_state_dict': model_to_save.state_dict(),
                'best_acc': best_acc,
                'epoch': epoch,
                'config': vars(config)
            }, f'best_model_{config.model_size}.pth')
            print(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}%")

        # æ—©åœæ£€æŸ¥
        if early_stopping(val_acc, epoch, model):
            print("ğŸ›‘ æ—©åœè§¦å‘")
            break

    print(f"\nğŸ è®­ç»ƒå®Œæˆ! æœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}%")


if __name__ == '__main__':
    main()
