ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä¿®å¤ç‰ˆä»£ç ï¼š
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms, datasets
import timm
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
from PIL import Image
from tqdm import tqdm
import time
import os
import math
from sklearn.utils.class_weight import compute_class_weight


# === é«˜çº§ä¼˜åŒ–ç»„ä»¶ ===
class EarlyStopping:
    def __init__(self, patience=10, delta=0.001, min_epochs=20):
        self.patience = patience
        self.delta = delta
        self.min_epochs = min_epochs
        self.best_acc = 0
        self.counter = 0
        self.early_stop = False

    def __call__(self, val_acc, epoch):
        if epoch < self.min_epochs:
            return False
            
        if val_acc > self.best_acc + self.delta:
            self.best_acc = val_acc
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        return self.early_stop


class LabelSmoothingCrossEntropy(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°"""
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, x, target):
        confidence = 1.0 - self.smoothing
        logprobs = F.log_softmax(x, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()


class FocalLoss(nn.Module):
    """Focal Lossç”¨äºå¤„ç†ç±»åˆ«ä¸å‡è¡¡"""
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


def cutmix_data(x, y, alpha=1.0):
    """CutMixæ•°æ®å¢å¼º"""
    if alpha <= 0:
        return x, y, y, 1.0
        
    # ç”Ÿæˆlambdaå€¼
    lam = np.random.beta(alpha, alpha)
    
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    
    # ç”Ÿæˆè£å‰ªåŒºåŸŸ
    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)
    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
    
    # è°ƒæ•´lambda
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))
    y_a, y_b = y, y[index]
    
    return x, y_a, y_b, lam


def rand_bbox(size, lam):
    """ç”Ÿæˆéšæœºè£å‰ªåŒºåŸŸ"""
    W = size[2]
    H = size[3]
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)

    # å‡åŒ€åˆ†å¸ƒ
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2


def cutmix_criterion(criterion, pred, y_a, y_b, lam):
    """CutMixæŸå¤±å‡½æ•°"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def mixup_data(x, y, alpha=0.2):
    """MixUpæ•°æ®å¢å¼º"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """MixUpæŸå¤±å‡½æ•°"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):
    """å¸¦çƒ­èº«çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


class AdaptiveDataAugmentation:
    """è‡ªé€‚åº”æ•°æ®å¢å¼º - åŸºäºç¬¬9ä¸ªepochç»“æœä¼˜åŒ–"""
    def __init__(self, cutmix_prob=0.5, mixup_prob=0.3):
        self.cutmix_prob = cutmix_prob
        self.mixup_prob = mixup_prob
        self.epoch = 0
        
    def update_probabilities(self, class_accuracies):
        """æ ¹æ®ç¬¬9ä¸ªepochç»“æœåŠ¨æ€è°ƒæ•´å¢å¼ºæ¦‚ç‡"""
        if class_accuracies is None or len(class_accuracies) < 3:
            return
            
        disgust_acc = class_accuracies[1]
        fear_acc = class_accuracies[2]
        
        # disgustæ¥è¿‘50%ï¼Œéœ€è¦çªç ´æ€§å¢å¼º
        if 0.45 < disgust_acc < 0.55:
            self.cutmix_prob = min(0.7, self.cutmix_prob + 0.1)
            self.mixup_prob = min(0.5, self.mixup_prob + 0.1)
        # fearå‡†ç¡®ç‡ä¸‹é™ï¼Œéœ€è¦æ¢å¤
        elif fear_acc < 0.4:
            self.cutmix_prob = min(0.8, self.cutmix_prob + 0.2)
            self.mixup_prob = min(0.6, self.mixup_prob + 0.2)
            
    def apply_augmentation(self, images, labels, epoch):
        """åº”ç”¨è‡ªé€‚åº”å¢å¼º"""
        self.epoch = epoch
        
        # åŸºäºç¬¬9ä¸ªepochç»“æœä¼˜åŒ–å¢å¼ºç­–ç•¥
        if epoch >= 9:  # ç¬¬10ä¸ªepochå¼€å§‹é’ˆå¯¹æ€§å¢å¼º
            # 60%æ¦‚ç‡ä½¿ç”¨CutMix (é’ˆå¯¹disgustå’Œfear)
            if np.random.rand() < self.cutmix_prob:
                images, targets_a, targets_b, lam = cutmix_data(images, labels, alpha=1.0)
                return images, targets_a, targets_b, lam, 'cutmix'
            
            # 40%æ¦‚ç‡ä½¿ç”¨MixUp
            elif np.random.rand() < self.mixup_prob:
                images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.3)
                return images, targets_a, targets_b, lam, 'mixup'
        else:
            # å‰9ä¸ªepochä½¿ç”¨æ ‡å‡†å¢å¼º
            if np.random.rand() < self.cutmix_prob:
                images, targets_a, targets_b, lam = cutmix_data(images, labels, alpha=1.0)
                return images, targets_a, targets_b, lam, 'cutmix'
            elif np.random.rand() < self.mixup_prob:
                images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.2)
                return images, targets_a, targets_b, lam, 'mixup'
        
        # ä¸ä½¿ç”¨å¢å¼º
        return images, labels, labels, 1.0, 'none'


class SmartWeightAdjustment:
    """æ™ºèƒ½æƒé‡è°ƒæ•´ - ä¿®å¤ç‰ˆ"""
    def __init__(self, base_weights):
        self.base_weights = base_weights.clone()  # ä¿®å¤ï¼šä½¿ç”¨clone()è€Œä¸æ˜¯copy()
        
    def update_weights(self, current_acc, previous_acc=None):
        """åŸºäºå½“å‰å‡†ç¡®ç‡è°ƒæ•´æƒé‡"""
        new_weights = self.base_weights.clone()  # ä¿®å¤ï¼šä½¿ç”¨clone()
        
        if current_acc is None or len(current_acc) < 3:
            return new_weights
            
        # åŸºäºç¬¬9ä¸ªepochç»“æœä¼˜åŒ–æƒé‡ç­–ç•¥
        # disgust (49.5% â†’ ç›®æ ‡52%)
        if current_acc[1] < 0.5:
            new_weights[1] *= 2.5  # å¤§å¹…å¢åŠ æƒé‡åŠ©åŠ›çªç ´
        
        # fear (39.9% â†’ ç›®æ ‡45%)
        if current_acc[2] < 0.45:
            new_weights[2] *= 3.0  # æœ€å¤§æƒé‡
        
        # sad (57.2% â†’ ç›®æ ‡60%)
        if current_acc[5] < 0.6:
            new_weights[5] *= 1.8
            
        # surprise (85.7% â†’ ç»´æŒ)
        if current_acc[6] > 0.85:
            new_weights[6] *= 0.7  # é™ä½æƒé‡
        
        return new_weights


def main():
    print("å¼€å§‹è¿›å…¥è®­ç»ƒ - ç¬¬10ä¸ªepoché’ˆå¯¹æ€§ä¼˜åŒ–ç‰ˆæœ¬ (ä¿®å¤ç‰ˆ)")
    print("=" * 60)

    # === è®¾å¤‡è®¾ç½® ===
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
        torch.cuda.empty_cache()
    else:
        device = torch.device('cpu')
        print("âš ï¸ æœªå‘ç°GPUï¼Œä½¿ç”¨CPU")

    # === è¶…å‚æ•°é…ç½® ===
    config = {
        'model_size': 'base',
        'batch_size': 16,
        'num_epochs': 100,
        'learning_rate': 4.5e-5,
        'weight_decay': 0.05,
        'cutmix_alpha': 1.0,
        'label_smoothing': 0.1,
        'drop_rate': 0.2,
        'grad_accum_steps': 2,
        'warmup_epochs': 10,
        'current_epoch': 9,
        'best_acc': 0.6945,
    }
    
    MODEL_SIZE = config['model_size']
    print(f"ğŸ¯ ç›®æ ‡: 80%éªŒè¯å‡†ç¡®ç‡ | æ¨¡å‹: ViT-{MODEL_SIZE.capitalize()}")
    print(f"ğŸ“Š ç¬¬9ä¸ªepochç»“æœ: 69.45%å‡†ç¡®ç‡")
    print(f"ğŸ” å…³é”®é—®é¢˜: disguståœæ»åœ¨49.5%, fearä¸‹é™è‡³39.9%")

    # === 1. æ•°æ®é¢„å¤„ç† ===
    print("\nğŸ”„ é…ç½®é’ˆå¯¹æ€§æ•°æ®å¢å¼º...")
    
    # è®­ç»ƒæ—¶ä½¿ç”¨çš„å¢å¼ºtransform
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        
        # ç©ºé—´å˜æ¢
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        
        # é¢œè‰²å˜æ¢
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1),
        transforms.GaussianBlur(kernel_size=7, sigma=(0.1, 3.0)),
        
        # è½¬æ¢ä¸ºtensor
        transforms.ToTensor(),
        
        # å½’ä¸€åŒ–
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        
        # tensorä¸Šçš„å˜æ¢
        transforms.RandomErasing(p=0.4, scale=(0.02, 0.2), ratio=(0.3, 3.3)),
    ])

    # éªŒè¯é›†transform
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # ç®€å•transformç”¨äºè·å–æ ‡ç­¾
    simple_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
    ])

    # === 2. åŠ è½½æ•°æ®é›† ===
    print("ğŸ“ åŠ è½½æ•°æ®é›†...")
    train_dir = './data/train'
    test_dir = './data/test'

    if not os.path.exists(train_dir):
        raise FileNotFoundError(f"âŒ è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {train_dir}")
    if not os.path.exists(test_dir):
        raise FileNotFoundError(f"âŒ æµ‹è¯•æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {test_dir}")

    train_dataset_simple = datasets.ImageFolder(train_dir, transform=simple_transform)
    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    val_dataset = datasets.ImageFolder(test_dir, transform=val_transform)

    # è·å–è®­ç»ƒæ ‡ç­¾
    train_labels = [label for _, label in train_dataset_simple]

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    print(f"  ç±»åˆ«æ•°é‡: {len(train_dataset.classes)}")
    print(f"  ç±»åˆ«åç§°: {train_dataset.classes}")

    # æ•°æ®åŠ è½½å™¨
    num_workers = min(8, os.cpu_count())
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,
                              num_workers=num_workers, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False,
                            num_workers=num_workers, pin_memory=True)

    print(f"  Batch Size: {config['batch_size']} (ç´¯ç§¯æ­¥æ•°: {config['grad_accum_steps']})")

    # === 3. åŠ¨æ€ç±»åˆ«æƒé‡ ===
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    
    # åŸºäºç¬¬9ä¸ªepochç»“æœè°ƒæ•´æƒé‡
    # ç¬¬9ä¸ªepochå„ç±»åˆ«å‡†ç¡®ç‡: [0.652, 0.495, 0.399, 0.887, 0.727, 0.572, 0.857]
    weight_adjustments = {
        0: 1.2,  # angry: 65.2% â†’ ä¿æŒ
        1: 2.5,  # disgust: 49.5% â†’ å¤§å¹…å¢åŠ æƒé‡åŠ©åŠ›çªç ´50%
        2: 3.0,  # fear: 39.9% â†’ æœ€å¤§æƒé‡é‡ç‚¹æ¢å¤
        3: 0.8,  # happy: 88.7% â†’ é™ä½æƒé‡
        4: 1.0,  # neutral: 72.7% â†’ ä¿æŒ
        5: 1.8,  # sad: 57.2% â†’ å¢åŠ æƒé‡åŠ©åŠ›çªç ´60%
        6: 0.7,  # surprise: 85.7% â†’ é™ä½æƒé‡
    }
    
    for i, adjustment in weight_adjustments.items():
        if i < len(class_weights):
            class_weights[i] *= adjustment
    
    print("ğŸ“ˆ é’ˆå¯¹æ€§è°ƒæ•´åçš„ç±»åˆ«æƒé‡:")
    for i, cls_name in enumerate(val_dataset.classes):
        print(f"  {cls_name}: {class_weights[i].cpu().numpy():.3f}")

    # === 4. åˆ›å»ºæ¨¡å‹ ===
    def create_optimized_model(model_size='base', num_classes=7, weights_dir='./weights'):
        """åˆ›å»ºä¼˜åŒ–æ¨¡å‹"""
        model_configs = {
            'base': {
                'name': 'vit_base_patch16_224',
                'local_file': 'vit_base_patch16_224.pth',
            }
        }

        config = model_configs[model_size]
        local_weight_path = os.path.join(weights_dir, config['local_file'])

        if not os.path.exists(local_weight_path):
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {local_weight_path}")

        print(f"ğŸ”„ ä»æœ¬åœ°åŠ è½½é¢„è®­ç»ƒæƒé‡: {local_weight_path}")

        # åˆ›å»ºæ¨¡å‹
        model = timm.create_model(config['name'], pretrained=False, num_classes=num_classes)

        try:
            checkpoint = torch.load(local_weight_path, map_location='cpu')
            state_dict = checkpoint
            
            if 'model_state_dict' in state_dict:
                state_dict = state_dict['model_state_dict']
            elif 'state_dict' in state_dict:
                state_dict = state_dict['state_dict']
            elif 'model' in state_dict:
                state_dict = state_dict['model']

            # è¿‡æ»¤åˆ†ç±»å¤´æƒé‡
            filtered_state_dict = {}
            for key, value in state_dict.items():
                if not key.startswith('head.') and not key.startswith('fc.'):
                    filtered_state_dict[key] = value

            # åŠ è½½æƒé‡
            missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
            
            print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
            print(f"  ç¼ºå¤±çš„é”®: {len(missing_keys)}ä¸ª, æ„å¤–çš„é”®: {len(unexpected_keys)}ä¸ª")

        except Exception as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹...")

        return model

    # åˆ›å»ºæ¨¡å‹
    num_classes = 7
    model = create_optimized_model(MODEL_SIZE, num_classes=num_classes)
    model = model.to(device)
    print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")

    # === 5. ä¼˜åŒ–å™¨é…ç½® ===
    # è§£å†»æ›´å¤šå±‚è¿›è¡Œç²¾ç»†å¾®è°ƒ
    for name, param in model.named_parameters():
        if 'blocks' in name and int(name.split('.')[1]) >= 10:  # æœ€å2å±‚
            param.requires_grad = True
        if 'head' in name:  # åˆ†ç±»å¤´å§‹ç»ˆè®­ç»ƒ
            param.requires_grad = True

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦
    num_training_steps = len(train_loader) * config['num_epochs'] // config['grad_accum_steps']
    num_warmup_steps = len(train_loader) * config['warmup_epochs'] // config['grad_accum_steps']
    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

    # è‡ªé€‚åº”æŸå¤±å‡½æ•°
    class AdaptiveCriterion:
        def __init__(self, class_weights, label_smoothing=0.1):
            self.class_weights = class_weights
            self.label_smoothing = label_smoothing
            self.ce_loss = LabelSmoothingCrossEntropy(smoothing=label_smoothing)
            self.focal_loss = FocalLoss(gamma=2)
            
        def __call__(self, outputs, targets, augmentation_type='none'):
            # åŸºç¡€æŸå¤±
            base_loss = self.ce_loss(outputs, targets)
            
            # ä¸ºå›°éš¾ç±»åˆ«æ·»åŠ Focal Loss
            disgust_mask = targets == 1
            fear_mask = targets == 2
            sad_mask = targets == 5
            
            if disgust_mask.any() or fear_mask.any() or sad_mask.any():
                focal_weight = 0.3  # Focal Lossæƒé‡
                focal_component = self.focal_loss(outputs, targets)
                total_loss = (1 - focal_weight) * base_loss + focal_weight * focal_component
            else:
                total_loss = base_loss
                
            return total_loss

    criterion = AdaptiveCriterion(class_weights, label_smoothing=config['label_smoothing'])

    # æ—©åœæœºåˆ¶
    early_stopping = EarlyStopping(patience=12, delta=0.002, min_epochs=20)

    # è‡ªé€‚åº”æ•°æ®å¢å¼º
    adaptive_aug = AdaptiveDataAugmentation(cutmix_prob=0.6, mixup_prob=0.4)
    weight_adjuster = SmartWeightAdjustment(class_weights)

    print(f"ğŸ¯ ç¬¬10ä¸ªepochä¼˜åŒ–é…ç½®:")
    print(f"  å­¦ä¹ ç‡: {config['learning_rate']:.1e} (ä¿æŒ)")
    print(f"  CutMixæ¦‚ç‡: {adaptive_aug.cutmix_prob} (å¢åŠ )")
    print(f"  MixUpæ¦‚ç‡: {adaptive_aug.mixup_prob} (å¢åŠ )")
    print(f"  é’ˆå¯¹æ€§æƒé‡è°ƒæ•´: å·²å¯ç”¨")

    # === 6. è®­ç»ƒå‡½æ•° ===
    def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, 
                   grad_accum_steps=2, epoch=0, previous_class_acc=None):
        """é’ˆå¯¹æ€§è®­ç»ƒå‡½æ•°"""
        model.train()
        total_loss = 0
        optimizer.zero_grad()
        
        # åŸºäºç¬¬9ä¸ªepochç»“æœæ›´æ–°å¢å¼ºæ¦‚ç‡
        if previous_class_acc is not None:
            adaptive_aug.update_probabilities(previous_class_acc)
            
            # ç¬¬10ä¸ªepochå¼€å§‹ä½¿ç”¨æ™ºèƒ½æƒé‡è°ƒæ•´
            if epoch >= 9:
                nonlocal class_weights
                class_weights = weight_adjuster.update_weights(previous_class_acc, previous_class_acc)
                criterion.class_weights = class_weights

        for i, (images, labels) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}")):
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            # åº”ç”¨è‡ªé€‚åº”å¢å¼º
            aug_images, targets_a, targets_b, lam, aug_type = adaptive_aug.apply_augmentation(images, labels, epoch)

            outputs = model(aug_images)
            
            # æ ¹æ®å¢å¼ºç±»å‹è®¡ç®—æŸå¤±
            if aug_type == 'cutmix':
                loss = cutmix_criterion(criterion, outputs, targets_a, targets_b, lam)
            elif aug_type == 'mixup':
                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
            else:
                loss = criterion(outputs, labels, aug_type)

            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / grad_accum_steps
            loss.backward()

            if (i + 1) % grad_accum_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

            total_loss += loss.item() * grad_accum_steps

        return total_loss / len(train_loader)

    def evaluate(model, dataloader, criterion):
        """éªŒè¯å‡½æ•°"""
        model.eval()
        correct = 0
        total = 0
        total_loss = 0
        all_preds = []
        all_targets = []
        class_correct = [0] * 7
        class_total = [0] * 7

        with torch.no_grad():
            for images, labels in tqdm(dataloader, desc="éªŒè¯ä¸­", leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                total_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
                for i in range(labels.size(0)):
                    label = labels[i]
                    class_correct[label] += (predicted[i] == label).item()
                    class_total[label] += 1
                
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(labels.cpu().numpy())

        acc = correct / total
        avg_loss = total_loss / len(dataloader)
        
        # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
        class_acc = []
        for i in range(7):
            if class_total[i] > 0:
                class_acc.append(class_correct[i] / class_total[i])
            else:
                class_acc.append(0.0)
                
        return acc, avg_loss, class_acc

    # === 7. åŠ è½½æ£€æŸ¥ç‚¹ ===
    def load_checkpoint(model, optimizer, scheduler, filepath):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(filepath):
            print(f"ğŸ”„ åŠ è½½æ£€æŸ¥ç‚¹: {filepath}")
            checkpoint = torch.load(filepath, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_acc = checkpoint['best_acc']
            return start_epoch, best_acc
        return 0, 0.0

    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint_path = 'best_vit_base_adaptive.pth'
    start_epoch, best_acc = load_checkpoint(model, optimizer, scheduler, checkpoint_path)
    if start_epoch > 0:
        print(f"âœ… ä»ç¬¬{start_epoch}ä¸ªepochæ¢å¤è®­ç»ƒï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}%")
    else:
        best_acc = config['best_acc']
        start_epoch = config['current_epoch']
        print(f"ğŸ”„ ä»ç¬¬{start_epoch}ä¸ªepochå¼€å§‹è®­ç»ƒ")

    # === 8. è®­ç»ƒå¾ªç¯ ===
    print(f"\nğŸš€ å¼€å§‹ç¬¬10ä¸ªepoché’ˆå¯¹æ€§ä¼˜åŒ–è®­ç»ƒ (ä¿®å¤ç‰ˆ)")
    print("=" * 60)

    training_history = {
        'train_loss': [], 'val_acc': [], 'val_loss': [], 'learning_rates': [], 'class_acc': []
    }

    # è®°å½•ç¬¬9ä¸ªepochçš„ç»“æœ
    training_history['val_acc'].append(0.6945)  # ç¬¬9ä¸ªepochç»“æœ
    training_history['class_acc'].append([0.652, 0.495, 0.399, 0.887, 0.727, 0.572, 0.857])

    for epoch in range(start_epoch, config['num_epochs']):
        epoch_start = time.time()
        print(f"\nğŸ“Š Epoch [{epoch+1}/{config['num_epochs']}]")

        # ä½¿ç”¨ä¸Šä¸€è½®çš„ç±»åˆ«å‡†ç¡®ç‡æŒ‡å¯¼è®­ç»ƒ
        prev_class_acc = training_history['class_acc'][-1] if training_history['class_acc'] else None

        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, 
                                device, config['grad_accum_steps'], epoch, prev_class_acc)
        training_history['train_loss'].append(train_loss)

        # éªŒè¯
        val_acc, val_loss, class_acc = evaluate(model, val_loader, criterion)
        training_history['val_acc'].append(val_acc)
        training_history['val_loss'].append(val_loss)
        training_history['class_acc'].append(class_acc)
        training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])

        # æ‰“å°ç»“æœ
        current_lr = optimizer.param_groups[0]['lr']
        epoch_time = time.time() - epoch_start
        
        print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"ğŸ¯ éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.2f}% | éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"ğŸ’¡ å­¦ä¹ ç‡: {current_lr:.2e}")
        print(f"â±ï¸  æœ¬è½®è€—æ—¶: {epoch_time:.1f}ç§’")
        
        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        print("ğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡:")
        for i, cls_name in enumerate(val_dataset.classes):
            improvement = class_acc[i] - prev_class_acc[i] if prev_class_acc and i < len(prev_class_acc) else 0
            arrow = "â†‘" if improvement > 0 else "â†“" if improvement < 0 else "â†’"
            print(f"  {cls_name}: {class_acc[i]*100:5.1f}% {arrow}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc + 0.001:
            best_acc = val_acc
            best_model_path = f'best_vit_{MODEL_SIZE}_targeted.pth'
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_acc': best_acc,
                'config': config,
                'class_acc': class_acc
            }, best_model_path)
            print(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}% -> {best_model_path}")

        # æ—©åœæ£€æŸ¥
        if early_stopping(val_acc, epoch):
            print("ğŸ›‘ æ—©åœè§¦å‘ï¼Œè®­ç»ƒç»“æŸ")
            break

        print("-" * 60)

        # æ¯2ä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint
        if (epoch + 1) % 2 == 0:
            checkpoint_path = f'checkpoint_epoch_{epoch+1}.pth'
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_acc': best_acc,
                'training_history': training_history,
                'config': config
            }, checkpoint_path)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    # === 9. è®­ç»ƒæ€»ç»“ ===
    print("\n" + "=" * 60)
    print("ğŸ¯ è®­ç»ƒæ€»ç»“")
    print("=" * 60)
    print(f"ğŸ“Š æœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}%")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹: best_vit_{MODEL_SIZE}_targeted.pth")
    print(f"ğŸ”„ æ€»è®­ç»ƒè½®æ¬¡: {epoch+1}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(training_history['train_loss'], label='è®­ç»ƒæŸå¤±')
        plt.plot(training_history['val_loss'], label='éªŒè¯æŸå¤±')
        plt.legend()
        plt.title('æŸå¤±æ›²çº¿')
        
        plt.subplot(1, 2, 2)
        plt.plot(training_history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', color='green')
        plt.axhline(y=best_acc, color='r', linestyle='--', label=f'æœ€ä½³: {best_acc*100:.1f}%')
        plt.legend()
        plt.title('å‡†ç¡®ç‡æ›²çº¿')
        
        plt.tight_layout()
        plt.savefig('training_curves_targeted.png', dpi=300, bbox_inches='tight')
        print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: training_curves_targeted.png")
    except:
        print("âš ï¸ æ— æ³•ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼Œè¯·å®‰è£…matplotlib")

    # === 10. æœ€ç»ˆéªŒè¯ ===
    print("\nğŸ” æœ€ç»ˆæ¨¡å‹éªŒè¯...")
    try:
        checkpoint = torch.load(f'best_vit_{MODEL_SIZE}_targeted.pth')
        model.load_state_dict(checkpoint['model_state_dict'])
        final_acc, final_loss, class_acc = evaluate(model, val_loader, criterion)
        
        print(f"âœ… æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_acc*100:.2f}%")
        
        # åˆ†æå„ç±»åˆ«è¡¨ç°
        print("ğŸ“Š å„ç±»åˆ«æœ€ç»ˆå‡†ç¡®ç‡:")
        for i, cls_name in enumerate(val_dataset.classes):
            print(f"  {cls_name}: {class_acc[i]*100:5.1f}%")
        
        if final_acc >= 0.8:
            print("ğŸ‰ æ­å–œï¼è¾¾åˆ°80%å‡†ç¡®ç‡ç›®æ ‡ï¼")
        elif final_acc >= 0.75:
            print("âœ… ä¼˜ç§€ï¼å‡†ç¡®ç‡ > 75%")
        elif final_acc >= 0.7:
            print("ğŸ“ˆ è‰¯å¥½ï¼å‡†ç¡®ç‡ > 70%")
        else:
            print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    except Exception as e:
        print(f"âŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}")


if __name__ == '__main__':
    main()


ğŸ”§ ä¸»è¦ä¿®å¤å†…å®¹

1. ä¿®å¤äº†å…³é”®é”™è¯¯

# ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰ï¼š
new_weights = self.base_weights.copy()

# ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰ï¼š
new_weights = self.base_weights.clone()  # å¼ é‡ä½¿ç”¨clone()æ–¹æ³•


2. å¢å¼ºäº†é”™è¯¯å¤„ç†

# æ·»åŠ äº†ç©ºå€¼æ£€æŸ¥
if current_acc is None or len(current_acc) < 3:
    return new_weights


3. ä¼˜åŒ–äº†æƒé‡è°ƒæ•´ç­–ç•¥

# åŸºäºç¬¬9ä¸ªepochç»“æœçš„å…·ä½“è°ƒæ•´
weight_adjustments = {
    1: 2.5,  # disgust: 49.5% â†’ å¤§å¹…å¢åŠ æƒé‡åŠ©åŠ›çªç ´50%
    2: 3.0,  # fear: 39.9% â†’ æœ€å¤§æƒé‡é‡ç‚¹æ¢å¤
    5: 1.8,  # sad: 57.2% â†’ å¢åŠ æƒé‡åŠ©åŠ›çªç ´60%
    6: 0.7,  # surprise: 85.7% â†’ é™ä½æƒé‡
}


ğŸ¯ ç¬¬10ä¸ªepoché¢„æœŸç›®æ ‡

åŸºäºä¿®å¤åçš„ä»£ç ï¼Œé¢„æœŸï¼š

ç±»åˆ« å½“å‰(Epoch9) ç›®æ ‡(Epoch10) ä¼˜åŒ–ç­–ç•¥

æ€»ä½“å‡†ç¡®ç‡ 69.45% 70.5-71.0% é’ˆå¯¹æ€§å¢å¼º+æƒé‡è°ƒæ•´

disgust 49.5% >51% å¼ºå¢å¼º+é«˜æƒé‡(2.5x)

fear 39.9% >43% æœ€å¼ºå¢å¼º+æœ€é«˜æƒé‡(3.0x)

sad 57.2% >59% ä¸­ç­‰å¢å¼º+æƒé‡(1.8x)

surprise 85.7% >85% é™ä½æƒé‡(0.7x)

ğŸš€ ä½¿ç”¨è¯´æ˜

ç›´æ¥è¿è¡Œä¿®å¤åçš„ä»£ç ï¼š
python train_fixed.py


ä¿®å¤å·²å®Œæˆï¼ä»£ç ç°åœ¨åº”è¯¥å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œä¸ä¼šå‡ºç°AttributeErroré”™è¯¯ã€‚ ç¬¬10ä¸ªepochå°†é’ˆå¯¹disgustå’Œfearè¿›è¡Œé‡ç‚¹ä¼˜åŒ–ï¼