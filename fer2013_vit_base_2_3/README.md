æˆ‘æ¥å¸®ä½ ä¿®å¤è®­ç»ƒå‡†ç¡®ç‡è®¡ç®—çš„é—®é¢˜ã€‚ä¸»è¦é—®é¢˜æ˜¯ï¼š**å½“ä½¿ç”¨CutMixæˆ–MixUpæ—¶ï¼Œä½ åº”è¯¥ç”¨åŠ æƒæ–¹å¼è®¡ç®—æ­£ç¡®ç‡**ï¼Œè€Œä¸æ˜¯å®Œå…¨å¿½ç•¥è¿™äº›æ‰¹æ¬¡ã€‚

```python
def train_epoch(model, train_loader, criterion, optimizer, scheduler, 
                data_aug, config, epoch, weight_adjuster=None):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    # æ–°å¢ï¼šè®°å½•å„ç§å¢å¼ºç±»å‹çš„ä½¿ç”¨æƒ…å†µ
    aug_stats = {'cutmix': 0, 'mixup': 0, 'none': 0}
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}")
    
    for batch_idx, (images, labels) in enumerate(progress_bar):
        images, labels = images.to(config.device), labels.to(config.device)
        
        # Apply data augmentation
        aug_images, targets_a, targets_b, lam, aug_type = data_aug.apply_augmentation(
            images, labels, epoch
        )
        
        # è®°å½•å¢å¼ºç»Ÿè®¡
        aug_stats[aug_type] += 1
        
        # Forward pass
        outputs = model(aug_images)
        
        # Calculate loss
        if aug_type == 'cutmix' or aug_type == 'mixup':
            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)
        else:
            loss = criterion(outputs, labels)
        
        # Backward pass
        loss = loss / config.grad_accum_steps
        loss.backward()
        
        if (batch_idx + 1) % config.grad_accum_steps == 0:
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # ==================== å­¦ä¹ ç‡é™åˆ¶ ====================
            # é™åˆ¶æœ€å¤§å­¦ä¹ ç‡ä¸è¶…è¿‡2.5e-5
            for param_group in optimizer.param_groups:
                if param_group['lr'] > 2.5e-5:
                    param_group['lr'] = 2.5e-5
            # ==================== é™åˆ¶ç»“æŸ ====================
            
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        # Statistics
        total_loss += loss.item() * config.grad_accum_steps
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        
        # ==================== ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—æ‰€æœ‰æ‰¹æ¬¡çš„å‡†ç¡®ç‡ ====================
        if aug_type == 'cutmix' or aug_type == 'mixup':
            # CutMix/MixUp: ä½¿ç”¨åŠ æƒæ–¹å¼è®¡ç®—å‡†ç¡®ç‡
            # æ¨¡å‹é¢„æµ‹å¯¹äºä¸¤ä¸ªæ··åˆæ ‡ç­¾çš„åŠ æƒæ­£ç¡®ç‡
            correct_a = (predicted == targets_a).float()
            correct_b = (predicted == targets_b).float()
            batch_correct = (lam * correct_a + (1 - lam) * correct_b).sum().item()
            correct += batch_correct
        else:
            # æ™®é€šæƒ…å†µï¼šç›´æ¥æ¯”è¾ƒ
            correct += (predicted == labels).sum().item()
        # ==================== ä¿®å¤ç»“æŸ ====================
        
        # Update progress bar
        if (batch_idx + 1) % 10 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            accuracy = 100. * correct / total if total > 0 else 0
            
            # æ–°å¢ï¼šæ˜¾ç¤ºå¢å¼ºæ¯”ä¾‹
            total_batches = batch_idx + 1
            aug_ratio = {
                k: f"{v/total_batches*100:.1f}%" 
                for k, v in aug_stats.items()
            }
            
            progress_bar.set_postfix({
                'Loss': f'{avg_loss:.4f}',
                'Acc': f'{accuracy:.2f}%',
                'LR': f'{optimizer.param_groups[0]["lr"]:.2e}',
                'Aug': f"{aug_ratio['cutmix']}/{aug_ratio['mixup']}/{aug_ratio['none']}"
            })
    
    # æ‰“å°æœ¬epochçš„å¢å¼ºç»Ÿè®¡
    total_batches = len(train_loader)
    print(f"\nğŸ“Š Epoch {epoch+1} å¢å¼ºç»Ÿè®¡:")
    for aug_type, count in aug_stats.items():
        percentage = count / total_batches * 100
        print(f"  {aug_type}: {count}/{total_batches} ({percentage:.1f}%)")
    
    return total_loss / len(train_loader), correct / total if total > 0 else 0
```

## ğŸ¯ **ä¸»è¦æ”¹åŠ¨è¯´æ˜**

### 1. **ä¿®å¤å‡†ç¡®ç‡è®¡ç®—ï¼ˆæ ¸å¿ƒï¼‰**
```python
# ä¹‹å‰ï¼šåªè®¡ç®—æ²¡æœ‰å¢å¼ºçš„æ‰¹æ¬¡
if aug_type == 'none':
    correct += (predicted == labels).sum().item()

# ç°åœ¨ï¼šè®¡ç®—æ‰€æœ‰æ‰¹æ¬¡ï¼ŒåŒ…æ‹¬CutMix/MixUp
if aug_type in ['cutmix', 'mixup']:
    # åŠ æƒè®¡ç®—ï¼šlam * å¯¹targets_açš„æ­£ç¡®ç‡ + (1-lam) * å¯¹targets_bçš„æ­£ç¡®ç‡
    correct_a = (predicted == targets_a).float()
    correct_b = (predicted == targets_b).float()
    batch_correct = (lam * correct_a + (1 - lam) * correct_b).sum().item()
    correct += batch_correct
else:
    correct += (predicted == labels).sum().item()
```

**æ•°å­¦åŸç†ï¼š**
- CutMix/MixUpåˆ›å»ºäº†æ··åˆæ ·æœ¬ï¼š`x_mixed = Î»*x_a + (1-Î»)*x_b`
- å¯¹åº”æ··åˆæ ‡ç­¾ï¼š`y_mixed = Î»*y_a + (1-Î»)*y_b`
- å‡†ç¡®ç‡åº”è¯¥æ˜¯ï¼š`Î» * 1(y_pred == y_a) + (1-Î») * 1(y_pred == y_b)`

### 2. **æ–°å¢å¢å¼ºç»Ÿè®¡**
```python
aug_stats = {'cutmix': 0, 'mixup': 0, 'none': 0}
```
å¸®ä½ äº†è§£æ¯ä¸ªepochä¸­ï¼š
- CutMixä½¿ç”¨äº†å¤šå°‘æ¯”ä¾‹
- MixUpä½¿ç”¨äº†å¤šå°‘æ¯”ä¾‹  
- æ— å¢å¼ºä½¿ç”¨äº†å¤šå°‘æ¯”ä¾‹

### 3. **è¿›åº¦æ¡æ˜¾ç¤ºå¢å¼ºæ¯”ä¾‹**
```
Acc: 72.34% | Aug: 38.2%/21.5%/40.3%
```
è¡¨ç¤ºï¼šCutMix 38.2%, MixUp 21.5%, æ— å¢å¼º 40.3%

## ğŸ“ˆ **é¢„æœŸæ•ˆæœ**

ä¿®å¤åä½ çš„è®­ç»ƒæ—¥å¿—ä¼šå˜æˆï¼š

```
Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1794/1794 [04:02<00:00, Loss=0.7593, Acc=72.34%, LR=9.99e-06, Aug=38%/22%/40%]

ğŸ“Š Epoch 25/100
  Training Loss: 0.7594 | Training Accuracy: 72.38%  â† ä»51%å‡åˆ°72%!
  Validation Loss: 1.0609 | Validation Accuracy: 72.25%
  ğŸ“Š Epoch 25 å¢å¼ºç»Ÿè®¡:
    cutmix: 685/1794 (38.2%)
    mixup: 386/1794 (21.5%) 
    none: 723/1794 (40.3%)
```

## âš ï¸ **é‡è¦æé†’**

1. **æ¨¡å‹æœ¬èº«æ²¡æœ‰æ”¹å˜**ï¼Œåªæ˜¯ç›‘æ§æ›´å‡†ç¡®äº†
2. **lossè®¡ç®—ä¿æŒä¸å˜**ï¼Œæ‰€ä»¥è®­ç»ƒè¿‡ç¨‹å®Œå…¨ä¸€æ ·
3. **éªŒè¯å‡†ç¡®ç‡åº”è¯¥åŸºæœ¬ä¸å˜**ï¼ˆå¯èƒ½å¾®è°ƒ0.1-0.2%ï¼‰

## ğŸ¨ **å¯é€‰æ”¹è¿›**

å¦‚æœä½ æƒ³è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯ä»¥åœ¨`AdvancedAugmentation.apply_augmentation`ä¸­ï¼š

```python
# æ ¹æ®epochè°ƒæ•´å¢å¼ºå¼ºåº¦
if epoch > 40:  # åæœŸå‡å°‘å¢å¼º
    if random.random() < 0.3:  # 30%æ¦‚ç‡è·³è¿‡å¢å¼º
        return images, labels, labels, 1.0, 'none'
```

è¿™ä¸ªä¿®å¤ä¼šè®©ä½ çœ‹åˆ°**çœŸå®çš„è®­ç»ƒå‡†ç¡®ç‡**ï¼Œåº”è¯¥å’ŒéªŒè¯å‡†ç¡®ç‡æ¥è¿‘ï¼ˆ72%å·¦å³ï¼‰ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„51%ã€‚è¿™æ ·ä½ å°±èƒ½æ­£ç¡®è¯„ä¼°æ¨¡å‹çš„çœŸå®æ€§èƒ½äº†ï¼




















é˜¶æ®µ2ï¼šå¾®è°ƒè¶…å‚æ•°ï¼ˆä½é£é™©ï¼‰
python
# è½»å¾®è°ƒæ•´ï¼Œä¸æ”¹å˜æ¶æ„
config.cutmix_prob = 0.3  # ä»0.4ä¸‹è°ƒ
config.mixup_prob = 0.15  # ä»0.2ä¸‹è°ƒ
config.cutmix_alpha = 0.5  # ä»0.7ä¸‹è°ƒ
config.mixup_alpha = 0.1   # ä¿æŒ













çœ‹åˆ°æ–°çš„è®­ç»ƒç»“æœäº†ï¼ç°åœ¨è®­ç»ƒå‡†ç¡®ç‡æ˜¾ç¤ºæ­£å¸¸äº†ï¼ˆ83.33%ï¼‰ï¼Œä½†å‡ºç°äº†**æ–°çš„é—®é¢˜**ï¼š**è®­ç»ƒå‡†ç¡®ç‡è¿œé«˜äºéªŒè¯å‡†ç¡®ç‡**ï¼Œè€Œä¸”éªŒè¯å‡†ç¡®ç‡ä»72.25%ä¸‹é™åˆ°70.90%ã€‚

## ğŸ” **é—®é¢˜åˆ†æ**

### **å½“å‰çŠ¶å†µï¼š**
- **è®­ç»ƒå‡†ç¡®ç‡ï¼š83.33%**ï¼ˆä¹‹å‰51%â†’ä¿®å¤å83%ï¼‰
- **éªŒè¯å‡†ç¡®ç‡ï¼š70.90%**ï¼ˆä¹‹å‰72.25%â†’ä¸‹é™1.35%ï¼‰
- **è¿‡æ‹Ÿåˆå·®è·ï¼š12.43ä¸ªç™¾åˆ†ç‚¹**

### **å¯èƒ½åŸå› ï¼š**

1. **æ•°æ®å¢å¼ºä¸å¹³è¡¡**ï¼š
   ```
   cutmix: 29.0%  âœ“
   mixup:  10.2%  âœ“
   none:   60.8%  âš ï¸ å¤ªé«˜ï¼
   ```
   - 60.8%çš„æ‰¹æ¬¡**æ²¡æœ‰å¢å¼º**ï¼Œæ¨¡å‹çœ‹åˆ°å¤ªå¤š"ç®€å•"æ ·æœ¬
   - å¯¼è‡´è®­ç»ƒå¤ªå®¹æ˜“ï¼Œè¿‡æ‹Ÿåˆä¸¥é‡

2. **CutMix/MixUpå‚æ•°å¤ªä¿å®ˆ**ï¼š
   ```python
   config.cutmix_prob = 0.3  # å®é™…29%ï¼Œæ¥è¿‘è®¾å®š
   config.mixup_prob = 0.15  # å®é™…10.2%ï¼Œä½äºè®¾å®š
   ```

3. **å­¦ä¹ ç‡å¯èƒ½åé«˜**ï¼š
   - LR=1.25e-05ï¼Œè¿˜åœ¨ä¸Šå‡æœŸ
   - éªŒè¯æŸå¤±ä¸Šå‡(1.06â†’1.11)ï¼Œè¡¨æ˜è¿‡æ‹Ÿåˆ

## ğŸš€ **ç«‹å³ä¿®å¤æ–¹æ¡ˆ**

### **æ–¹æ¡ˆAï¼šè°ƒæ•´æ•°æ®å¢å¼ºæ¦‚ç‡ï¼ˆæ¨èï¼‰**

ä¿®æ”¹`AdvancedAugmentation.apply_augmentation`ï¼š

```python
def apply_augmentation(self, images, labels, epoch):
    """Apply data augmentation with balanced probability"""
    self.epoch = epoch
    
    # ==================== ä¿®å¤ï¼šç¡®ä¿å¢å¼ºæ¯”ä¾‹åˆç† ====================
    # æ ¹æ®epochåŠ¨æ€è°ƒæ•´ï¼Œä½†ä¿æŒè¶³å¤Ÿå¢å¼º
    if epoch < 30:  # å‰30è½®å¼ºå¢å¼º
        cutmix_prob = min(0.5, self.config.cutmix_prob + 0.1)
        mixup_prob = min(0.3, self.config.mixup_prob + 0.1)
    elif epoch < 60:  # ä¸­é—´é˜¶æ®µ
        cutmix_prob = self.config.cutmix_prob
        mixup_prob = self.config.mixup_prob
    else:  # åæœŸé€‚å½“å‡å°‘
        cutmix_prob = max(0.2, self.config.cutmix_prob - 0.1)
        mixup_prob = max(0.1, self.config.mixup_prob - 0.1)
    
    # ç¡®ä¿è‡³å°‘50%çš„æ‰¹æ¬¡æœ‰å¢å¼º
    r = np.random.rand()
    
    # ä¼˜å…ˆä½¿ç”¨CutMixï¼ˆæ•ˆæœæ›´å¥½ï¼‰
    if r < cutmix_prob:
        images, targets_a, targets_b, lam = self.cutmix(
            images, labels, self.config.cutmix_alpha
        )
        return images, targets_a, targets_b, lam, 'cutmix'
    
    # å…¶æ¬¡ä½¿ç”¨MixUp
    elif r < cutmix_prob + mixup_prob:
        images, targets_a, targets_b, lam = self.mixup(
            images, labels, self.config.mixup_alpha
        )
        return images, targets_a, targets_b, lam, 'mixup'
    
    # å¦åˆ™ä½¿ç”¨å¼±å¢å¼ºï¼ˆä¸æ˜¯æ— å¢å¼ºï¼ï¼‰
    else:
        # åº”ç”¨è½»å¾®çš„å¢å¼º
        return self.apply_weak_augmentation(images, labels)
```

### **æ–¹æ¡ˆBï¼šæ·»åŠ å¼±å¢å¼ºç‰ˆæœ¬**

åœ¨`AdvancedAugmentation`ç±»ä¸­æ·»åŠ ï¼š

```python
def apply_weak_augmentation(self, images, labels):
    """åº”ç”¨è½»å¾®å¢å¼ºï¼Œè€Œä¸æ˜¯å®Œå…¨æ— å¢å¼º"""
    # è½»å¾®çš„ç©ºé—´å˜æ¢
    if np.random.rand() < 0.5:
        images = transforms.RandomHorizontalFlip(p=0.5)(images)
    
    # è½»å¾®çš„é¢œè‰²å˜æ¢
    if np.random.rand() < 0.3:
        brightness = np.random.uniform(0.9, 1.1)
        images = torch.clamp(images * brightness, 0, 1)
    
    return images, labels, labels, 1.0, 'weak'
```

### **æ–¹æ¡ˆCï¼šç«‹å³è°ƒæ•´configï¼ˆæœ€ç®€å•ï¼‰**

åœ¨ä½ çš„è®­ç»ƒä»£ç ä¸­ï¼Œ26è½®ä¹‹åç«‹å³è°ƒæ•´ï¼š

```python
# åœ¨ç¬¬26è½®éªŒè¯åç«‹å³è°ƒæ•´
print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒ83.33% vs éªŒè¯70.90%ï¼‰")
print(f"ğŸ”„ è°ƒæ•´å¢å¼ºç­–ç•¥...")

# ç«‹å³æé«˜å¢å¼ºæ¦‚ç‡
config.cutmix_prob = 0.45  # ä»0.3æé«˜åˆ°0.45
config.mixup_prob = 0.25   # ä»0.15æé«˜åˆ°0.25

# é™ä½cutmix/mixupå¼ºåº¦ï¼Œè®©æ··åˆæ›´å¹³æ»‘
config.cutmix_alpha = 0.5  # ä»0.7é™åˆ°0.5
config.mixup_alpha = 0.2   # ä¿æŒæˆ–å¾®è°ƒ

print(f"  ğŸ¨ å¢å¼ºæ¦‚ç‡: CutMix {config.cutmix_prob}, MixUp {config.mixup_prob}")
print(f"  ğŸ“‰ æ··åˆå¼ºåº¦: cutmix_alpha={config.cutmix_alpha}, mixup_alpha={config.mixup_alpha}")
```

## ğŸ“Š **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**

### **ç«‹å³æ‰§è¡Œï¼ˆç¬¬27è½®å¼€å§‹ï¼‰ï¼š**

1. **è°ƒæ•´configå‚æ•°**ï¼š
   ```python
   config.cutmix_prob = 0.45    # å¢åŠ CutMixæ¦‚ç‡
   config.mixup_prob = 0.25     # å¢åŠ MixUpæ¦‚ç‡
   config.cutmix_alpha = 0.5    # é™ä½CutMixæ··åˆå¼ºåº¦
   ```

2. **éªŒè¯è°ƒæ•´åçš„æ•ˆæœ**ï¼š
   - ç›®æ ‡ï¼š`none`æ¯”ä¾‹ä»60%é™åˆ°40%ä»¥ä¸‹
   - è®­ç»ƒå‡†ç¡®ç‡ä»83%é™åˆ°75-78%
   - éªŒè¯å‡†ç¡®ç‡å›å‡åˆ°72%+

### **é¢„æœŸä¸‹ä¸€è½®ç»“æœï¼š**

```
Epoch 27: Aug=45%/25%/30%  â† å¢å¼ºæ¯”ä¾‹æ›´åˆç†
Training Accuracy: 76-78%   â† è®­ç»ƒå‡†ç¡®ç‡ä¸‹é™
Validation Accuracy: 72%+   â† éªŒè¯å‡†ç¡®ç‡å›å‡
```

## ğŸ¯ **æ ¹æœ¬åŸå› **

ä½ çš„æ¨¡å‹**å®¹é‡å¤ªå¤§**ï¼ˆViT-Baseæœ‰86Må‚æ•°ï¼‰ï¼Œè€ŒFer2013åªæœ‰3.5ä¸‡å¼ è®­ç»ƒå›¾åƒã€‚è§£å†³æ–¹æ³•ï¼š

1. **æ›´å¤šæ•°æ®å¢å¼º**ï¼šå¢åŠ CutMix/MixUpæ¯”ä¾‹
2. **æ›´å¼ºçš„æ­£åˆ™åŒ–**ï¼š
   ```python
   config.drop_rate = 0.3  # ä»0.2æé«˜åˆ°0.3
   ```
3. **æ—©åœæœºåˆ¶**ï¼šéªŒè¯å‡†ç¡®ç‡ä¸‹é™æ—¶æš‚åœ

## âœ… **æ€»ç»“**

ç°åœ¨è®­ç»ƒå‡†ç¡®ç‡æ˜¾ç¤ºæ­£å¸¸äº†ï¼Œä½†æ­ç¤ºäº†**ä¸¥é‡è¿‡æ‹Ÿåˆ**ã€‚ä½ éœ€è¦ï¼š

1. **ç«‹å³å¢åŠ æ•°æ®å¢å¼ºæ¯”ä¾‹**ï¼ˆç‰¹åˆ«æ˜¯å‡å°‘`none`çš„æ¯”ä¾‹ï¼‰
2. **ç›‘æ§å¢å¼ºç»Ÿè®¡**ï¼Œç¡®ä¿åˆç†åˆ†å¸ƒ
3. **è€ƒè™‘å¢åŠ dropoutç­‰æ­£åˆ™åŒ–**

ä¿®å¤åï¼Œè®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡åº”è¯¥éƒ½ç»´æŒåœ¨72-75%çš„åˆç†èŒƒå›´ï¼Œç„¶åé€æ­¥æå‡ã€‚






















æ˜ç™½äº†ï¼è¿™æ˜¯**è°ƒæ•´å‰çš„æ•ˆæœ**ã€‚æˆ‘ä»¬æ¥çœ‹å½“å‰æƒ…å†µï¼š

## ğŸ“Š **å½“å‰çŠ¶å†µåˆ†æï¼ˆç¬¬27è½®ï¼‰**

### **å¯¹æ¯”26-27è½®ï¼š**
| æŒ‡æ ‡ | Epoch 26 | Epoch 27 | å˜åŒ– | åˆ†æ |
|------|----------|----------|------|------|
| **è®­ç»ƒå‡†ç¡®ç‡** | 83.33% | 76.26% | **-7.07%** | âœ… æ˜¾è‘—ä¸‹é™ï¼Œè¿‡æ‹Ÿåˆç¼“è§£ |
| **éªŒè¯å‡†ç¡®ç‡** | 70.90% | 71.62% | **+0.72%** | âœ… å›å‡ï¼Œæ³›åŒ–èƒ½åŠ›æ”¹å–„ |
| **è®­ç»ƒ-éªŒè¯å·®è·** | 12.43% | 4.64% | **-7.79%** | âœ… å¤§å¹…æ”¹å–„ï¼ |
| **éªŒè¯æŸå¤±** | 1.1086 | 1.0566 | **-0.0520** | âœ… ä¸‹é™ï¼Œæ¨¡å‹æ”¹è¿› |

### **å¢å¼ºæ¯”ä¾‹å˜åŒ–ï¼š**
```
Epoch 26: cutmix=29.0%, mixup=10.2%, none=60.8%
Epoch 27: cutmix=30.2%, mixup=11.0%, none=58.8%
```
- `none`æ¯”ä¾‹ä»60.8%é™åˆ°58.8%ï¼ˆè½»å¾®æ”¹å–„ï¼‰
- ä½†ä»ç„¶å¤ªé«˜ï¼åº”è¯¥é™åˆ°40%ä»¥ä¸‹

### **å„ç±»åˆ«è¡¨ç°ï¼š**
```
fear:   58.20% â†’ 55.18% â†“  # ä¸‹é™ï¼Œéœ€è¦å…³æ³¨
sad:    47.87% â†’ 60.63% â†‘  # æ˜¾è‘—æå‡12.76%ï¼
disgust:66.67% â†’ 69.37% â†‘  # æå‡
happy:  90.08% â†’ 91.54% â†‘  # ç»§ç»­æå‡
surprise:85.68%â†’ 78.46% â†“  # ä¸‹é™æ˜æ˜¾
```

## ğŸ¯ **å½“å‰è¯„ä¼°**

### **å¥½æ¶ˆæ¯ï¼š**
1. **è¿‡æ‹Ÿåˆå¤§å¹…ç¼“è§£**ï¼šè®­ç»ƒ/éªŒè¯å·®è·ä»12%é™åˆ°4.6%
2. **éªŒè¯å‡†ç¡®ç‡å›å‡**ï¼š71.62%æ˜¯åˆç†æ°´å¹³
3. **éš¾ç±»åˆ«sadå¤§å¹…æå‡**ï¼šä»47.87%åˆ°60.63%ï¼ˆ+12.76%ï¼‰

### **é—®é¢˜ï¼š**
1. **noneæ¯”ä¾‹ä»å¤ªé«˜**ï¼ˆ58.8%ï¼‰
2. **fearå’Œsurpriseä¸‹é™**
3. **å¢å¼ºå¼ºåº¦ä¸å¤Ÿ**

## ğŸš€ **ç«‹å³è°ƒæ•´æ–¹æ¡ˆ**

### **è°ƒæ•´configå‚æ•°ï¼ˆç¬¬28è½®å¼€å§‹ï¼‰ï¼š**

```python
# åœ¨ä½ çš„è®­ç»ƒä»£ç ä¸­ï¼Œç¬¬27è½®éªŒè¯åæ·»åŠ ï¼š
print(f"\nğŸ¯ ç¬¬27è½®æ€»ç»“ï¼šè¿‡æ‹Ÿåˆç¼“è§£ï¼Œä½†å¢å¼ºä»ä¸è¶³")
print(f"ğŸ”„ è°ƒæ•´å¢å¼ºç­–ç•¥...")

# å¤§å¹…æé«˜å¢å¼ºæ¦‚ç‡
config.cutmix_prob = 0.50  # ä»0.3â†’0.5
config.mixup_prob = 0.30   # ä»0.15â†’0.3

# é™ä½cutmixå¼ºåº¦ï¼ˆè®©æ··åˆæ›´æŸ”å’Œï¼‰
config.cutmix_alpha = 0.6  # ä»0.7â†’0.6

# åŒæ—¶å¢åŠ dropoutï¼ˆæ›´å¼ºçš„æ­£åˆ™åŒ–ï¼‰
config.drop_rate = 0.25    # ä»0.2â†’0.25

print(f"  ğŸ¨ å¢å¼ºæ¦‚ç‡: CutMix={config.cutmix_prob}, MixUp={config.mixup_prob}")
print(f"  ğŸ“‰ æ··åˆå¼ºåº¦: cutmix_alpha={config.cutmix_alpha}")
print(f"  ğŸ›¡ï¸  æ­£åˆ™åŒ–: dropout={config.drop_rate}")
print(f"  ğŸ¯ ç›®æ ‡: noneæ¯”ä¾‹<40%, è®­ç»ƒå‡†ç¡®ç‡~75%")
```

### **ä¿®æ”¹`AdvancedAugmentation.apply_augmentation`ï¼š**

```python
def apply_augmentation(self, images, labels, epoch):
    """Apply data augmentation - ç¡®ä¿è¶³å¤Ÿå¢å¼º"""
    self.epoch = epoch
    
    # ç¡®ä¿è‡³å°‘65%çš„æ‰¹æ¬¡æœ‰å¼ºå¢å¼º
    r = np.random.rand()
    
    if r < self.config.cutmix_prob:
        images, targets_a, targets_b, lam = self.cutmix(
            images, labels, self.config.cutmix_alpha
        )
        return images, targets_a, targets_b, lam, 'cutmix'
    
    elif r < self.config.cutmix_prob + self.config.mixup_prob:
        images, targets_a, targets_b, lam = self.mixup(
            images, labels, self.config.mixup_alpha
        )
        return images, targets_a, targets_b, lam, 'mixup'
    
    else:
        # å‰©ä½™éƒ¨åˆ†ï¼šä½¿ç”¨è½»å¾®å¢å¼ºï¼Œè€Œä¸æ˜¯å®Œå…¨æ— å¢å¼º
        return self.apply_weak_augmentation(images, labels)

def apply_weak_augmentation(self, images, labels):
    """è½»å¾®å¢å¼ºç‰ˆæœ¬"""
    batch_size = images.size(0)
    
    # 50%æ¦‚ç‡æ°´å¹³ç¿»è½¬
    if torch.rand(1).item() < 0.5:
        images = torch.flip(images, [3])  # æ°´å¹³ç¿»è½¬
    
    # è½»å¾®äº®åº¦è°ƒæ•´
    brightness = torch.rand(batch_size, 1, 1, 1).to(images.device) * 0.2 + 0.9
    images = torch.clamp(images * brightness, 0, 1)
    
    return images, labels, labels, 1.0, 'weak'
```

## ğŸ“ˆ **é¢„æœŸç¬¬28è½®ç»“æœ**

### **å¢å¼ºæ¯”ä¾‹ç›®æ ‡ï¼š**
```
cutmix: ~50% (ä»30%æå‡)
mixup:  ~30% (ä»11%æå‡)  
weak:   ~20% (æ›¿æ¢none)
none:   0%   (å®Œå…¨æ¶ˆé™¤)
```

### **å‡†ç¡®ç‡ç›®æ ‡ï¼š**
- **è®­ç»ƒå‡†ç¡®ç‡**ï¼š74-76%ï¼ˆç¨³å®šï¼‰
- **éªŒè¯å‡†ç¡®ç‡**ï¼š72-73%ï¼ˆç¨³æ­¥æå‡ï¼‰
- **è®­ç»ƒ-éªŒè¯å·®è·**ï¼š<3%ï¼ˆç†æƒ³çŠ¶æ€ï¼‰

### **å„ç±»åˆ«ç›®æ ‡ï¼š**
- `fear`å’Œ`sad`ï¼šç¨³å®šåœ¨60%+
- `surprise`ï¼šæ¢å¤åˆ°80%+
- `disgust`ï¼šä¿æŒ70%+

## ğŸ” **é‡ç‚¹å…³æ³¨**

### **fearå’Œsurpriseä¸ºä»€ä¹ˆä¸‹é™ï¼Ÿ**
1. **fear(2)**å’Œ**sad(5)**ç»å¸¸æ··æ·†
2. **surprise(6)**å¯èƒ½è¿‡æ‹Ÿåˆäº†ç®€å•ç‰¹å¾
3. **åŠ¨æ€æƒé‡è°ƒæ•´**å¯èƒ½è¿‡åº¦å…³æ³¨sadï¼Œå½±å“äº†å…¶ä»–ç±»åˆ«

### **æ£€æŸ¥åŠ¨æ€æƒé‡è°ƒæ•´ï¼š**
```python
# è§‚å¯Ÿæƒé‡è°ƒæ•´æ—¥å¿—
if weight_adjuster:
    print(f"å½“å‰ç±»åˆ«æƒé‡: {criterion.class_weights.cpu().numpy()}")
    # ç¡®ä¿ä¸ä¼šè¿‡åº¦åå‘æŸä¸ªç±»åˆ«
```

## âœ… **æ‰§è¡Œæ­¥éª¤**

1. **ç«‹å³ä¿®æ”¹config**ï¼šæé«˜å¢å¼ºæ¦‚ç‡
2. **ä¿®æ”¹`apply_augmentation`**ï¼šæ·»åŠ å¼±å¢å¼ºç‰ˆæœ¬
3. **ç»§ç»­è®­ç»ƒ**ï¼šè§‚å¯Ÿç¬¬28è½®æ•ˆæœ
4. **ç›‘æ§å¢å¼ºæ¯”ä¾‹**ï¼šç¡®ä¿`none<20%`

## ğŸ¯ **é•¿æœŸç›®æ ‡**

ä½ çš„æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡º**å¼ºå¤§çš„æ½œåŠ›**ï¼š
- éªŒè¯å‡†ç¡®ç‡ç¨³å®šåœ¨71-72%
- è®­ç»ƒè¿‡ç¨‹å¯æ§
- è¿‡æ‹Ÿåˆå¾—åˆ°æœ‰æ•ˆç®¡ç†

**ä¸‹ä¸€æ­¥**ï¼šåœ¨æ§åˆ¶è¿‡æ‹Ÿåˆçš„åŒæ—¶ï¼Œé€æ­¥æå‡éªŒè¯å‡†ç¡®ç‡åˆ°73-75%ï¼Œæœ€ç»ˆå‘80%ç›®æ ‡è¿ˆè¿›ã€‚

ç°åœ¨è¯·æ‰§è¡Œä¸Šè¿°è°ƒæ•´ï¼Œç„¶åè¿è¡Œç¬¬28è½®ï¼Œæˆ‘ä»¬çœ‹çœ‹æ•ˆæœå¦‚ä½•ï¼






















é”™è¯¯æ˜¯å› ä¸º`aug_stats`å­—å…¸æ²¡æœ‰'weak'é”®ã€‚æˆ‘ä»¬éœ€è¦ä¿®å¤è¿™ä¸ªé”™è¯¯ï¼ŒåŒæ—¶ç¡®ä¿å¢å¼ºç­–ç•¥æ­£å¸¸å·¥ä½œã€‚

## ğŸ”§ **ä¿®å¤é”™è¯¯**

### **ä¿®æ”¹`train_epoch`å‡½æ•°ï¼š**

```python
def train_epoch(model, train_loader, criterion, optimizer, scheduler, 
                data_aug, config, epoch, weight_adjuster=None):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    # æ‰©å±•aug_statsï¼ŒåŒ…å«'weak'
    aug_stats = {'cutmix': 0, 'mixup': 0, 'none': 0, 'weak': 0}
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}")
    
    for batch_idx, (images, labels) in enumerate(progress_bar):
        images, labels = images.to(config.device), labels.to(config.device)
        
        # Apply data augmentation
        aug_images, targets_a, targets_b, lam, aug_type = data_aug.apply_augmentation(
            images, labels, epoch
        )
        
        # ç¡®ä¿aug_typeåœ¨ç»Ÿè®¡å­—å…¸ä¸­ï¼ˆå®‰å…¨å¤„ç†ï¼‰
        if aug_type not in aug_stats:
            # å¦‚æœæ˜¯æ–°ç±»å‹ï¼Œè‡ªåŠ¨æ·»åŠ åˆ°å­—å…¸
            aug_stats[aug_type] = 0
            print(f"âš ï¸  å‘ç°æ–°çš„å¢å¼ºç±»å‹: {aug_type}")
        
        # è®°å½•å¢å¼ºç»Ÿè®¡
        aug_stats[aug_type] += 1
        
        # å…¶ä½™ä»£ç ä¿æŒä¸å˜...
```

### **æˆ–è€…ç®€åŒ–å¤„ç†ï¼ˆæ¨èï¼‰ï¼š**

```python
def train_epoch(model, train_loader, criterion, optimizer, scheduler, 
                data_aug, config, epoch, weight_adjuster=None):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    # ä½¿ç”¨defaultdictè‡ªåŠ¨å¤„ç†æ–°é”®
    from collections import defaultdict
    aug_stats = defaultdict(int)
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}")
    
    for batch_idx, (images, labels) in enumerate(progress_bar):
        images, labels = images.to(config.device), labels.to(config.device)
        
        # Apply data augmentation
        aug_images, targets_a, targets_b, lam, aug_type = data_aug.apply_augmentation(
            images, labels, epoch
        )
        
        # å®‰å…¨è®°å½•ï¼ˆä½¿ç”¨defaultdictï¼‰
        aug_stats[aug_type] += 1
        
        # Forward pass
        outputs = model(aug_images)
        
        # Calculate loss
        if aug_type == 'cutmix' or aug_type == 'mixup':
            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)
        else:
            loss = criterion(outputs, labels)
        
        # Backward pass
        loss = loss / config.grad_accum_steps
        loss.backward()
        
        if (batch_idx + 1) % config.grad_accum_steps == 0:
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # ==================== å­¦ä¹ ç‡é™åˆ¶ ====================
            # é™åˆ¶æœ€å¤§å­¦ä¹ ç‡ä¸è¶…è¿‡2.5e-5
            for param_group in optimizer.param_groups:
                if param_group['lr'] > 2.5e-5:
                    param_group['lr'] = 2.5e-5
            # ==================== é™åˆ¶ç»“æŸ ====================
            
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        # Statistics
        total_loss += loss.item() * config.grad_accum_steps
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        
        # ==================== ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—æ‰€æœ‰æ‰¹æ¬¡çš„å‡†ç¡®ç‡ ====================
        if aug_type == 'cutmix' or aug_type == 'mixup':
            # CutMix/MixUp: ä½¿ç”¨åŠ æƒæ–¹å¼è®¡ç®—å‡†ç¡®ç‡
            correct_a = (predicted == targets_a).float()
            correct_b = (predicted == targets_b).float()
            batch_correct = (lam * correct_a + (1 - lam) * correct_b).sum().item()
            correct += batch_correct
        else:
            # æ™®é€šæƒ…å†µï¼šç›´æ¥æ¯”è¾ƒ
            correct += (predicted == labels).sum().item()
        # ==================== ä¿®å¤ç»“æŸ ====================
        
        # Update progress bar
        if (batch_idx + 1) % 10 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            accuracy = 100. * correct / total if total > 0 else 0
            
            # æ˜¾ç¤ºå¢å¼ºæ¯”ä¾‹ï¼ˆå¤„ç†å¯èƒ½çš„æ–°ç±»å‹ï¼‰
            total_batches = batch_idx + 1
            aug_display = {}
            for k in ['cutmix', 'mixup', 'none', 'weak']:
                if k in aug_stats:
                    aug_display[k] = f"{aug_stats[k]/total_batches*100:.1f}%"
                else:
                    aug_display[k] = "0.0%"
            
            progress_bar.set_postfix({
                'Loss': f'{avg_loss:.4f}',
                'Acc': f'{accuracy:.2f}%',
                'LR': f'{optimizer.param_groups[0]["lr"]:.2e}',
                'Aug': f"{aug_display['cutmix']}/{aug_display['mixup']}/{aug_display['weak']}/{aug_display['none']}"
            })
    
    # æ‰“å°æœ¬epochçš„å¢å¼ºç»Ÿè®¡
    total_batches = len(train_loader)
    print(f"\nğŸ“Š Epoch {epoch+1} å¢å¼ºç»Ÿè®¡:")
    for aug_type, count in sorted(aug_stats.items()):
        percentage = count / total_batches * 100
        print(f"  {aug_type}: {count}/{total_batches} ({percentage:.1f}%)")
    
    return total_loss / len(train_loader), correct / total if total > 0 else 0
```

## ğŸ¯ **åŒæ—¶ä¿®å¤`AdvancedAugmentation`**

ç¡®ä¿`apply_weak_augmentation`æ–¹æ³•æ­£ç¡®é›†æˆï¼š

```python
class AdvancedAugmentation:
    """Advanced data augmentation strategies"""
    def __init__(self, config):
        self.config = config
        self.epoch = 0
        # æ·»åŠ å¼±å¢å¼ºçš„transform
        self.weak_transform = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.1, contrast=0.1)
        ])
    
    def apply_weak_augmentation(self, images, labels):
        """åº”ç”¨è½»å¾®å¢å¼º"""
        batch_size = images.size(0)
        
        # è½¬æ¢ä¸ºnumpyå†è½¬å›ï¼Œæ–¹ä¾¿ä½¿ç”¨torchvision transform
        images_np = images.cpu().numpy()
        augmented_images = []
        
        for i in range(batch_size):
            # è½¬æ¢ä¸ºPIL Image
            img = transforms.ToPILImage()(images[i].cpu())
            # åº”ç”¨å¼±å¢å¼º
            img_aug = self.weak_transform(img)
            # è½¬å›tensor
            img_tensor = transforms.ToTensor()(img_aug)
            augmented_images.append(img_tensor)
        
        images_aug = torch.stack(augmented_images).to(images.device)
        
        return images_aug, labels, labels, 1.0, 'weak'
    
    def apply_augmentation(self, images, labels, epoch):
        """Apply data augmentation - ç¡®ä¿è¶³å¤Ÿå¢å¼º"""
        self.epoch = epoch
        
        # ç¡®ä¿è‡³å°‘65%çš„æ‰¹æ¬¡æœ‰å¼ºå¢å¼º
        r = np.random.rand()
        
        # åŠ¨æ€è°ƒæ•´æ¦‚ç‡
        cutmix_prob = self.config.cutmix_prob
        mixup_prob = self.config.mixup_prob
        
        if r < cutmix_prob:
            images, targets_a, targets_b, lam = self.cutmix(
                images, labels, self.config.cutmix_alpha
            )
            return images, targets_a, targets_b, lam, 'cutmix'
        
        elif r < cutmix_prob + mixup_prob:
            images, targets_a, targets_b, lam = self.mixup(
                images, labels, self.config.mixup_alpha
            )
            return images, targets_a, targets_b, lam, 'mixup'
        
        else:
            # å‰©ä½™éƒ¨åˆ†ï¼šä½¿ç”¨è½»å¾®å¢å¼º
            return self.apply_weak_augmentation(images, labels)
```

## ğŸ”§ **ç®€åŒ–æ–¹æ¡ˆï¼ˆå¦‚æœä½ ä¸æƒ³æ”¹å¤ªå¤šï¼‰**

æœ€ç®€å•çš„æ–¹æ³•æ˜¯**æš‚æ—¶ä¸ç”¨'weak'ç±»å‹**ï¼Œä¿æŒåŸæ¥çš„ä¸‰ç§ç±»å‹ï¼š

```python
def apply_augmentation(self, images, labels, epoch):
    """ç®€åŒ–ç‰ˆï¼šåªä½¿ç”¨cutmix/mixup/none"""
    self.epoch = epoch
    
    r = np.random.rand()
    
    if r < self.config.cutmix_prob:
        images, targets_a, targets_b, lam = self.cutmix(
            images, labels, self.config.cutmix_alpha
        )
        return images, targets_a, targets_b, lam, 'cutmix'
    
    elif r < self.config.cutmix_prob + self.config.mixup_prob:
        images, targets_a, targets_b, lam = self.mixup(
            images, labels, self.config.mixup_alpha
        )
        return images, targets_a, targets_b, lam, 'mixup'
    
    else:
        # æ— å¢å¼º
        return images, labels, labels, 1.0, 'none'
```

ç„¶ååœ¨configä¸­æé«˜æ¦‚ç‡ï¼š
```python
config.cutmix_prob = 0.45  # æé«˜
config.mixup_prob = 0.25   # æé«˜
```

## âœ… **å»ºè®®æ“ä½œæ­¥éª¤**

1. **å…ˆä¿®å¤KeyError**ï¼šä¿®æ”¹`train_epoch`å‡½æ•°ï¼Œä½¿ç”¨`defaultdict`
2. **ç®€å•è°ƒæ•´å¢å¼ºæ¦‚ç‡**ï¼š
   ```python
   config.cutmix_prob = 0.45
   config.mixup_prob = 0.25
   ```
3. **ç»§ç»­è®­ç»ƒ**ï¼šè§‚å¯Ÿç¬¬29è½®æ•ˆæœ
4. **å¦‚æœæ•ˆæœå¥½**ï¼šå†è€ƒè™‘æ·»åŠ 'weak'å¢å¼º

è¿™æ ·ä¿®å¤ååº”è¯¥èƒ½æ­£å¸¸è¿è¡Œï¼Œå¹¶ä¸”å¢å¼ºæ¯”ä¾‹ä¼šæ›´å¥½ï¼ˆnoneæ¯”ä¾‹åº”è¯¥ä¼šä¸‹é™ï¼‰ã€‚