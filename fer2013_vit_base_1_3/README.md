ä»¥ä¸‹æ˜¯å®Œæ•´çš„ç¨³å®šæ€§ä¼˜åŒ–ç‰ˆæœ¬ä»£ç ï¼ŒåŸºäºæ‚¨çš„åŸå§‹ä»£ç ç»“æ„è¿›è¡Œä¼˜åŒ–ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms, datasets
import timm
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
from PIL import Image
from tqdm import tqdm
import time
import os
import math
from sklearn.utils.class_weight import compute_class_weight


# === è¶…å‚æ•°é…ç½® ===
config = {
    'model_size': 'base',
    'batch_size': 16,
    'num_epochs': 50,  # å‡å°‘è®­ç»ƒè½®æ¬¡
    'learning_rate': 1e-5,  # å¤§å¹…é™ä½å­¦ä¹ ç‡
    'weight_decay': 0.01,  # å¢åŠ æƒé‡è¡°å‡
    'cutmix_alpha': 0.5,  # é™ä½å¢å¼ºå¼ºåº¦
    'label_smoothing': 0.1,
    'drop_rate': 0.2,
    'grad_accum_steps': 2,
    'warmup_epochs': 5,  # å‡å°‘çƒ­èº«è½®æ¬¡
    'current_epoch': 18,  # ä»ç¬¬18ä¸ªepochç»§ç»­
    'best_acc': 0.6956,  # å½“å‰æœ€ä½³69.56%
    'patience': 8,  # æ—©åœè€å¿ƒå€¼
}


# === ç¨³å®šæ€§ä¼˜åŒ–ç»„ä»¶ ===
class StabilizedEarlyStopping:
    """ç¨³å®šæ€§ä¼˜åŒ–çš„æ—©åœæœºåˆ¶"""
    def __init__(self, patience=8, delta=0.001, min_epochs=5):
        self.patience = patience
        self.delta = delta
        self.min_epochs = min_epochs
        self.best_acc = 0
        self.counter = 0
        self.early_stop = False
        self.best_weights = None
        
    def __call__(self, val_acc, model_weights=None):
        if val_acc > self.best_acc + self.delta:
            self.best_acc = val_acc
            self.counter = 0
            if model_weights is not None:
                self.best_weights = {k: v.clone() for k, v in model_weights.items()}
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        return self.early_stop


class StabilizedDataAugmentation:
    """ç¨³å®šæ€§æ•°æ®å¢å¼º"""
    def __init__(self, cutmix_prob=0.3, mixup_prob=0.2):  # å¤§å¹…é™ä½å¢å¼ºæ¦‚ç‡
        self.cutmix_prob = cutmix_prob
        self.mixup_prob = mixup_prob
        self.epoch = 0
        
    def apply_augmentation(self, images, labels, epoch):
        """åº”ç”¨ç¨³å®šæ€§å¢å¼º"""
        self.epoch = epoch
        
        # é™ä½å¢å¼ºæ¦‚ç‡ï¼Œæé«˜ç¨³å®šæ€§
        if np.random.rand() < self.cutmix_prob:
            images, targets_a, targets_b, lam = self.cutmix(images, labels, alpha=0.5)
            return images, targets_a, targets_b, lam, 'cutmix'
        
        elif np.random.rand() < self.mixup_prob:
            images, targets_a, targets_b, lam = self.mixup(images, labels, alpha=0.1)
            return images, targets_a, targets_b, lam, 'mixup'
        
        return images, labels, labels, 1.0, 'none'
    
    def cutmix(self, x, y, alpha=0.5):
        """CutMixå¢å¼º"""
        if alpha <= 0:
            return x, y, y, 1.0
            
        lam = np.random.beta(alpha, alpha)
        batch_size = x.size()[0]
        index = torch.randperm(batch_size).to(x.device)
        
        bbx1, bby1, bbx2, bby2 = self.rand_bbox(x.size(), lam)
        x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
        
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))
        y_a, y_b = y, y[index]
        
        return x, y_a, y_b, lam
    
    def mixup(self, x, y, alpha=0.1):
        """MixUpå¢å¼º"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        
        batch_size = x.size()[0]
        index = torch.randperm(batch_size).to(x.device)
        
        mixed_x = lam * x + (1 - lam) * x[index]
        y_a, y_b = y, y[index]
        return mixed_x, y_a, y_b, lam
    
    def rand_bbox(self, size, lam):
        """ç”Ÿæˆéšæœºè£å‰ªåŒºåŸŸ"""
        W = size[2]
        H = size[3]
        cut_rat = np.sqrt(1. - lam)
        cut_w = int(W * cut_rat)
        cut_h = int(H * cut_rat)

        cx = np.random.randint(W)
        cy = np.random.randint(H)

        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)

        return bbx1, bby1, bbx2, bby2


class StabilizedLossFunction:
    """ç¨³å®šæ€§æŸå¤±å‡½æ•°"""
    def __init__(self, class_weights=None, smoothing=0.1):
        self.class_weights = class_weights
        self.smoothing = smoothing
        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)
        
    def __call__(self, outputs, targets, augmentation_type='none'):
        # åŸºç¡€äº¤å‰ç†µæŸå¤±
        base_loss = self.ce_loss(outputs, targets)
        
        # æ·»åŠ æ ‡ç­¾å¹³æ»‘
        if self.smoothing > 0:
            confidence = 1.0 - self.smoothing
            logprobs = F.log_softmax(outputs, dim=-1)
            nll_loss = -logprobs.gather(dim=-1, index=targets.unsqueeze(1))
            nll_loss = nll_loss.squeeze(1)
            smooth_loss = -logprobs.mean(dim=-1)
            smooth_loss = confidence * nll_loss + self.smoothing * smooth_loss
            base_loss = smooth_loss.mean()
        
        return base_loss


class StabilizedWeightAdjuster:
    """ç¨³å®šæ€§æƒé‡è°ƒæ•´å™¨"""
    def __init__(self, base_weights, max_adjustment=1.5):
        self.base_weights = base_weights.clone()
        self.max_adjustment = max_adjustment
        self.previous_acc = None
        
    def update_weights(self, current_acc, previous_acc):
        """åŸºäºå‡†ç¡®ç‡å˜åŒ–è°ƒæ•´æƒé‡ï¼ˆæ›´ç¨³å®šï¼‰"""
        if previous_acc is None or current_acc is None:
            return self.base_weights.clone()
            
        new_weights = self.base_weights.clone()
        improvements = []
        
        for i in range(len(current_acc)):
            improvement = current_acc[i] - previous_acc[i]
            improvements.append((i, improvement))
        
        # æ‰¾å‡ºè¡¨ç°æœ€å·®çš„3ä¸ªç±»åˆ«
        worst_classes = sorted(improvements, key=lambda x: x[1])[:3]
        
        # åªå¯¹æœ€å·®çš„ç±»åˆ«è¿›è¡Œé€‚åº¦è°ƒæ•´
        for idx, improvement in worst_classes:
            if improvement < 0:  # å‡†ç¡®ç‡ä¸‹é™
                adjustment = min(1.0 + abs(improvement) * 5, self.max_adjustment)
                new_weights[idx] *= adjustment
        
        return new_weights


# === æ¨¡å‹è®­ç»ƒä¸»å¾ªç¯ ===
def main():
    print("å¼€å§‹è¿›å…¥è®­ç»ƒ - ç¨³å®šæ€§ä¼˜åŒ–ç‰ˆæœ¬")
    print("=" * 60)
    
    # åˆå§‹åŒ–è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç¨³å®šæ€§æ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),  # å›ºå®šå°ºå¯¸
        transforms.Grayscale(num_output_channels=3),
        
        # æ¸©å’Œçš„ç©ºé—´å˜æ¢
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),
        
        # æ¸©å’Œçš„é¢œè‰²å˜æ¢
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
        
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        
        # é™ä½é®æŒ¡æ¦‚ç‡
        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“ åŠ è½½æ•°æ®é›†...")
    train_dir = './data/train'
    test_dir = './data/test'
    
    if not os.path.exists(train_dir):
        raise FileNotFoundError(f"âŒ è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {train_dir}")
    if not os.path.exists(test_dir):
        raise FileNotFoundError(f"âŒ æµ‹è¯•æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {test_dir}")
    
    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    val_dataset = datasets.ImageFolder(test_dir, transform=val_transform)
    
    # è·å–è®­ç»ƒæ ‡ç­¾è®¡ç®—ç±»åˆ«æƒé‡
    train_labels = [label for _, label in train_dataset]
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    print(f"  ç±»åˆ«æ•°é‡: {len(train_dataset.classes)}")
    print(f"  ç±»åˆ«åç§°: {train_dataset.classes}")
    
    # æ•°æ®åŠ è½½å™¨
    num_workers = min(8, os.cpu_count())
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], 
                             shuffle=True, num_workers=num_workers, 
                             pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], 
                           shuffle=False, num_workers=num_workers, pin_memory=True)
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ”„ åˆ›å»ºæ¨¡å‹...")
    def create_stabilized_model(model_size='base', num_classes=7, weights_dir='./weights'):
        """åˆ›å»ºç¨³å®šæ€§ä¼˜åŒ–æ¨¡å‹"""
        model_configs = {
            'base': {
                'name': 'vit_base_patch16_224',
                'local_file': 'vit_base_patch16_224.pth',
            }
        }
        
        config = model_configs[model_size]
        local_weight_path = os.path.join(weights_dir, config['local_file'])
        
        if not os.path.exists(local_weight_path):
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {local_weight_path}")
        
        print(f"ğŸ”„ ä»æœ¬åœ°åŠ è½½é¢„è®­ç»ƒæƒé‡: {local_weight_path}")
        
        # åˆ›å»ºæ¨¡å‹
        model = timm.create_model(config['name'], pretrained=False, num_classes=num_classes)
        
        try:
            checkpoint = torch.load(local_weight_path, map_location='cpu')
            state_dict = checkpoint
            
            if 'model_state_dict' in state_dict:
                state_dict = state_dict['model_state_dict']
            elif 'state_dict' in state_dict:
                state_dict = state_dict['state_dict']
            elif 'model' in state_dict:
                state_dict = state_dict['model']
            
            # è¿‡æ»¤åˆ†ç±»å¤´æƒé‡
            filtered_state_dict = {}
            for key, value in state_dict.items():
                if not key.startswith('head.') and not key.startswith('fc.'):
                    filtered_state_dict[key] = value
            
            # åŠ è½½æƒé‡
            missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
            print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹...")
        
        return model
    
    model = create_stabilized_model(config['model_size'], num_classes=7)
    model = model.to(device)
    print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # ä¼˜åŒ–å™¨é…ç½®
    print("âš™ï¸ é…ç½®ä¼˜åŒ–å™¨...")
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦ - ä½¿ç”¨æ›´ç¨³å®šçš„ä½™å¼¦é€€ç«
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config['num_epochs'], eta_min=1e-7
    )
    
    # æŸå¤±å‡½æ•°
    criterion = StabilizedLossFunction(class_weights=class_weights, smoothing=config['label_smoothing'])
    
    # æ•°æ®å¢å¼º
    data_aug = StabilizedDataAugmentation(
        cutmix_prob=config['cutmix_alpha'],
        mixup_prob=0.2
    )
    
    # æƒé‡è°ƒæ•´å™¨
    weight_adjuster = StabilizedWeightAdjuster(class_weights, max_adjustment=1.5)
    
    # æ—©åœæœºåˆ¶
    early_stopping = StabilizedEarlyStopping(
        patience=config['patience'],
        delta=0.001,
        min_epochs=5
    )
    
    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    def load_checkpoint(model, optimizer, filepath):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(filepath):
            print(f"ğŸ”„ åŠ è½½æ£€æŸ¥ç‚¹: {filepath}")
            checkpoint = torch.load(filepath, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint.get('epoch', 0) + 1
            best_acc = checkpoint.get('best_acc', 0)
            return start_epoch, best_acc
        return 0, config['best_acc']
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint_path = 'best_vit_base_targeted.pth'
    start_epoch, best_acc = load_checkpoint(model, optimizer, checkpoint_path)
    if start_epoch > 0:
        print(f"âœ… ä»ç¬¬{start_epoch}ä¸ªepochæ¢å¤è®­ç»ƒï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}%")
    else:
        start_epoch = config['current_epoch']
        print(f"ğŸ”„ ä»ç¬¬{start_epoch}ä¸ªepochå¼€å§‹è®­ç»ƒ")
    
    print(f"ğŸ¯ ç¨³å®šæ€§ä¼˜åŒ–é…ç½®:")
    print(f"  å­¦ä¹ ç‡: {config['learning_rate']:.1e} (é™ä½)")
    print(f"  CutMixæ¦‚ç‡: {data_aug.cutmix_prob} (é™ä½)")
    print(f"  MixUpæ¦‚ç‡: {data_aug.mixup_prob} (é™ä½)")
    print(f"  æ—©åœè€å¿ƒå€¼: {config['patience']}")
    
    # è®­ç»ƒå‡½æ•°
    def train_epoch(model, train_loader, criterion, optimizer, device, 
                   grad_accum_steps=2, epoch=0, previous_class_acc=None):
        """ç¨³å®šæ€§è®­ç»ƒå‡½æ•°"""
        model.train()
        total_loss = 0
        optimizer.zero_grad()
        
        for i, (images, labels) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}")):
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)
            
            # åº”ç”¨æ•°æ®å¢å¼º
            aug_images, targets_a, targets_b, lam, aug_type = data_aug.apply_augmentation(images, labels, epoch)
            
            outputs = model(aug_images)
            
            # æ ¹æ®å¢å¼ºç±»å‹è®¡ç®—æŸå¤±
            if aug_type == 'cutmix':
                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)
            elif aug_type == 'mixup':
                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)
            else:
                loss = criterion(outputs, labels)
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / grad_accum_steps
            loss.backward()
            
            if (i + 1) % grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()
            
            total_loss += loss.item() * grad_accum_steps
        
        return total_loss / len(train_loader)
    
    def validate(model, val_loader, criterion, device):
        """éªŒè¯æ¨¡å‹"""
        model.eval()
        total_loss = 0
        correct = 0
        total = 0
        class_correct = [0] * 7
        class_total = [0] * 7
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc="éªŒè¯ä¸­", leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                total_loss += loss.item()
                
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                for i in range(labels.size(0)):
                    label = labels[i]
                    class_correct[label] += (predicted[i] == label).item()
                    class_total[label] += 1
        
        acc = correct / total
        avg_loss = total_loss / len(val_loader)
        
        # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
        class_acc = []
        for i in range(7):
            if class_total[i] > 0:
                class_acc.append(class_correct[i] / class_total[i])
            else:
                class_acc.append(0.0)
        
        return acc, avg_loss, class_acc
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹ç¨³å®šæ€§ä¼˜åŒ–è®­ç»ƒ (ä»ç¬¬{start_epoch+1}ä¸ªepochå¼€å§‹)")
    print("=" * 60)
    
    training_history = {
        'train_loss': [], 'val_acc': [], 'val_loss': [], 'learning_rates': [], 'class_acc': []
    }
    
    previous_class_acc = None
    
    for epoch in range(start_epoch, config['num_epochs']):
        epoch_start = time.time()
        print(f"\nğŸ“Š Epoch [{epoch+1}/{config['num_epochs']}]")
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, 
                               config['grad_accum_steps'], epoch, previous_class_acc)
        training_history['train_loss'].append(train_loss)
        
        # éªŒè¯
        val_acc, val_loss, class_acc = validate(model, val_loader, criterion, device)
        training_history['val_acc'].append(val_acc)
        training_history['val_loss'].append(val_loss)
        training_history['class_acc'].append(class_acc)
        training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # æ‰“å°ç»“æœ
        current_lr = optimizer.param_groups[0]['lr']
        epoch_time = time.time() - epoch_start
        
        print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"ğŸ¯ éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.2f}% | éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"ğŸ’¡ å­¦ä¹ ç‡: {current_lr:.2e}")
        print(f"â±ï¸  æœ¬è½®è€—æ—¶: {epoch_time:.1f}ç§’")
        
        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        print("ğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡:")
        for i, cls_name in enumerate(val_dataset.classes):
            if previous_class_acc and i < len(previous_class_acc):
                improvement = class_acc[i] - previous_class_acc[i]
                arrow = "â†‘" if improvement > 0 else "â†“" if improvement < 0 else "â†’"
                print(f"  {cls_name}: {class_acc[i]*100:5.1f}% {arrow}")
            else:
                print(f"  {cls_name}: {class_acc[i]*100:5.1f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc + 0.001:
            best_acc = val_acc
            best_model_path = f'best_vit_{config["model_size"]}_stable.pth'
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_acc': best_acc,
                'config': config,
                'class_acc': class_acc
            }, best_model_path)
            print(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}% -> {best_model_path}")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_acc, {k: v.clone() for k, v in model.state_dict().items()}):
            print("ğŸ›‘ æ—©åœè§¦å‘ï¼Œè®­ç»ƒç»“æŸ")
            # å¦‚æœæ—©åœï¼Œæ¢å¤æœ€ä½³æƒé‡
            if early_stopping.best_weights is not None:
                model.load_state_dict(early_stopping.best_weights)
            break
        
        # æ›´æ–°æƒé‡è°ƒæ•´
        if previous_class_acc is not None:
            class_weights = weight_adjuster.update_weights(class_acc, previous_class_acc)
            criterion.class_weights = class_weights
        
        previous_class_acc = class_acc
        
        print("-" * 60)
        
        # æ¯3ä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint
        if (epoch + 1) % 3 == 0:
            checkpoint_path = f'checkpoint_epoch_{epoch+1}.pth'
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_acc': best_acc,
                'training_history': training_history,
                'config': config
            }, checkpoint_path)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # è®­ç»ƒæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ¯ è®­ç»ƒæ€»ç»“")
    print("=" * 60)
    print(f"ğŸ“Š æœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}%")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹: best_vit_{config['model_size']}_stable.pth")
    print(f"ğŸ”„ æ€»è®­ç»ƒè½®æ¬¡: {epoch+1}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(training_history['train_loss'], label='è®­ç»ƒæŸå¤±')
        plt.plot(training_history['val_loss'], label='éªŒè¯æŸå¤±')
        plt.legend()
        plt.title('æŸå¤±æ›²çº¿')
        
        plt.subplot(1, 2, 2)
        plt.plot(training_history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', color='green')
        plt.axhline(y=best_acc, color='r', linestyle='--', label=f'æœ€ä½³: {best_acc*100:.1f}%')
        plt.legend()
        plt.title('å‡†ç¡®ç‡æ›²çº¿')
        
        plt.tight_layout()
        plt.savefig('training_curves_stable.png', dpi=300, bbox_inches='tight')
        print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: training_curves_stable.png")
    except:
        print("âš ï¸ æ— æ³•ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼Œè¯·å®‰è£…matplotlib")
    
    # æœ€ç»ˆéªŒè¯
    print("\nğŸ” æœ€ç»ˆæ¨¡å‹éªŒè¯...")
    try:
        checkpoint = torch.load(f'best_vit_{config["model_size"]}_stable.pth')
        model.load_state_dict(checkpoint['model_state_dict'])
        final_acc, final_loss, class_acc = validate(model, val_loader, criterion, device)
        
        print(f"âœ… æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_acc*100:.2f}%")
        print("ğŸ“Š å„ç±»åˆ«æœ€ç»ˆå‡†ç¡®ç‡:")
        for i, cls_name in enumerate(val_dataset.classes):
            print(f"  {cls_name}: {class_acc[i]*100:5.1f}%")
        
        if final_acc >= 0.8:
            print("ğŸ‰ æ­å–œï¼è¾¾åˆ°80%å‡†ç¡®ç‡ç›®æ ‡ï¼")
        elif final_acc >= 0.75:
            print("âœ… ä¼˜ç§€ï¼å‡†ç¡®ç‡ > 75%")
        elif final_acc >= 0.7:
            print("ğŸ“ˆ è‰¯å¥½ï¼å‡†ç¡®ç‡ > 70%")
        else:
            print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    except Exception as e:
        print(f"âŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}")


if __name__ == '__main__':
    main()
```

## ğŸ”§ **ç¨³å®šæ€§ä¼˜åŒ–çš„å…³é”®æ”¹è¿›**

### 1. **é™ä½å­¦ä¹ ç‡**
```python
'learning_rate': 1e-5,  # ä»4.5e-5å¤§å¹…é™ä½
```

### 2. **å‡å°‘æ•°æ®å¢å¼ºå¼ºåº¦**
```python
class StabilizedDataAugmentation:
    def __init__(self, cutmix_prob=0.3, mixup_prob=0.2):  # å¤§å¹…é™ä½
```

### 3. **æ›´æ¸©å’Œçš„æƒé‡è°ƒæ•´**
```python
class StabilizedWeightAdjuster:
    def __init__(self, base_weights, max_adjustment=1.5):  # é™åˆ¶æœ€å¤§è°ƒæ•´å¹…åº¦
```

### 4. **æ”¹è¿›çš„æ—©åœæœºåˆ¶**
```python
class StabilizedEarlyStopping:
    def __init__(self, patience=8, delta=0.001):  # å¢åŠ è€å¿ƒå€¼
```

## ğŸ¯ **é¢„æœŸæ•ˆæœ**

åŸºäºå½“å‰69.56%çš„æœ€ä½³ç»“æœï¼Œç¨³å®šæ€§ä¼˜åŒ–ç‰ˆæœ¬é¢„æœŸï¼š

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | é¢„æœŸç¨³å®šæ€§ä¼˜åŒ–å |
|------|--------|------------------|
| **å‡†ç¡®ç‡æ³¢åŠ¨** | Â±3% | Â±1% |
| **å„ç±»åˆ«ç¨³å®šæ€§** | å·® | è‰¯å¥½ |
| **æ”¶æ•›é€Ÿåº¦** | å¿«ä½†ä¸ç¨³å®š | æ…¢ä½†ç¨³å®š |
| **æœ€ç»ˆå‡†ç¡®ç‡** | 69-70% | 70-72% |

## ğŸš€ **ä½¿ç”¨è¯´æ˜**

ç›´æ¥è¿è¡Œç¨³å®šæ€§ä¼˜åŒ–ç‰ˆæœ¬ï¼š
```bash
python train_stable.py
```

**è¿™ä¸ªç‰ˆæœ¬åº”è¯¥èƒ½å¤Ÿæ˜¾è‘—å‡å°‘è®­ç»ƒæ³¢åŠ¨ï¼Œä½¿å„ç±»åˆ«è¡¨ç°æ›´åŠ ç¨³å®šï¼Œæœ‰æœ›åœ¨ç¨³å®šåŸºç¡€ä¸Šå®ç°å°å¹…æå‡ï¼**