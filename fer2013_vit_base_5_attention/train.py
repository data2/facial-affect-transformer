# train_3090_with_attention_optimized_complete.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
import timm
import numpy as np
from tqdm import tqdm
import time
import os
import math
import random
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt


# ==================== 3090ä¼˜åŒ–é…ç½® (å†…å­˜<18G) - å®Œæ•´ä¼˜åŒ–ç‰ˆ ====================
class Config3090:
    """é’ˆå¯¹NVIDIA RTX 3090çš„ä¼˜åŒ–é…ç½®ï¼Œå†…å­˜æ§åˆ¶åœ¨18GBä»¥ä¸‹"""
    def __init__(self):
        # Basic configuration
        self.model_name = 'vit_base_patch16_224'
        self.num_classes = 7
        self.img_size = 224

        # 3090ä¼˜åŒ–å‚æ•°
        self.batch_size = 32
        self.num_epochs = 80
        self.learning_rate = 2.5e-5
        self.weight_decay = 0.05
        self.warmup_epochs = 10
        
        # Data augmentation (åˆå§‹å€¼)
        self.cutmix_prob = 0.45
        self.mixup_prob = 0.25
        self.cutmix_alpha = 0.7
        self.mixup_alpha = 0.1

        # Regularization
        self.drop_rate = 0.3
        self.label_smoothing = 0.1

        # Optimization strategies
        self.grad_accum_steps = 1
        self.patience = 25
        self.target_acc = 0.74

        # Class weights
        self.class_weights = None
        self.dynamic_weight_adjust = True
        
        # æ³¨æ„åŠ›æœºåˆ¶é…ç½®
        self.use_attention = True
        self.attention_type = 'se'
        self.attention_reduction = 16
        self.hard_class_focus = True
        
        # ç»­è®­é…ç½®
        # self.resume_from = 'checkpoint_3090_epoch_035_with_attention.pth'  # ä»ç¬¬35è½®æ£€æŸ¥ç‚¹ç»§ç»­
        # self.start_epoch = 35  # å·²å®Œæˆçš„è½®æ•°
        # self.resume_optimizer = True
        # self.resume_scheduler = True
        # self.resume_history = True

        # No resume training
        self.resume_from = None
        self.start_epoch = 0
        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Mixed precision
        self.use_amp = True
        self.amp_dtype = torch.float16
        
        # æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
        self.grad_clip = 1.0
        
        # å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
        self.use_warmup = True
        self.min_lr = 1e-6

    def __str__(self):
        info = "=" * 60 + "\n"
        info += "ğŸ¯ 3090ä¼˜åŒ–è®­ç»ƒé…ç½® (å†…å­˜<18GB) - å®Œæ•´ä¼˜åŒ–ç‰ˆ\n"
        info += "=" * 60 + "\n"
        info += f"ğŸ“Š æ¨¡å‹: {self.model_name} + {self.attention_type.upper()}æ³¨æ„åŠ›\n"
        info += f"ğŸ“ˆ ç›®æ ‡å‡†ç¡®ç‡: {self.target_acc*100:.1f}%\n"
        info += f"âš™ï¸  æ‰¹æ¬¡å¤§å°: {self.batch_size}\n"
        info += f"ğŸ“š æ€»è½®æ•°: {self.num_epochs}\n"
        if self.resume_from:
            info += f"ğŸ”„ ç»­è®­ä»: {self.resume_from} (ç¬¬{self.start_epoch}è½®å)\n"
        info += f"ğŸ’¡ å­¦ä¹ ç‡: {self.learning_rate:.1e}\n"
        info += f"ğŸ”„ çƒ­èº«è½®æ•°: {self.warmup_epochs}\n"
        info += f"ğŸ¨ å¢å¼ºç­–ç•¥: CutMix({self.cutmix_prob}), MixUp({self.mixup_prob})\n"
        info += f"ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶: {self.attention_type.upper()} (å¯ç”¨)\n"
        info += f"âš¡ æ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if self.use_amp else 'ç¦ç”¨'}\n"
        info += f"ğŸ’» è®­ç»ƒè®¾å¤‡: {self.device}\n"
        info += f"ğŸ“Š GPUæ˜¾å­˜: 24GB RTX 3090\n"
        info += "=" * 60
        return info


# ==================== Enhanced Data Augmentation ====================
class EnhancedAugmentation:
    """å¢å¼ºçš„æ•°æ®å¢å¼ºç­–ç•¥"""
    def __init__(self, config):
        self.config = config
        self.epoch = 0
        self.adaptive_aug_strength = 1.0  # è‡ªé€‚åº”å¢å¼ºå¼ºåº¦

    def get_train_transform(self):
        """è·å–è®­ç»ƒæ•°æ®å¢å¼º"""
        return transforms.Compose([
            transforms.RandomResizedCrop(self.config.img_size, scale=(0.7, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=20),
            transforms.RandomAffine(
                degrees=15,
                translate=(0.1, 0.1),
                scale=(0.85, 1.15),
                shear=10
            ),
            transforms.Grayscale(num_output_channels=3),
            transforms.ColorJitter(
                brightness=0.4,
                contrast=0.4,
                saturation=0.4,
                hue=0.1
            ),
            transforms.GaussianBlur(kernel_size=7, sigma=(0.1, 3.0)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            transforms.RandomErasing(
                p=0.5,
                scale=(0.02, 0.2),
                ratio=(0.3, 3.3)
            ),
        ])

    def get_val_transform(self):
        """è·å–éªŒè¯æ•°æ®å¢å¼º"""
        return transforms.Compose([
            transforms.Resize((self.config.img_size, self.config.img_size)),
            transforms.Grayscale(num_output_channels=3),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    def cutmix(self, x, y, alpha=1.0):
        """CutMixå¢å¼º"""
        if alpha <= 0:
            return x, y, y, 1.0

        lam = np.random.beta(alpha, alpha)
        batch_size = x.size()[0]
        index = torch.randperm(batch_size).to(x.device)

        H, W = x.shape[2], x.shape[3]
        cut_rat = np.sqrt(1. - lam)
        cut_w = int(W * cut_rat)
        cut_h = int(H * cut_rat)

        cx = np.random.randint(W)
        cy = np.random.randint(H)

        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)

        x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]

        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
        y_a, y_b = y, y[index]

        return x, y_a, y_b, lam

    def mixup(self, x, y, alpha=0.2):
        """MixUpå¢å¼º"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1

        batch_size = x.size()[0]
        index = torch.randperm(batch_size).to(x.device)

        mixed_x = lam * x + (1 - lam) * x[index]
        y_a, y_b = y, y[index]
        return mixed_x, y_a, y_b, lam

    def apply_augmentation(self, images, labels, epoch):
        """åº”ç”¨æ•°æ®å¢å¼º - è‡ªé€‚åº”è°ƒæ•´å¼ºåº¦"""
        self.epoch = epoch
        
        # è‡ªé€‚åº”è°ƒæ•´å¢å¼ºå¼ºåº¦
        if epoch >= 35:
            # ç¬¬36è½®åé€æ¸é™ä½å¢å¼ºå¼ºåº¦
            reduction_factor = max(0.5, 1.0 - (epoch - 35) * 0.02)
            self.adaptive_aug_strength = reduction_factor
        
        r = np.random.rand()

        # æ ¹æ®è‡ªé€‚åº”å¼ºåº¦è°ƒæ•´æ¦‚ç‡
        effective_cutmix_prob = self.config.cutmix_prob * self.adaptive_aug_strength
        effective_mixup_prob = self.config.mixup_prob * self.adaptive_aug_strength

        if r < effective_cutmix_prob:
            images, targets_a, targets_b, lam = self.cutmix(
                images, labels, self.config.cutmix_alpha * self.adaptive_aug_strength
            )
            return images, targets_a, targets_b, lam, 'cutmix'

        elif r < effective_cutmix_prob + effective_mixup_prob:
            images, targets_a, targets_b, lam = self.mixup(
                images, labels, self.config.mixup_alpha * self.adaptive_aug_strength
            )
            return images, targets_a, targets_b, lam, 'mixup'

        else:
            return self.apply_medium_augmentation(images, labels)

    def apply_medium_augmentation(self, images, labels):
        """ä¸­ç­‰å¼ºåº¦å¢å¼º"""
        batch_size = images.size(0)
        
        # éšæœºè£å‰ª
        if np.random.rand() < 0.7:
            scale = np.random.uniform(0.85, 1.0)
            H, W = images.shape[2], images.shape[3]
            new_H, new_W = int(H * scale), int(W * scale)
            
            top = np.random.randint(0, H - new_H) if H > new_H else 0
            left = np.random.randint(0, W - new_W) if W > new_W else 0
            images = images[:, :, top:top+new_H, left:left+new_W]
            images = F.interpolate(images, size=(H, W), mode='bilinear')
        
        # æ°´å¹³ç¿»è½¬
        if torch.rand(1).item() < 0.5:
            images = torch.flip(images, [3])
        
        # é¢œè‰²æ‰°åŠ¨
        brightness = torch.rand(batch_size, 1, 1, 1).to(images.device) * 0.3 + 0.85
        contrast = torch.rand(batch_size, 1, 1, 1).to(images.device) * 0.3 + 0.85
        
        images = images * brightness
        mean = images.mean(dim=[1,2,3], keepdim=True)
        images = (images - mean) * contrast + mean
        images = torch.clamp(images, 0, 1)
        
        # è½»å¾®æ—‹è½¬
        if np.random.rand() < 0.3:
            angle = np.random.uniform(-10, 10)
            images = transforms.functional.rotate(images, angle)
        
        return images, labels, labels, 1.0, 'medium'


# ==================== Enhanced Attention Module ====================
class EnhancedSEAttention(nn.Module):
    """
    å¢å¼ºçš„Squeeze-and-Excitationæ³¨æ„åŠ›æ¨¡å—
    """
    def __init__(self, channel, reduction=16, num_classes=7):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        
        # å¢å¼ºçš„MLPå±‚
        self.mlp = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
        
        # ç±»åˆ«æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆæ–°å¢ï¼‰
        self.class_aware_attention = nn.Sequential(
            nn.Linear(channel, channel // 4),
            nn.ReLU(),
            nn.Linear(channel // 4, num_classes),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, x, labels=None):
        # å…¨å±€å¹³å‡å’Œæœ€å¤§æ± åŒ–
        if x.dim() == 3:
            avg_out = self.mlp(self.avg_pool(x.transpose(1, 2)).squeeze(-1))
            max_out = self.mlp(self.max_pool(x.transpose(1, 2)).squeeze(-1))
        else:
            avg_out = self.mlp(self.avg_pool(x.unsqueeze(-1)).squeeze(-1))
            max_out = self.mlp(self.max_pool(x.unsqueeze(-1)).squeeze(-1))
        
        # é€šé“æ³¨æ„åŠ›æƒé‡
        attention = (avg_out + max_out) / 2.0
        
        if x.dim() == 3:
            attended_features = x * attention.unsqueeze(1)
        else:
            attended_features = x * attention
        
        # ç±»åˆ«æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆè®­ç»ƒæ—¶ï¼‰
        if labels is not None and self.training:
            class_attention = self.class_aware_attention(
                attended_features.mean(dim=1) if attended_features.dim() == 3 else attended_features
            )
            return attended_features, class_attention
        
        return attended_features


# ==================== Adaptive Loss Function ====================
# ==================== Adaptive Loss Function - ä¿®å¤ç‰ˆ ====================
class AdaptiveLossFunction:
    """è‡ªé€‚åº”æŸå¤±å‡½æ•° - ä¿®å¤ç‰ˆ"""
    def __init__(self, class_weights, config):
        self.class_weights = class_weights
        self.config = config
        self.label_smoothing = config.label_smoothing
        self.epoch = 0
        self.training = True  # æ·»åŠ trainingå±æ€§
        
        # åŸºç¡€æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)
        self.focal_loss = FocalLoss(gamma=2.0, alpha=class_weights)
        
        # ç±»åˆ«å›°éš¾åº¦è·Ÿè¸ª
        self.class_difficulty = torch.zeros(config.num_classes)
        self.class_samples = torch.zeros(config.num_classes)
    
    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.training = True
        
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.training = False
        
    def update_class_difficulty(self, outputs, targets):
        """æ›´æ–°ç±»åˆ«å›°éš¾åº¦"""
        with torch.no_grad():
            probs = F.softmax(outputs, dim=1)
            pred_probs, preds = torch.max(probs, dim=1)
            
            for i in range(len(targets)):
                cls_idx = targets[i].item()
                if preds[i] == targets[i]:
                    # æ­£ç¡®é¢„æµ‹ï¼Œéš¾åº¦é™ä½ï¼ˆç½®ä¿¡åº¦è¶Šé«˜ï¼Œéš¾åº¦é™ä½è¶Šå¤šï¼‰
                    self.class_difficulty[cls_idx] += pred_probs[i].item()
                else:
                    # é”™è¯¯é¢„æµ‹ï¼Œéš¾åº¦å¢åŠ ï¼ˆç½®ä¿¡åº¦è¶Šä½ï¼Œéš¾åº¦å¢åŠ è¶Šå¤šï¼‰
                    self.class_difficulty[cls_idx] -= (1 - pred_probs[i].item())
                self.class_samples[cls_idx] += 1
    
    def __call__(self, outputs, targets, augmentation_type='none', lam=1.0):
        # åŸºç¡€äº¤å‰ç†µæŸå¤±
        if self.label_smoothing > 0:
            ce_loss = self.label_smooth_ce(outputs, targets)
        else:
            ce_loss = self.ce_loss(outputs, targets)
        
        # åªåœ¨è®­ç»ƒæ—¶æ›´æ–°ç±»åˆ«å›°éš¾åº¦
        if self.training:
            self.update_class_difficulty(outputs, targets)
        
        # è‡ªé€‚åº”Focal Lossæƒé‡
        fear_mask = (targets == 2)  # fear
        disgust_mask = (targets == 1)  # disgust
        sad_mask = (targets == 5)  # sad

        if fear_mask.any() or disgust_mask.any() or sad_mask.any():
            # åŠ¨æ€è°ƒæ•´Focal Lossæƒé‡ï¼ˆåæœŸå¢åŠ æƒé‡ï¼‰
            if self.epoch > 35:  # ç¬¬36è½®å
                focal_weight = 0.5  # å¢åŠ æƒé‡
            else:
                focal_weight = 0.3
            
            focal_component = self.focal_loss(outputs, targets)
            
            # æ ¹æ®ç±»åˆ«å›°éš¾åº¦è°ƒæ•´æŸå¤±ï¼ˆä»…è®­ç»ƒæ—¶ä¸”ç¬¬30è½®åï¼‰
            if self.training and self.epoch > 30:
                avg_difficulty = self.class_difficulty / (self.class_samples + 1e-8)
                
                # ç‰¹åˆ«å…³æ³¨å›°éš¾ç±»åˆ«
                for i in range(len(targets)):
                    if targets[i] == 2 or targets[i] == 5:  # fearæˆ–sad
                        difficulty = avg_difficulty[targets[i]]
                        if difficulty < 0.3:  # éå¸¸å›°éš¾çš„æ ·æœ¬
                            focal_component = focal_component * 2.0
                        elif difficulty < 0.5:  # ä¸­ç­‰å›°éš¾çš„æ ·æœ¬
                            focal_component = focal_component * 1.5
            
            total_loss = (1 - focal_weight) * ce_loss + focal_weight * focal_component
        else:
            total_loss = ce_loss

        return total_loss
    
    def label_smooth_ce(self, x, target):
        """æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µ"""
        confidence = 1.0 - self.label_smoothing
        
        if target.dim() > 1:
            target = target.squeeze()
        
        logprobs = F.log_softmax(x, dim=-1)
        
        if target.dim() == 1:
            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
            nll_loss = nll_loss.squeeze(1)
        else:
            nll_loss = -torch.sum(logprobs * target, dim=-1)
        
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.label_smoothing * smooth_loss
        return loss.mean()


class FocalLoss(nn.Module):
    """Focal Loss"""
    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


# ==================== Advanced Dynamic Weight Adjuster ====================
class AdvancedWeightAdjuster:
    """é«˜çº§åŠ¨æ€æƒé‡è°ƒæ•´å™¨"""
    def __init__(self, base_weights, history_size=5):
        self.base_weights = base_weights.clone()
        self.history_size = history_size
        self.history = []
        self.performance_history = []
        self.improvement_threshold = 0.01  # æ”¹è¿›é˜ˆå€¼

    def update_weights(self, class_accuracies, epoch):
        """æ ¹æ®ç±»åˆ«æ€§èƒ½è°ƒæ•´æƒé‡"""
        if len(self.history) >= self.history_size:
            self.history.pop(0)

        self.history.append(class_accuracies)
        self.performance_history.append({
            'epoch': epoch,
            'accuracies': class_accuracies
        })

        if len(self.history) < 2:
            return self.base_weights

        new_weights = self.base_weights.clone()

        # è®¡ç®—è¿‘æœŸå¹³å‡å‡†ç¡®ç‡
        recent_acc = np.mean(self.history[-2:], axis=0)

        for i in range(len(class_accuracies)):
            current_acc = class_accuracies[i]
            recent_avg = recent_acc[i] if i < len(recent_acc) else 0
            improvement = current_acc - recent_avg

            # ç‰¹åˆ«å¤„ç†å›°éš¾ç±»åˆ«
            if i == 2 or i == 5:  # fearå’Œsad
                if current_acc < 0.50:  # è¡¨ç°å¾ˆå·®
                    if improvement < -0.05:  # å¤§å¹…ä¸‹é™
                        new_weights[i] *= 3.0  # æ˜¾è‘—å¢åŠ æƒé‡
                        print(f"    âš ï¸  {['fear','sad'][i==5]}: å¤§å¹…ä¸‹é™({improvement:.3f})ï¼Œæƒé‡Ã—3.0")
                    elif improvement < 0:  # è½»å¾®ä¸‹é™
                        new_weights[i] *= 2.0
                    else:  # æ²¡æœ‰æ”¹è¿›
                        new_weights[i] *= 1.5
                elif 0.50 <= current_acc < 0.65:  # ä¸­ç­‰è¡¨ç°
                    if improvement < -0.03:  # ä¸‹é™
                        new_weights[i] *= 1.8
                    elif improvement < 0.02:  # åœæ»
                        new_weights[i] *= 1.3
                    else:  # æ”¹è¿›
                        new_weights[i] *= 1.1
                elif current_acc >= 0.65:  # è¡¨ç°è‰¯å¥½
                    new_weights[i] *= 0.9  # é™ä½æƒé‡

            # å…¶ä»–ç±»åˆ«
            else:
                if current_acc < 0.55 and improvement < -0.04:
                    new_weights[i] *= 1.8
                elif 0.55 <= current_acc < 0.75 and abs(improvement) < 0.02:
                    new_weights[i] *= 1.2
                elif current_acc > 0.80:
                    new_weights[i] *= 0.7

        return new_weights


# ==================== Adaptive Early Stopping ====================
class AdaptiveEarlyStopping:
    """è‡ªé€‚åº”æ—©åœæœºåˆ¶"""
    def __init__(self, patience=15, min_epochs=30, target_acc=0.75):
        self.patience = patience
        self.original_patience = patience
        self.min_epochs = min_epochs
        self.target_acc = target_acc
        self.best_acc = 0
        self.best_epoch = 0
        self.counter = 0
        self.early_stop = False
        self.improvement_history = []
        
    def __call__(self, val_acc, epoch, train_acc=None):
        if epoch < self.min_epochs:
            return False
        
        # è®¡ç®—æ”¹è¿›
        improvement = val_acc - self.best_acc
        self.improvement_history.append(improvement)
        
        # è‡ªé€‚åº”è°ƒæ•´è€å¿ƒå€¼
        if val_acc > self.target_acc - 0.02:  # æ¥è¿‘ç›®æ ‡
            self.patience = max(self.original_patience, 20)
        elif val_acc < self.target_acc - 0.05:  # è¿œç¦»ç›®æ ‡
            self.patience = min(self.original_patience, 10)
        
        if val_acc > self.best_acc + 1e-4:
            self.best_acc = val_acc
            self.best_epoch = epoch
            self.counter = 0
            print(f"    âœ… å‡†ç¡®ç‡æå‡: +{improvement*100:.3f}%")
        else:
            self.counter += 1
            if improvement < -0.02:  # æ˜¾è‘—ä¸‹é™
                self.counter += 1  # åŠ é€Ÿæ—©åœ
                print(f"    âš ï¸  å‡†ç¡®ç‡ä¸‹é™: {improvement*100:.3f}%ï¼ŒåŠ é€Ÿæ—©åœ")
        
        if self.counter >= self.patience:
            self.early_stop = True
            print(f"    ğŸ›‘ æ—©åœè§¦å‘: {self.counter}è½®æ— æ”¹è¿›")
        
        return self.early_stop


# ==================== Model Creation with Enhanced Attention ====================
def create_enhanced_model(config, pretrained_path='./weights/vit_base_patch16_224.pth'):
    """åˆ›å»ºå¢å¼ºçš„æ¨¡å‹ - é›†æˆæ”¹è¿›çš„æ³¨æ„åŠ›æœºåˆ¶"""
    print(f"ğŸ”„ åˆ›å»ºå¢å¼ºæ¨¡å‹: {config.model_name} (å¸¦{config.attention_type.upper()}æ³¨æ„åŠ›)")
    
    # åˆ›å»ºåŸºç¡€ViTæ¨¡å‹
    model = timm.create_model(
        config.model_name,
        pretrained=False,
        num_classes=config.num_classes,
        drop_rate=config.drop_rate
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if os.path.exists(pretrained_path):
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
        try:
            checkpoint = torch.load(pretrained_path, map_location='cpu')
            state_dict = checkpoint

            if 'model_state_dict' in state_dict:
                state_dict = state_dict['model_state_dict']
            elif 'state_dict' in state_dict:
                state_dict = state_dict['state_dict']
            elif 'model' in state_dict:
                state_dict = state_dict['model']

            filtered_state_dict = {}
            for key, value in state_dict.items():
                if not key.startswith('head.') and not key.startswith('fc.'):
                    filtered_state_dict[key] = value

            missing_keys, unexpected_keys = model.load_state_dict(
                filtered_state_dict, strict=False
            )

            if missing_keys:
                print(f"âš ï¸  ç¼ºå¤±çš„é”®: {len(missing_keys)}ä¸ª")
                for i, key in enumerate(missing_keys[:3]):
                    print(f"    {i+1}. {key}")
            if unexpected_keys:
                print(f"âš ï¸  æ„å¤–çš„é”®: {len(unexpected_keys)}ä¸ª")

            print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")

        except Exception as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–...")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    # =============== é›†æˆå¢å¼ºçš„æ³¨æ„åŠ›æ¨¡å— ===============
    if config.use_attention:
        print(f"ğŸ¯ é›†æˆå¢å¼ºçš„{config.attention_type.upper()}æ³¨æ„åŠ›æ¨¡å—")
        
        # è·å–åŸå§‹åˆ†ç±»å¤´
        original_head = None
        if hasattr(model, 'head'):
            original_head = model.head
            model.head = nn.Identity()
        elif hasattr(model, 'fc'):
            original_head = model.fc
            model.fc = nn.Identity()
        
        # ç‰¹å¾ç»´åº¦
        if hasattr(model, 'num_features'):
            feature_dim = model.num_features
        else:
            feature_dim = 768
        
        # åˆ›å»ºå¢å¼ºçš„æ³¨æ„åŠ›æ¨¡å—
        attention = EnhancedSEAttention(
            channel=feature_dim, 
            reduction=config.attention_reduction,
            num_classes=config.num_classes
        )
        
        # åˆ›å»ºæ–°çš„åˆ†ç±»å¤´
        if original_head is not None and isinstance(original_head, nn.Linear):
            new_head = nn.Linear(feature_dim, config.num_classes)
            if original_head.weight.shape[0] == config.num_classes and original_head.weight.shape[1] == feature_dim:
                new_head.weight.data.copy_(original_head.weight.data)
                new_head.bias.data.copy_(original_head.bias.data)
                print(f"âœ… å¤ç”¨åŸå§‹åˆ†ç±»å¤´æƒé‡ (ç»´åº¦: {feature_dim}->{config.num_classes})")
            else:
                print(f"âš ï¸  åŸå§‹åˆ†ç±»å¤´ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        else:
            new_head = nn.Linear(feature_dim, config.num_classes)
            print(f"âœ… åˆ›å»ºæ–°çš„åˆ†ç±»å¤´ (ç»´åº¦: {feature_dim}->{config.num_classes})")
        
        # åˆ›å»ºæ–°çš„å‰å‘ä¼ æ’­
        # åœ¨create_enhanced_modelå‡½æ•°ä¸­ï¼Œä¿®å¤å‰å‘ä¼ æ’­å‡½æ•°
        def new_forward(x, labels=None):
            # è·å–ç‰¹å¾
            features = model.forward_features(x)
            
            # å¦‚æœç‰¹å¾æ˜¯3D [batch, num_patches, dim]ï¼Œå–å…¨å±€å¹³å‡
            if features.dim() == 3:
                features = features.mean(dim=1)
            
            # åº”ç”¨æ³¨æ„åŠ›
            if labels is not None and model.training:  # ä½¿ç”¨model.trainingè€Œä¸æ˜¯self.training
                attended_features, class_attention = attention(features, labels)
            else:
                attended_features = attention(features)
            
            # åˆ†ç±»
            output = new_head(attended_features)
            
            if labels is not None and model.training:  # ä½¿ç”¨model.training
                return output, class_attention
            return output
        
        # æ›¿æ¢å‰å‘ä¼ æ’­
        model.forward = new_forward
        
        # ä¿å­˜ç»„ä»¶
        model.attention = attention
        model.new_head = new_head
        model.has_attention = True
        
        print(f"âœ… å¢å¼ºæ³¨æ„åŠ›æ¨¡å—é›†æˆå®Œæˆ (ç‰¹å¾ç»´åº¦: {feature_dim})")
    
    return model.to(config.device)


# ==================== ä¼˜åŒ–åçš„è®­ç»ƒå‡½æ•° ====================
def train_epoch_optimized(model, train_loader, criterion, optimizer, scheduler,
                          data_aug, config, epoch, weight_adjuster=None, scaler=None):
    """é’ˆå¯¹3090ä¼˜åŒ–çš„è®­ç»ƒepoch - å®Œæ•´ä¼˜åŒ–ç‰ˆ"""
    
    # =============== å…³é”®è°ƒæ•´ç‚¹ ===============
    if epoch == 24:  # ç¬¬25è½®
        optimizer.param_groups[0]['lr'] = 2.0e-05
        print(f"ğŸ¯ ç¬¬{epoch+1}è½®ï¼šå­¦ä¹ ç‡è°ƒæ•´ä¸º2.00e-05")
    
    if epoch == 35:  # ç¬¬36è½®ï¼ˆå·²ç»åº”ç”¨ï¼‰
        config.cutmix_prob = 0.35
        config.mixup_prob = 0.15
        optimizer.param_groups[0]['lr'] = 1.8e-05
        if hasattr(model, 'drop_rate'):
            model.drop_rate = 0.4
        print(f"ğŸš¨ ç¬¬{epoch+1}è½®ï¼šç»¼åˆè°ƒæ•´å¯¹æŠ—è¿‡æ‹Ÿåˆ")
    
    # æ–°å¢ï¼šç¬¬40è½®è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå…³é”®è°ƒæ•´ï¼‰
    if epoch == 39:  # ç¬¬40è½®
        # 1. è¿›ä¸€æ­¥é™ä½æ•°æ®å¢å¼ºå¼ºåº¦
        config.cutmix_prob = 0.20
        config.mixup_prob = 0.08
        
        # 2. æ˜¾è‘—é™ä½å­¦ä¹ ç‡ï¼ˆç²¾ç»†è°ƒæ•´é˜¶æ®µï¼‰
        optimizer.param_groups[0]['lr'] = 8.0e-06
        
        # 3. å¢åŠ æ­£åˆ™åŒ–
        if hasattr(model, 'drop_rate'):
            model.drop_rate = 0.5
        
        # 4. å¢åŠ æ ‡ç­¾å¹³æ»‘
        if hasattr(criterion, 'label_smoothing'):
            criterion.label_smoothing = 0.15
        
        print(f"ğŸ”’ ç¬¬{epoch+1}è½®ï¼šå¯åŠ¨ç²¾ç»†è°ƒæ•´é˜¶æ®µ")
        print(f"  ğŸ“‰ æ•°æ®å¢å¼º: CutMix={config.cutmix_prob}, MixUp={config.mixup_prob}")
        print(f"  ğŸ“‰ å­¦ä¹ ç‡: 1.6e-05 â†’ 8.0e-06")
        print(f"  ğŸ”§ Dropout: {model.drop_rate}")
        
        # 5. å†»ç»“éƒ¨åˆ†ç½‘ç»œå±‚ï¼ˆå‡å°‘è¿‡æ‹Ÿåˆï¼‰
        freeze_blocks = ['patch_embed', 'blocks.0', 'blocks.1']
        for name, param in model.named_parameters():
            if any(block in name for block in freeze_blocks):
                param.requires_grad = False
        
        # éªŒè¯å†»ç»“æ•ˆæœ
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  ğŸ” å‚æ•°çŠ¶æ€: {trainable_params:,}/{total_params:,} ({trainable_params/total_params*100:.1f}%)å¯è®­ç»ƒ")
    
    # æ–°å¢ï¼šç¬¬50è½®å­¦ä¹ ç‡å†æ¬¡è¡°å‡
    if epoch == 49:  # ç¬¬50è½®
        optimizer.param_groups[0]['lr'] = 3.0e-06
        print(f"ğŸ¯ ç¬¬{epoch+1}è½®ï¼šè¿›å…¥è¶…ç²¾ç»†è°ƒæ•´é˜¶æ®µï¼Œå­¦ä¹ ç‡: 3.0e-06")
    
    # =============== è®­ç»ƒå¾ªç¯ ===============
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    aug_stats = {'cutmix': 0, 'mixup': 0, 'medium': 0}

    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}")

    for batch_idx, (images, labels) in enumerate(progress_bar):
        images, labels = images.to(config.device), labels.to(config.device)

        # åº”ç”¨æ•°æ®å¢å¼º
        aug_images, targets_a, targets_b, lam, aug_type = data_aug.apply_augmentation(
            images, labels, epoch
        )

        if aug_type not in aug_stats:
            aug_stats[aug_type] = 0
        aug_stats[aug_type] += 1

        # æ··åˆç²¾åº¦è®­ç»ƒ
        if config.use_amp:
            with torch.amp.autocast('cuda', dtype=torch.float16):
                outputs = model(aug_images)
                
                if aug_type == 'cutmix' or aug_type == 'mixup':
                    targets_a = targets_a.long()
                    targets_b = targets_b.long()
                    loss_a = criterion(outputs, targets_a)
                    loss_b = criterion(outputs, targets_b)
                    loss = lam * loss_a + (1 - lam) * loss_b
                else:
                    loss = criterion(outputs, labels)
                
                loss = loss / config.grad_accum_steps
            
            scaler.scale(loss).backward()
            
            if (batch_idx + 1) % config.grad_accum_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
        else:
            outputs = model(aug_images)
            
            if aug_type == 'cutmix' or aug_type == 'mixup':
                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)
            else:
                loss = criterion(outputs, labels)
            
            loss = loss / config.grad_accum_steps
            loss.backward()
            
            if (batch_idx + 1) % config.grad_accum_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
                optimizer.step()
                optimizer.zero_grad()

        # ç»Ÿè®¡
        total_loss += loss.item() * config.grad_accum_steps
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)

        if aug_type == 'cutmix' or aug_type == 'mixup':
            correct_a = (predicted == targets_a).float()
            correct_b = (predicted == targets_b).float()
            batch_correct = (lam * correct_a + (1 - lam) * correct_b).sum().item()
            correct += batch_correct
        else:
            correct += (predicted == labels).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        if (batch_idx + 1) % 10 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            accuracy = 100. * correct / total if total > 0 else 0
            current_lr = optimizer.param_groups[0]['lr']

            progress_bar.set_postfix({
                'Loss': f'{avg_loss:.4f}',
                'Acc': f'{accuracy:.2f}%',
                'LR': f'{current_lr:.2e}',
            })

    # æ‰“å°å¢å¼ºç»Ÿè®¡
    total_batches = len(train_loader)
    print(f"\nğŸ“Š Epoch {epoch+1} å¢å¼ºç»Ÿè®¡:")
    total_augmented = sum(aug_stats.values())
    for aug_type, count in aug_stats.items():
        percentage = count / total_batches * 100
        print(f"  {aug_type}: {count:3d}/{total_batches:3d} ({percentage:5.1f}%)")
    print(f"  Total augmented batches: {total_augmented}/{total_batches} ({total_augmented/total_batches*100:.1f}%)")

    avg_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0
    accuracy = correct / total if total > 0 else 0
    
    return avg_loss, accuracy


# ==================== å¢å¼ºçš„éªŒè¯å‡½æ•° ====================
def enhanced_validate(model, val_loader, criterion, config, epoch):
    """å¢å¼ºçš„éªŒè¯å‡½æ•°"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    class_correct = [0] * config.num_classes
    class_total = [0] * config.num_classes
    
    # æ··æ·†çŸ©é˜µç»Ÿè®¡
    confusion_matrix = np.zeros((config.num_classes, config.num_classes), dtype=int)
    
    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="éªŒè¯ä¸­", leave=False):
            images, labels = images.to(config.device), labels.to(config.device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡
            for i in range(labels.size(0)):
                label = labels[i]
                pred = predicted[i]
                class_correct[label] += (pred == label).item()
                class_total[label] += 1
                confusion_matrix[label][pred] += 1

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_accuracies = []
    for i in range(config.num_classes):
        if class_total[i] > 0:
            class_accuracies.append(class_correct[i] / class_total[i])
        else:
            class_accuracies.append(0.0)

    accuracy = correct / total if total > 0 else 0
    avg_loss = total_loss / len(val_loader) if len(val_loader) > 0 else 0

    # åˆ†ææ··æ·†çŸ©é˜µ
    if epoch % 5 == 0 and epoch > 30:
        print("\nğŸ” æ··æ·†çŸ©é˜µåˆ†æ:")
        class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
        
        # ç‰¹åˆ«å…³æ³¨fearå’Œsadçš„æ··æ·†
        fear_confusions = confusion_matrix[2]
        sad_confusions = confusion_matrix[5]
        
        print(f"  Fearä¸»è¦è¢«è¯¯åˆ¤ä¸º: ", end="")
        for i in range(config.num_classes):
            if i != 2 and fear_confusions[i] > fear_confusions[2] * 0.1:  # è¶…è¿‡10%çš„è¯¯åˆ¤
                print(f"{class_names[i]}({fear_confusions[i]}) ", end="")
        print()
        
        print(f"  Sadä¸»è¦è¢«è¯¯åˆ¤ä¸º: ", end="")
        for i in range(config.num_classes):
            if i != 5 and sad_confusions[i] > sad_confusions[5] * 0.1:
                print(f"{class_names[i]}({sad_confusions[i]}) ", end="")
        print()

    return accuracy, avg_loss, class_accuracies, confusion_matrix


# ==================== ä¸»è®­ç»ƒå‡½æ•° ====================
def train_optimized_3090(config):
    """ä¼˜åŒ–çš„ä¸»è®­ç»ƒå‡½æ•°"""
    print(config)
    print("\nğŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ!")
    if config.resume_from:
        print(f"ğŸ”„ ç»­è®­æ¨¡å¼ï¼šä» {config.resume_from} ç»§ç»­è®­ç»ƒ")
    print("=" * 60)
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ğŸ® GPU: {gpu_name}")
        print(f"ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f} GB")
        print(f"ğŸ“Š Batch Size: {config.batch_size}")
        print(f"âš¡ æ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if config.use_amp else 'ç¦ç”¨'}")
        if config.use_attention:
            print(f"ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶: {config.attention_type.upper()} (å¯ç”¨)")

    # åˆ›å»ºæ•°æ®å¢å¼º
    data_aug = EnhancedAugmentation(config)
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“ åŠ è½½æ•°æ®é›†...")
    train_dir = './data/train'
    val_dir = './data/test'
    
    train_dataset = datasets.ImageFolder(train_dir, transform=data_aug.get_train_transform())
    val_dataset = datasets.ImageFolder(val_dir, transform=data_aug.get_val_transform())
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    train_labels = [label for _, label in train_dataset]
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(config.device)
    config.class_weights = class_weights
    
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset):,} images")
    print(f"  éªŒè¯é›†: {len(val_dataset):,} images")
    print(f"  ç±»åˆ«: {train_dataset.classes}")
    
    # ä¼˜åŒ–æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=8,
        pin_memory=True,
        drop_last=True,
        prefetch_factor=2
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=8,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = create_enhanced_model(config)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    print(f"âœ… ä¼˜åŒ–å™¨: AdamW (lr={config.learning_rate:.2e}, weight_decay={config.weight_decay})")

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config.use_warmup and config.warmup_epochs > 0:
        print(f"ğŸ”¥ å¯ç”¨å­¦ä¹ ç‡Warmup ({config.warmup_epochs}è½®)")
        
        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=0.01,
            end_factor=1.0,
            total_iters=config.warmup_epochs
        )
        
        after_warmup_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=config.num_epochs - config.warmup_epochs,
            eta_min=config.min_lr
        )
        
        scheduler = torch.optim.lr_scheduler.SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, after_warmup_scheduler],
            milestones=[config.warmup_epochs]
        )
    else:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=config.num_epochs,
            eta_min=config.min_lr
        )
    
    # ==================== ç»­è®­é€»è¾‘ ====================
    start_epoch = 0
    best_acc = 0
    best_epoch = 0
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': [],
        'class_acc': [], 'learning_rates': []
    }
    
    # åŠ¨æ€æƒé‡è°ƒæ•´å™¨
    if config.dynamic_weight_adjust:
        weight_adjuster = AdvancedWeightAdjuster(class_weights)
    else:
        weight_adjuster = None
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if config.resume_from and os.path.exists(config.resume_from):
        print(f"\nğŸ“¥ åŠ è½½ç»­è®­æ£€æŸ¥ç‚¹: {config.resume_from}")
        try:
            checkpoint = torch.load(config.resume_from, map_location=config.device)
            
            # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦åŒ…å«æ³¨æ„åŠ›æ¨¡å—
            checkpoint_has_attention = checkpoint.get('has_attention', False)
            model_has_attention = hasattr(model, 'has_attention')
            
            if checkpoint_has_attention and model_has_attention:
                print("âœ… æ£€æŸ¥ç‚¹åŒ…å«æ³¨æ„åŠ›æ¨¡å—ï¼Œä¸å½“å‰æ¨¡å‹åŒ¹é…")
            elif not checkpoint_has_attention and model_has_attention:
                print("âš ï¸  æ£€æŸ¥ç‚¹ä¸åŒ…å«æ³¨æ„åŠ›æ¨¡å—ï¼Œä½†å½“å‰æ¨¡å‹æœ‰æ³¨æ„åŠ›æ¨¡å—")
                print("ğŸ”„ å°†åŠ è½½åŸºç¡€æ¨¡å‹æƒé‡ï¼Œæ³¨æ„åŠ›æ¨¡å—éšæœºåˆå§‹åŒ–")
            elif checkpoint_has_attention and not model_has_attention:
                print("âš ï¸  æ£€æŸ¥ç‚¹åŒ…å«æ³¨æ„åŠ›æ¨¡å—ï¼Œä½†å½“å‰æ¨¡å‹æ²¡æœ‰")
                print("ğŸ”„ å°†å¿½ç•¥æ£€æŸ¥ç‚¹ä¸­çš„æ³¨æ„åŠ›æƒé‡")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if config.resume_optimizer and 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
            
            # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
            if config.resume_scheduler and 'scheduler_state_dict' in checkpoint:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                print("âœ… è°ƒåº¦å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
            
            # åŠ è½½è®­ç»ƒå†å²
            if config.resume_history and 'history' in checkpoint:
                history = checkpoint['history']
                print(f"âœ… è®­ç»ƒå†å²åŠ è½½æˆåŠŸï¼ˆå·²è®­ç»ƒ {len(history['train_loss'])} è½®ï¼‰")
            
            # åŠ è½½æœ€ä½³å‡†ç¡®ç‡
            if 'best_acc' in checkpoint:
                best_acc = checkpoint['best_acc']
                best_epoch = checkpoint.get('epoch', config.start_epoch)
                print(f"âœ… æœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}% (ç¬¬{best_epoch+1}è½®)")
            
            # è®¾ç½®èµ·å§‹epoch
            start_epoch = checkpoint.get('epoch', config.start_epoch) + 1
            print(f"ğŸ”„ ä»ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒ")
            
            # æ¢å¤åŠ¨æ€æƒé‡
            if 'class_acc' in checkpoint and weight_adjuster:
                recent_acc = checkpoint['class_acc']
                weight_adjuster.history = [recent_acc]
                print("âœ… åŠ¨æ€æƒé‡è°ƒæ•´å™¨å·²åˆå§‹åŒ–")
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨é»˜è®¤é…ç½®å¼€å§‹è®­ç»ƒ")
            start_epoch = 0
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = AdaptiveLossFunction(class_weights, config)
    
    # åˆ›å»ºæ—©åœæœºåˆ¶
    early_stopping = AdaptiveEarlyStopping(
        patience=config.patience,
        min_epochs=30,
        target_acc=config.target_acc
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    if config.use_amp:
        try:
            scaler = torch.amp.GradScaler('cuda')
            print("âœ… å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (torch.amp API)")
        except AttributeError:
            scaler = torch.cuda.amp.GradScaler()
            print("âœ… å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (torch.cuda.amp API)")
    else:
        scaler = None
        print("â­ï¸  æ··åˆç²¾åº¦è®­ç»ƒå·²ç¦ç”¨")
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯ (ä»ç¬¬{start_epoch}è½®å¼€å§‹ï¼Œç›®æ ‡: {config.target_acc*100:.1f}%)")
    print("=" * 60)
    
    for epoch in range(start_epoch, config.num_epochs):
        epoch_start = time.time()
        
        # æ›´æ–°æŸå¤±å‡½æ•°çš„epoch
        criterion.epoch = epoch
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch_optimized(
            model, train_loader, criterion, optimizer, scheduler,
            data_aug, config, epoch, weight_adjuster, scaler
        )
        
        # éªŒè¯
        val_acc, val_loss, class_acc, confusion_matrix = enhanced_validate(
            model, val_loader, criterion, config, epoch
        )
        
        # åœ¨epochç»“æŸæ—¶è°ƒç”¨è°ƒåº¦å™¨
        if scheduler is not None:
            scheduler.step()
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['class_acc'].append(class_acc)
        history['learning_rates'].append(optimizer.param_groups[0]['lr'])
        
        # æ‰“å°ç»“æœ
        epoch_time = time.time() - epoch_start
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"\nğŸ“Š Epoch {epoch+1:3d}/{config.num_epochs}")
        print(f"  Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc*100:6.2f}%")
        print(f"  Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc*100:6.2f}%")
        print(f"  Learning Rate: {current_lr:.2e} | Time: {epoch_time:.1f}s")
        
        # ç‰¹åˆ«å…³æ³¨å›°éš¾ç±»åˆ«çš„è¡¨ç°
        if class_acc:
            print(f"  ğŸ¯ å›°éš¾ç±»åˆ«å‡†ç¡®ç‡ - fear: {class_acc[2]*100:.2f}%, sad: {class_acc[5]*100:.2f}%")
            
            # è®¡ç®—è®­ç»ƒ-éªŒè¯å·®è·
            train_val_gap = train_acc - val_acc
            if train_val_gap > 0.10:  # å·®è·å¤§äº10%
                print(f"  âš ï¸  è®­ç»ƒ-éªŒè¯å·®è·è¾ƒå¤§: {train_val_gap*100:.2f}% (å¯èƒ½è¿‡æ‹Ÿåˆ)")
            elif train_val_gap < 0.02:  # å·®è·å°äº2%
                print(f"  âœ… è®­ç»ƒ-éªŒè¯å·®è·è‰¯å¥½: {train_val_gap*100:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc + 0.0001:
            best_acc = val_acc
            best_epoch = epoch
            save_dict = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                'epoch': epoch,
                'best_acc': best_acc,
                'val_acc': val_acc,
                'class_acc': class_acc,
                'config': config.__dict__,
                'history': history,
                'has_attention': hasattr(model, 'has_attention')
            }
            
            filename = 'best_model_3090_with_attention.pth'
            torch.save(save_dict, filename)
            print(f"  ğŸ‰ New Best Accuracy: {best_acc*100:.2f}% (saved to {filename})")
        
        # åŠ¨æ€æƒé‡è°ƒæ•´
        if weight_adjuster and epoch >= 5:
            new_weights = weight_adjuster.update_weights(class_acc, epoch)
            criterion.class_weights = new_weights
            print("  âš–ï¸  åŠ¨æ€æƒé‡è°ƒæ•´å®Œæˆ")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_acc, epoch, train_acc):
            print(f"\nğŸ›‘ Early stopping triggered! Best Accuracy: {early_stopping.best_acc*100:.2f}% "
                  f"(Epoch {early_stopping.best_epoch+1})")
            break
        
        # æ¯5è½®ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 5 == 0:
            checkpoint_path = f'checkpoint_3090_epoch_{epoch+1:03d}_with_attention.pth'
            
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'val_acc': val_acc,
                'class_acc': class_acc,
                'history': history,
                'has_attention': hasattr(model, 'has_attention')
            }, checkpoint_path)
            print(f"  ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        
        print("-" * 60)
    
    # è®­ç»ƒæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ¯ è®­ç»ƒæ€»ç»“")
    print("=" * 60)
    print(f"ğŸ“Š Final Best Accuracy: {best_acc*100:.2f}% (Epoch {best_epoch+1})")
    print(f"ğŸ¯ Target Accuracy: {config.target_acc*100:.1f}%")
    print(f"ğŸ“ˆ Difference: {(config.target_acc - best_acc)*100:+.2f}%")
    print(f"ğŸ”„ Total Training Epochs: {epoch + 1} (ä»ç¬¬{start_epoch}è½®å¼€å§‹)")
    if config.use_attention:
        print(f"ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶: {config.attention_type.upper()} (å¯ç”¨)")
    
    return model, history, best_acc


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    # åˆ›å»ºé…ç½®
    config = Config3090()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(config)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        model, history, best_acc = train_optimized_3090(config)
        
        # æœ€ç»ˆè¯„ä¼°
        print("\nğŸ” æœ€ç»ˆæ¨¡å‹è¯„ä¼°...")
        
        model_file = 'best_model_3090_with_attention.pth'
        if os.path.exists(model_file):
            checkpoint = torch.load(model_file, map_location=config.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            best_acc = checkpoint['best_acc']
            class_acc = checkpoint.get('class_acc', [])
            
            print(f"âœ… Final Best Accuracy: {best_acc*100:.2f}%")
            
            if class_acc:
                print("ğŸ“Š Per-class Accuracy:")
                class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
                for i, (cls_name, acc) in enumerate(zip(class_names, class_acc)):
                    if i < len(class_acc):
                        print(f"  {cls_name}: {acc*100:6.2f}%")
            
            # è®¡ç®—å¹³å‡ç±»åˆ«å‡†ç¡®ç‡
            avg_class_acc = np.mean(class_acc) * 100 if class_acc else 0
            print(f"ğŸ“ˆ Average Class Accuracy: {avg_class_acc:.2f}%")
            
            # ç‰¹åˆ«å…³æ³¨å›°éš¾ç±»åˆ«
            if len(class_acc) > 5:
                hard_class_avg = (class_acc[2] + class_acc[5]) / 2 * 100
                print(f"ğŸ¯ å›°éš¾ç±»åˆ«(fear+sad)å¹³å‡: {hard_class_avg:.2f}%")
            
            if best_acc >= 0.74:
                print(f"ğŸ‰ æˆåŠŸçªç ´74%!")
            elif best_acc >= 0.73:
                print(f"âœ… è¾¾åˆ°73%ä»¥ä¸Š!")
            else:
                print(f"ğŸ“ˆ æœ€ç»ˆå‡†ç¡®ç‡: {best_acc*100:.2f}%")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
