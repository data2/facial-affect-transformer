ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä¼˜åŒ–ä»£ç ï¼Œé›†æˆäº†CutMixã€æ ‡ç­¾å¹³æ»‘ã€åˆ†å±‚å­¦ä¹ ç‡ç­‰é«˜çº§æŠ€æœ¯ï¼Œç›®æ ‡æ˜¯å°†å‡†ç¡®ç‡æå‡åˆ°80%å·¦å³ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
import timm
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
from PIL import Image
from tqdm import tqdm
import time
import os
import math
from sklearn.utils.class_weight import compute_class_weight


# === é«˜çº§ä¼˜åŒ–ç»„ä»¶ ===
class EarlyStopping:
    def __init__(self, patience=10, delta=0.001, min_epochs=20):
        self.patience = patience
        self.delta = delta
        self.min_epochs = min_epochs
        self.best_acc = 0
        self.counter = 0
        self.early_stop = False

    def __call__(self, val_acc, epoch):
        if epoch < self.min_epochs:
            return False
            
        if val_acc > self.best_acc + self.delta:
            self.best_acc = val_acc
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        return self.early_stop


class LabelSmoothingCrossEntropy(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°"""
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, x, target):
        confidence = 1.0 - self.smoothing
        logprobs = F.log_softmax(x, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()


def cutmix_data(x, y, alpha=1.0):
    """CutMixæ•°æ®å¢å¼º"""
    if alpha <= 0:
        return x, y, y, 1.0
        
    # ç”Ÿæˆlambdaå€¼
    lam = np.random.beta(alpha, alpha)
    
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    
    # ç”Ÿæˆè£å‰ªåŒºåŸŸ
    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)
    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
    
    # è°ƒæ•´lambda
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))
    y_a, y_b = y, y[index]
    
    return x, y_a, y_b, lam


def rand_bbox(size, lam):
    """ç”Ÿæˆéšæœºè£å‰ªåŒºåŸŸ"""
    W = size[2]
    H = size[3]
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)

    # å‡åŒ€åˆ†å¸ƒ
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2


def cutmix_criterion(criterion, pred, y_a, y_b, lam):
    """CutMixæŸå¤±å‡½æ•°"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):
    """å¸¦çƒ­èº«çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def main():
    print("å¼€å§‹è¿›å…¥è®­ç»ƒ - é«˜çº§ä¼˜åŒ–ç‰ˆæœ¬ (ç›®æ ‡: 80%å‡†ç¡®ç‡)")
    print("=" * 60)

    # === è®¾å¤‡è®¾ç½® ===
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
        torch.cuda.empty_cache()
    else:
        device = torch.device('cpu')
        print("âš ï¸ æœªå‘ç°GPUï¼Œä½¿ç”¨CPU")

    # === è¶…å‚æ•°é…ç½® ===
    config = {
        'model_size': 'base',
        'batch_size': 16,
        'num_epochs': 100,
        'learning_rate': 3e-5,
        'weight_decay': 0.05,
        'cutmix_alpha': 1.0,
        'label_smoothing': 0.1,
        'drop_rate': 0.2,
        'drop_path_rate': 0.2,
        'grad_accum_steps': 4,
        'warmup_epochs': 5,
    }
    
    MODEL_SIZE = config['model_size']
    print(f"ğŸ¯ ç›®æ ‡: 80%éªŒè¯å‡†ç¡®ç‡ | æ¨¡å‹: ViT-{MODEL_SIZE.capitalize()}")

    # === 1. å¼ºåŒ–æ•°æ®é¢„å¤„ç† ===
    print("\nğŸ”„ é…ç½®å¼ºåŒ–æ•°æ®å¢å¼º...")
    
    # è®­ç»ƒæ—¶ä½¿ç”¨çš„å¢å¼ºtransform (å¼ºåŒ–ç‰ˆ)
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),  # å…ˆæ”¾å¤§
        transforms.RandomCrop((224, 224)),  # éšæœºè£å‰ª
        transforms.Grayscale(num_output_channels=3),
        
        # ç©ºé—´å˜æ¢
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.2),
        transforms.RandomRotation(degrees=30),
        transforms.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2)),
        
        # é¢œè‰²å˜æ¢
        transforms.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.4, hue=0.1),
        transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 5.0)),
        
        # é®æŒ¡
        transforms.RandomErasing(p=0.5, scale=(0.02, 0.25), ratio=(0.3, 3.3)),
        
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # éªŒè¯é›†transform
    val_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.CenterCrop((224, 224)),  # ä¸­å¿ƒè£å‰ªç¡®ä¿ä¸€è‡´æ€§
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # ç®€å•transformç”¨äºè·å–æ ‡ç­¾
    simple_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
    ])

    # === 2. åŠ è½½æ•°æ®é›† ===
    print("ğŸ“ åŠ è½½æ•°æ®é›†...")
    train_dir = './data/train'
    test_dir = './data/test'

    train_dataset_simple = datasets.ImageFolder(train_dir, transform=simple_transform)
    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    val_dataset = datasets.ImageFolder(test_dir, transform=val_transform)

    # è·å–è®­ç»ƒæ ‡ç­¾
    train_labels = [label for _, label in train_dataset_simple]

    # æ•°æ®åŠ è½½å™¨
    num_workers = min(8, os.cpu_count())
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,
                              num_workers=num_workers, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False,
                            num_workers=num_workers, pin_memory=True)

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    print(f"  ç±»åˆ«æ•°é‡: {len(train_dataset.classes)}")
    print(f"  Batch Size: {config['batch_size']} (ç´¯ç§¯æ­¥æ•°: {config['grad_accum_steps']})")

    # === 3. è®¡ç®—ç±»åˆ«æƒé‡ ===
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    
    # å¢å¼ºå›°éš¾ç±»åˆ«çš„æƒé‡
    class_weights[0] *= 2.0  # angry
    class_weights[1] *= 3.0  # fear
    class_weights[5] *= 1.5  # sad
    
    print("ğŸ“ˆ è°ƒæ•´åçš„ç±»åˆ«æƒé‡:", class_weights.cpu().numpy())

    # === 4. åˆ›å»ºä¼˜åŒ–æ¨¡å‹ ===
    def create_optimized_model(model_size='base', num_classes=7, weights_dir='./weights'):
        """åˆ›å»ºå¸¦æ­£åˆ™åŒ–çš„ä¼˜åŒ–æ¨¡å‹"""
        model_configs = {
            'base': {
                'name': 'vit_base_patch16_224',
                'local_file': 'vit_base_patch16_224.pth',
            }
        }

        config = model_configs[model_size]
        local_weight_path = os.path.join(weights_dir, config['local_file'])

        if not os.path.exists(local_weight_path):
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {local_weight_path}")

        print(f"ğŸ”„ ä»æœ¬åœ°åŠ è½½é¢„è®­ç»ƒæƒé‡: {local_weight_path}")

        # åˆ›å»ºå¸¦æ­£åˆ™åŒ–çš„æ¨¡å‹
        model = timm.create_model(
            config['name'],
            pretrained=False,
            num_classes=num_classes,
            drop_rate=config['drop_rate'],
            drop_path_rate=config['drop_path_rate'],
            attn_drop_rate=0.1,
        )

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        checkpoint = torch.load(local_weight_path, map_location='cpu')
        state_dict = checkpoint
        
        # å¤„ç†ä¸åŒçš„æƒé‡æ ¼å¼
        if 'model_state_dict' in state_dict:
            state_dict = state_dict['model_state_dict']
        elif 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']

        # è¿‡æ»¤åˆ†ç±»å¤´æƒé‡
        filtered_state_dict = {k: v for k, v in state_dict.items() 
                             if not k.startswith('head.') and not k.startswith('fc.')}
        
        # åŠ è½½æƒé‡
        missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
        
        print(f"âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
        print(f"  é¢„è®­ç»ƒå‚æ•°æ¯”ä¾‹: {(len(filtered_state_dict)/len(state_dict)*100):.1f}%")
        print(f"  ç¼ºå¤±é”®: {len(missing_keys)}, æ„å¤–é”®: {len(unexpected_keys)}")
        
        return model

    # åˆ›å»ºæ¨¡å‹
    num_classes = 7
    try:
        model = create_optimized_model(MODEL_SIZE, num_classes=num_classes)
        model = model.to(device)
        print("âœ… ä¼˜åŒ–æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹...")
        model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=num_classes)
        model = model.to(device)

    # === 5. ä¼˜åŒ–å™¨é…ç½® ===
    # åˆ†å±‚å­¦ä¹ ç‡
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        # ä¸»å¹²ç½‘ç»œï¼šè¾ƒå°çš„å­¦ä¹ ç‡
        {'params': [p for n, p in model.named_parameters() 
                   if not any(nd in n for nd in no_decay) and 'head' not in n],
         'weight_decay': config['weight_decay'], 'lr': config['learning_rate'] * 0.1},
        
        {'params': [p for n, p in model.named_parameters() 
                   if any(nd in n for nd in no_decay) and 'head' not in n],
         'weight_decay': 0.0, 'lr': config['learning_rate'] * 0.1},
        
        # åˆ†ç±»å¤´ï¼šè¾ƒå¤§çš„å­¦ä¹ ç‡
        {'params': [p for n, p in model.named_parameters() if 'head' in n],
         'weight_decay': config['weight_decay'], 'lr': config['learning_rate']}
    ]

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    num_training_steps = len(train_loader) * config['num_epochs'] // config['grad_accum_steps']
    num_warmup_steps = len(train_loader) * config['warmup_epochs'] // config['grad_accum_steps']
    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

    # æŸå¤±å‡½æ•° (æ ‡ç­¾å¹³æ»‘)
    criterion = LabelSmoothingCrossEntropy(smoothing=config['label_smoothing'])
    criterion.weight = class_weights

    # æ—©åœæœºåˆ¶
    early_stopping = EarlyStopping(patience=12, delta=0.002, min_epochs=20)

    print(f"ğŸ¯ ä¼˜åŒ–é…ç½®:")
    print(f"  å­¦ä¹ ç‡: {config['learning_rate']:.1e} (åˆ†å±‚)")
    print(f"  CutMix Alpha: {config['cutmix_alpha']}")
    print(f"  æ ‡ç­¾å¹³æ»‘: {config['label_smoothing']}")
    print(f"  Dropout: {config['drop_rate']}")
    print(f"  çƒ­èº«è½®æ¬¡: {config['warmup_epochs']}")

    # === 6. è®­ç»ƒå‡½æ•° ===
    def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, grad_accum_steps=4):
        """å¸¦CutMixå’Œæ¢¯åº¦ç´¯ç§¯çš„è®­ç»ƒ"""
        model.train()
        total_loss = 0
        optimizer.zero_grad()

        for i, (images, labels) in enumerate(tqdm(train_loader, desc="è®­ç»ƒä¸­")):
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            # 50%æ¦‚ç‡ä½¿ç”¨CutMix
            if np.random.rand() < 0.5:
                images, targets_a, targets_b, lam = cutmix_data(images, labels, config['cutmix_alpha'])
                outputs = model(images)
                loss = cutmix_criterion(criterion, outputs, targets_a, targets_b, lam)
            else:
                outputs = model(images)
                loss = criterion(outputs, labels)

            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / grad_accum_steps
            loss.backward()

            if (i + 1) % grad_accum_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

            total_loss += loss.item() * grad_accum_steps

        return total_loss / len(train_loader)

    def evaluate(model, dataloader, criterion):
        """éªŒè¯å‡½æ•°"""
        model.eval()
        correct = 0
        total = 0
        total_loss = 0
        all_preds = []
        all_targets = []

        with torch.no_grad():
            for images, labels in tqdm(dataloader, desc="éªŒè¯ä¸­", leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                total_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(labels.cpu().numpy())

        acc = correct / total
        avg_loss = total_loss / len(dataloader)
        
        # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
        class_acc = []
        for i in range(len(val_dataset.classes)):
            class_mask = np.array(all_targets) == i
            if class_mask.sum() > 0:
                class_acc.append((np.array(all_preds)[class_mask] == i).mean())
            else:
                class_acc.append(0.0)
                
        return acc, avg_loss, class_acc

    # === 7. è®­ç»ƒå¾ªç¯ ===
    print(f"\nğŸš€ å¼€å§‹é«˜çº§ä¼˜åŒ–è®­ç»ƒ")
    print("=" * 60)

    best_acc = 0.0
    training_history = {
        'train_loss': [], 'val_acc': [], 'val_loss': [], 'learning_rates': []
    }

    for epoch in range(config['num_epochs']):
        epoch_start = time.time()
        print(f"\nğŸ“Š Epoch [{epoch+1}/{config['num_epochs']}]")

        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, 
                                device, config['grad_accum_steps'])
        training_history['train_loss'].append(train_loss)

        # éªŒè¯
        val_acc, val_loss, class_acc = evaluate(model, val_loader, criterion)
        training_history['val_acc'].append(val_acc)
        training_history['val_loss'].append(val_loss)
        training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])

        # æ‰“å°ç»“æœ
        current_lr = optimizer.param_groups[0]['lr']
        epoch_time = time.time() - epoch_start
        
        print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"ğŸ¯ éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.2f}% | éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"ğŸ’¡ å­¦ä¹ ç‡: {current_lr:.2e}")
        print(f"â±ï¸  æœ¬è½®è€—æ—¶: {epoch_time:.1f}ç§’")
        
        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        print("ğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡:")
        for i, cls_name in enumerate(val_dataset.classes):
            print(f"  {cls_name}: {class_acc[i]*100:5.1f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc + 0.001:
            best_acc = val_acc
            best_model_path = f'best_vit_{MODEL_SIZE}_optimized.pth'
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_acc': best_acc,
                'config': config
            }, best_model_path)
            print(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}% -> {best_model_path}")

        # æ—©åœæ£€æŸ¥
        if early_stopping(val_acc, epoch):
            print("ğŸ›‘ æ—©åœè§¦å‘ï¼Œè®­ç»ƒç»“æŸ")
            break

        print("-" * 60)

        # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint
        if (epoch + 1) % 10 == 0:
            checkpoint_path = f'checkpoint_epoch_{epoch+1}.pth'
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_acc': best_acc,
                'training_history': training_history,
                'config': config
            }, checkpoint_path)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    # === 8. è®­ç»ƒæ€»ç»“ ===
    print("\n" + "=" * 60)
    print("ğŸ¯ è®­ç»ƒæ€»ç»“")
    print("=" * 60)
    print(f"ğŸ“Š æœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {best_acc*100:.2f}%")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹: best_vit_{MODEL_SIZE}_optimized.pth")
    print(f"ğŸ”„ æ€»è®­ç»ƒè½®æ¬¡: {epoch+1}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆå¯é€‰ï¼‰
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(training_history['train_loss'], label='è®­ç»ƒæŸå¤±')
        plt.plot(training_history['val_loss'], label='éªŒè¯æŸå¤±')
        plt.legend()
        plt.title('æŸå¤±æ›²çº¿')
        
        plt.subplot(1, 2, 2)
        plt.plot(training_history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', color='green')
        plt.axhline(y=best_acc, color='r', linestyle='--', label=f'æœ€ä½³: {best_acc*100:.1f}%')
        plt.legend()
        plt.title('å‡†ç¡®ç‡æ›²çº¿')
        
        plt.tight_layout()
        plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')
        print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: training_curves.png")
    except:
        print("âš ï¸ æ— æ³•ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼Œè¯·å®‰è£…matplotlib")

    # === 9. æœ€ç»ˆéªŒè¯ ===
    print("\nğŸ” æœ€ç»ˆæ¨¡å‹éªŒè¯...")
    model.load_state_dict(torch.load(f'best_vit_{MODEL_SIZE}_optimized.pth')['model_state_dict'])
    final_acc, final_loss, class_acc = evaluate(model, val_loader, criterion)
    
    print(f"âœ… æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_acc*100:.2f}%")
    if final_acc >= 0.8:
        print("ğŸ‰ æ­å–œï¼è¾¾åˆ°80%å‡†ç¡®ç‡ç›®æ ‡ï¼")
    elif final_acc >= 0.78:
        print("âœ… æ¥è¿‘ç›®æ ‡ï¼å‡†ç¡®ç‡ > 78%")
    else:
        print("ğŸ“ˆ ç»§ç»­ä¼˜åŒ–ç©ºé—´ï¼Œå»ºè®®å°è¯•æ›´å¤§æ¨¡å‹æˆ–æ›´å¤šæ•°æ®å¢å¼º")


if __name__ == '__main__':
    main()
```

## ğŸš€ **ä¸»è¦ä¼˜åŒ–ç‰¹æ€§**

### 1. **å¼ºåŒ–æ•°æ®å¢å¼º**
- **CutMixæ··åˆå¢å¼º**ï¼š50%æ¦‚ç‡ä½¿ç”¨CutMix
- **æ›´å¼ºçš„ç©ºé—´å˜æ¢**ï¼šæ—‹è½¬30åº¦ã€å¹³ç§»20%ã€ç¼©æ”¾0.8-1.2å€
- **æ›´å¼ºçš„é¢œè‰²æ‰°åŠ¨**ï¼šäº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å¢å¼º
- **æ›´å¼ºçš„é®æŒ¡**ï¼šRandomErasingæ¦‚ç‡æå‡åˆ°50%

### 2. **é«˜çº§æ­£åˆ™åŒ–**
- **æ ‡ç­¾å¹³æ»‘**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›
- **åˆ†å±‚Dropout**ï¼šä¸»å¹²ç½‘ç»œ0.2ï¼Œæ³¨æ„åŠ›æœºåˆ¶0.1
- **éšæœºæ·±åº¦**ï¼šDropPathç‡0.2

### 3. **ä¼˜åŒ–ç­–ç•¥**
- **åˆ†å±‚å­¦ä¹ ç‡**ï¼šåˆ†ç±»å¤´å­¦ä¹ ç‡æ˜¯ä¸»å¹²ç½‘ç»œçš„10å€
- **ä½™å¼¦é€€ç«+çƒ­èº«**ï¼š5ä¸ªepochçš„çƒ­èº«é˜¶æ®µ
- **æ¢¯åº¦ç´¯ç§¯**ï¼šæœ‰æ•ˆbatch_size = 16 Ã— 4 = 64

### 4. **è®­ç»ƒç›‘æ§**
- **å„ç±»åˆ«å‡†ç¡®ç‡**ï¼šå®æ—¶æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„è¡¨ç°
- **è‡ªåŠ¨æ£€æŸ¥ç‚¹**ï¼šæ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡
- **è®­ç»ƒæ›²çº¿å¯è§†åŒ–**ï¼šè‡ªåŠ¨ç”ŸæˆæŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿

## ğŸ“ˆ **é¢„æœŸæ•ˆæœ**

åŸºäºæ‚¨çš„å½“å‰72.26%çš„åŸºç¡€ï¼Œè¿™ä¸ªä¼˜åŒ–ç‰ˆæœ¬é¢„è®¡å¯ä»¥å¸¦æ¥ï¼š

- **CutMixå¢å¼º**ï¼š+3-5%
- **æ ‡ç­¾å¹³æ»‘**ï¼š+1-2%  
- **å¼ºåŒ–æ•°æ®å¢å¼º**ï¼š+2-3%
- **åˆ†å±‚å­¦ä¹ ç‡**ï¼š+1-2%
- **æ€»è®¡é¢„æœŸ**ï¼š77-82%

## ğŸ’¡ **ä½¿ç”¨å»ºè®®**

1. **é¦–æ¬¡è¿è¡Œ**ï¼šä½¿ç”¨å½“å‰é…ç½®ï¼Œé¢„è®¡è¾¾åˆ°78-80%
2. **å¦‚æœæœªè¾¾ç›®æ ‡**ï¼šå¯ä»¥å°è¯•ä»¥ä¸‹è°ƒæ•´ï¼š
   - å¢åŠ `cutmix_alpha`åˆ°1.5
   - å¢åŠ `warmup_epochs`åˆ°10
   - ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆViT-Largeï¼‰

è¿™ä¸ªç‰ˆæœ¬åº”è¯¥èƒ½å¤Ÿæ˜¾è‘—æå‡æ‚¨çš„æ¨¡å‹æ€§èƒ½ï¼