import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
import timm
import numpy as np
from tqdm import tqdm
import os
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import json
import warnings
warnings.filterwarnings('ignore')

# ========== å…¨å±€å­—ä½“é…ç½® ==========
def setup_global_font():
    """å…¨å±€è®¾ç½®ä¸­æ–‡å­—ä½“ - ä½¿ç”¨ AR PL UMing CN"""
    try:
        # è®¾ç½®å­—ä½“ä¸º AR PL UMing CNï¼ˆæ–‡é¼æ˜ä½“ï¼‰
        matplotlib.rcParams['font.family'] = 'AR PL UMing CN'
        matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
        
        # å¯é€‰ï¼šè®¾ç½®å­—ä½“å¤§å°
        matplotlib.rcParams['font.size'] = 12
        matplotlib.rcParams['axes.titlesize'] = 14
        matplotlib.rcParams['axes.labelsize'] = 12
        matplotlib.rcParams['xtick.labelsize'] = 10
        matplotlib.rcParams['ytick.labelsize'] = 10
        matplotlib.rcParams['legend.fontsize'] = 10
        
        print("âœ… å…¨å±€å­—ä½“å·²è®¾ç½®ä¸º: AR PL UMing CN (æ–‡é¼æ˜ä½“)")
        
        # éªŒè¯å­—ä½“æ˜¯å¦è®¾ç½®æˆåŠŸ
        from matplotlib import font_manager
        current_font = matplotlib.rcParams['font.family']
        print(f"ğŸ“ å½“å‰ä½¿ç”¨å­—ä½“: {current_font}")
        
        return True
    except Exception as e:
        print(f"âŒ å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        return False

# ç«‹å³æ‰§è¡Œå…¨å±€å­—ä½“è®¾ç½®
setup_global_font()

class PrivateTestEvaluator:
    """åœ¨PrivateTesté›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹çš„æµ‹è¯•ç±»"""
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ä½¿ç”¨é…ç½®æˆ–åˆ›å»ºé»˜è®¤é…ç½®
        if config is None:
            from dataclasses import dataclass
            @dataclass
            class TestConfig:
                model_name = 'vit_base_patch16_224'
                num_classes = 7
                img_size = 224
                batch_size = 16
                device = self.device
                class_weights = None
                drop_rate = 0.3
                
            self.config = TestConfig()
        else:
            self.config = config
            
        # ç±»åˆ«åç§°
        self.class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
        
        # æ¨¡å‹å’Œè½¬æ¢
        self.model = None
        self.test_transform = None
        
        # ç»“æœå­˜å‚¨
        self.results = {}
        
        # ç¡®ä¿å­—ä½“å·²æ­£ç¡®è®¾ç½®
        self._verify_font()
    
    def _verify_font(self):
        """éªŒè¯å­—ä½“è®¾ç½®"""
        print("ğŸ” éªŒè¯å­—ä½“è®¾ç½®...")
        current_font = matplotlib.rcParams.get('font.family', 'æœªçŸ¥')
        print(f"  å½“å‰å­—ä½“: {current_font}")
        
        # åˆ›å»ºä¸€ä¸ªå°æµ‹è¯•å›¾éªŒè¯å­—ä½“
        try:
            test_fig, test_ax = plt.subplots(figsize=(4, 3))
            test_ax.text(0.5, 0.5, 'å­—ä½“æµ‹è¯•: ä¸­æ–‡', 
                        fontsize=14, ha='center', va='center', 
                        transform=test_ax.transAxes)
            test_ax.set_title('å­—ä½“éªŒè¯')
            test_ax.axis('off')
            plt.savefig('font_verification.png', dpi=150, bbox_inches='tight')
            plt.close(test_fig)
            print("âœ… å­—ä½“éªŒè¯å›¾å·²ä¿å­˜: font_verification.png")
        except Exception as e:
            print(f"âš ï¸  å­—ä½“éªŒè¯å¤±è´¥: {e}")
    
    def create_model(self, model_path='./best_model_3090.pth'):
        """åˆ›å»ºå’ŒåŠ è½½æ¨¡å‹"""
        print(f"\nğŸ“¦ åˆ›å»ºæ¨¡å‹: {self.config.model_name}")
        
        # åˆ›å»ºæ¨¡å‹
        model = timm.create_model(
            self.config.model_name,
            pretrained=False,
            num_classes=self.config.num_classes,
            drop_rate=self.config.drop_rate
        ).to(self.device)
        
        # åŠ è½½è®­ç»ƒçš„æœ€ä½³æ¨¡å‹
        if os.path.exists(model_path):
            print(f"ğŸ“¥ åŠ è½½æœ€ä½³æ¨¡å‹: {model_path}")
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                
                # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint
                
                # åŠ è½½çŠ¶æ€å­—å…¸
                model.load_state_dict(state_dict)
                print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                
                # å¦‚æœæœ‰é…ç½®ä¿¡æ¯ï¼Œæ›´æ–°é…ç½®
                if 'config' in checkpoint:
                    checkpoint_config = checkpoint['config']
                    print(f"ğŸ“Š æ¨¡å‹è®­ç»ƒä¿¡æ¯:")
                    print(f"  - æœ€ä½³å‡†ç¡®ç‡: {checkpoint.get('best_acc', 0)*100:.2f}%")
                    print(f"  - è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 0)+1}")
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                # å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼
                try:
                    model.load_state_dict(checkpoint)
                    print("âœ… ä½¿ç”¨ç›´æ¥åŠ è½½æ–¹å¼æˆåŠŸ")
                except:
                    raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹æƒé‡: {e}")
        else:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        self.model = model
        return model
    
    def get_test_transform(self):
        """è·å–æµ‹è¯•æ•°æ®è½¬æ¢"""
        if self.test_transform is None:
            self.test_transform = transforms.Compose([
                transforms.Resize((self.config.img_size, self.config.img_size)),
                transforms.Grayscale(num_output_channels=3),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
        return self.test_transform
    
    def load_test_dataset(self, test_dir='./data/test'):
        """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
        print(f"\nğŸ“ åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_dir}")
        
        if not os.path.exists(test_dir):
            # å°è¯•ä¸åŒçš„è·¯å¾„
            possible_paths = [
                './data/private',
                './data/private_test',
                './datasets/PrivateTest',
                '../data/PrivateTest'
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    test_dir = path
                    print(f"âœ… æ‰¾åˆ°æµ‹è¯•é›†: {test_dir}")
                    break
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æµ‹è¯•é›†ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚å°è¯•è¿‡çš„è·¯å¾„: {possible_paths}")
        
        # åŠ è½½æ•°æ®é›†
        test_dataset = datasets.ImageFolder(
            test_dir, 
            transform=self.get_test_transform()
        )
        
        # éªŒè¯ç±»åˆ«æ•°é‡
        if len(test_dataset.classes) != self.config.num_classes:
            print(f"âš ï¸  è­¦å‘Š: æ•°æ®é›†ç±»åˆ«æ•°({len(test_dataset.classes)})ä¸æ¨¡å‹ç±»åˆ«æ•°({self.config.num_classes})ä¸åŒ¹é…")
            print(f"    æ•°æ®é›†ç±»åˆ«: {test_dataset.classes}")
        
        print(f"ğŸ“Š æµ‹è¯•é›†ç»Ÿè®¡:")
        print(f"  - æ€»æ ·æœ¬æ•°: {len(test_dataset):,}")
        print(f"  - ç±»åˆ«: {test_dataset.classes}")
        
        # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        class_counts = {}
        for _, label in test_dataset.samples:
            class_name = test_dataset.classes[label]
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        print(f"  - å„ç±»åˆ«æ ·æœ¬æ•°:")
        for cls, count in class_counts.items():
            print(f"    {cls}: {count}")
        
        return test_dataset
    
    def evaluate(self, test_dir='./data/test', model_path='./best_model_3090.pth'):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        print("=" * 70)
        print("ğŸ§ª å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¨¡å‹è¯„ä¼°")
        print("=" * 70)
        
        # 1. åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
        model = self.create_model(model_path)
        model.eval()
        
        # 2. åŠ è½½æµ‹è¯•æ•°æ®é›†
        test_dataset = self.load_test_dataset(test_dir)
        
        # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # 4. è¿›è¡Œé¢„æµ‹
        print(f"\nğŸ”® è¿›è¡Œé¢„æµ‹...")
        all_predictions = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc="é¢„æµ‹")):
                images = images.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = model(images)
                probs = F.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)
                
                # ä¿å­˜ç»“æœ
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.numpy())
                all_probs.extend(probs.cpu().numpy())
        
        # 5. è®¡ç®—æŒ‡æ ‡
        print(f"\nğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        # æ€»ä½“å‡†ç¡®ç‡
        overall_accuracy = accuracy_score(all_labels, all_predictions)
        
        # åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(
            all_labels, 
            all_predictions, 
            target_names=test_dataset.classes,
            digits=4,
            output_dict=True
        )
        
        # æ··æ·†çŸ©é˜µ
        conf_matrix = confusion_matrix(all_labels, all_predictions)
        
        # 6. ä¿å­˜ç»“æœ
        self.results = {
            'overall_accuracy': overall_accuracy,
            'class_report': class_report,
            'confusion_matrix': conf_matrix.tolist(),
            'predictions': all_predictions,
            'labels': all_labels,
            'probabilities': all_probs,
            'class_names': test_dataset.classes,
            'total_samples': len(test_dataset)
        }
        
        return self.results
    
    def generate_report(self, save_dir='./results'):
        """ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
        if not self.results:
            print("âš ï¸  è¯·å…ˆè¿è¡Œevaluate()æ–¹æ³•")
            return
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ“„ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # 1. æ‰“å°æ€»ä½“ç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ¯ è¯„ä¼°ç»“æœæ€»ç»“")
        print("=" * 70)
        print(f"ğŸ“Š æ€»ä½“å‡†ç¡®ç‡: {self.results['overall_accuracy']*100:.4f}%")
        print(f"ğŸ“ˆ æ€»æ ·æœ¬æ•°: {self.results['total_samples']:,}")
        print("-" * 70)
        
        # 2. æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†ç»“æœ
        print("ğŸ“‹ æ¯ä¸ªç±»åˆ«æ€§èƒ½:")
        class_report = self.results['class_report']
        
        # åˆ›å»ºè¡¨æ ¼
        metrics_df = pd.DataFrame({
            'Precision': [class_report[cls]['precision'] * 100 for cls in self.results['class_names']],
            'Recall': [class_report[cls]['recall'] * 100 for cls in self.results['class_names']],
            'F1-Score': [class_report[cls]['f1-score'] * 100 for cls in self.results['class_names']],
            'Support': [class_report[cls]['support'] for cls in self.results['class_names']]
        }, index=self.results['class_names'])
        
        # æ·»åŠ å¹³å‡å€¼è¡Œ
        metrics_df.loc['Weighted Avg'] = [
            class_report['weighted avg']['precision'] * 100,
            class_report['weighted avg']['recall'] * 100,
            class_report['weighted avg']['f1-score'] * 100,
            class_report['weighted avg']['support']
        ]
        
        metrics_df.loc['Macro Avg'] = [
            class_report['macro avg']['precision'] * 100,
            class_report['macro avg']['recall'] * 100,
            class_report['macro avg']['f1-score'] * 100,
            class_report['macro avg']['support']
        ]
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        pd.set_option('display.float_format', '{:.4f}'.format)
        print(metrics_df.to_string())
        
        # 3. ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
        report_path = os.path.join(save_dir, 'test_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump({
                'overall_accuracy': float(self.results['overall_accuracy']),
                'class_report': self.results['class_report'],
                'class_names': self.results['class_names'],
                'total_samples': self.results['total_samples'],
                'timestamp': pd.Timestamp.now().isoformat()
            }, f, indent=4)
        print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # 4. ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSV
        predictions_df = pd.DataFrame({
            'true_label': [self.results['class_names'][l] for l in self.results['labels']],
            'predicted_label': [self.results['class_names'][p] for p in self.results['predictions']],
            'correct': [l == p for l, p in zip(self.results['labels'], self.results['predictions'])]
        })
        
        # æ·»åŠ æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡
        for i, cls in enumerate(self.results['class_names']):
            predictions_df[f'prob_{cls}'] = [prob[i] for prob in self.results['probabilities']]
        
        predictions_path = os.path.join(save_dir, 'detailed_predictions.csv')
        predictions_df.to_csv(predictions_path, index=False)
        print(f"âœ… è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜: {predictions_path}")
        
        return metrics_df
    
    def plot_confusion_matrix(self, save_dir='./results', figsize=(12, 10)):
        """ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ"""
        if not self.results:
            print("âš ï¸  è¯·å…ˆè¿è¡Œevaluate()æ–¹æ³•")
            return
        
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ¨ ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
        
        # å†æ¬¡ç¡®è®¤å­—ä½“è®¾ç½®ï¼ˆä¿é™©èµ·è§ï¼‰
        plt.rcParams['font.family'] = 'AR PL UMing CN'
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºæ··æ·†çŸ©é˜µ
        conf_matrix = np.array(self.results['confusion_matrix'])
        class_names = self.results['class_names']
        
        # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
        conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]
        
        # åˆ›å»ºå›¾å½¢
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
        
        # åŸå§‹æ··æ·†çŸ©é˜µ
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names,
                   ax=ax1, cbar=False)
        ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        ax1.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        ax1.set_title('æ··æ·†çŸ©é˜µï¼ˆåŸå§‹è®¡æ•°ï¼‰', fontsize=14, pad=20)
        
        # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
        sns.heatmap(conf_matrix_norm, annot=True, fmt='.2f', cmap='Oranges',
                   xticklabels=class_names, yticklabels=class_names,
                   ax=ax2, cbar=False)
        ax2.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        ax2.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        ax2.set_title('æ··æ·†çŸ©é˜µï¼ˆå½’ä¸€åŒ–ï¼‰', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        cm_path = os.path.join(save_dir, 'confusion_matrix.png')
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.savefig(os.path.join(save_dir, 'confusion_matrix.pdf'), bbox_inches='tight')
        plt.close(fig)  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
        
        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_path}")
        print(f"ğŸ“ ä½¿ç”¨å­—ä½“: AR PL UMing CN")
        
        return cm_path
    
    def plot_class_performance(self, save_dir='./results', figsize=(12, 8)):
        """ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½æŒ‡æ ‡"""
        if not self.results:
            print("âš ï¸  è¯·å…ˆè¿è¡Œevaluate()æ–¹æ³•")
            return
        
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ“ˆ ç»˜åˆ¶ç±»åˆ«æ€§èƒ½å›¾...")
        
        # å†æ¬¡ç¡®è®¤å­—ä½“è®¾ç½®
        plt.rcParams['font.family'] = 'AR PL UMing CN'
        plt.rcParams['axes.unicode_minus'] = False
        
        class_report = self.results['class_report']
        class_names = self.results['class_names']
        
        # æå–æŒ‡æ ‡
        precision = [class_report[cls]['precision'] * 100 for cls in class_names]
        recall = [class_report[cls]['recall'] * 100 for cls in class_names]
        f1 = [class_report[cls]['f1-score'] * 100 for cls in class_names]
        
        # æ”¯æŒåº¦
        support = [class_report[cls]['support'] for cls in class_names]
        
        # åˆ›å»ºå›¾å½¢ - æ”¹ä¸º1è¡Œ2åˆ—å¸ƒå±€
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        # æŒ‡æ ‡æŸ±çŠ¶å›¾ï¼ˆå·¦ï¼‰
        x = np.arange(len(class_names))
        width = 0.25
        
        ax1 = axes[0]
        bars1 = ax1.bar(x - width, precision, width, label='ç²¾ç¡®ç‡', color='#4C72B0', alpha=0.8)
        bars2 = ax1.bar(x, recall, width, label='å¬å›ç‡', color='#55A868', alpha=0.8)
        bars3 = ax1.bar(x + width, f1, width, label='F1åˆ†æ•°', color='#C44E52', alpha=0.8)
        
        ax1.set_xlabel('æƒ…æ„Ÿç±»åˆ«', fontsize=12)
        ax1.set_ylabel('ç™¾åˆ†æ¯” (%)', fontsize=12)
        ax1.set_title('å„ç±»åˆ«æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=14, pad=20)
        ax1.set_xticks(x)
        ax1.set_xticklabels(class_names, rotation=45, ha='right')
        ax1.legend(loc='upper right')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # è®¾ç½®yè½´èŒƒå›´
        ax1.set_ylim(0, 105)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2, bars3]:
            for bar in bars:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{height:.1f}', ha='center', va='bottom', fontsize=8)
        
        # æ”¯æŒåº¦é¥¼å›¾ï¼ˆå³ï¼‰
        ax2 = axes[1]
        colors = plt.cm.Set3(np.linspace(0, 1, len(class_names)))
        wedges, texts, autotexts = ax2.pie(support, labels=class_names, colors=colors,
                                          autopct='%1.1f%%', startangle=90,
                                          textprops={'fontsize': 10})
        
        ax2.set_title('å„ç±»åˆ«æ ·æœ¬åˆ†å¸ƒ', fontsize=14, pad=20)
        
        # è°ƒæ•´å›¾ä¾‹
        ax2.legend(wedges, class_names, title="æƒ…æ„Ÿç±»åˆ«",
                  loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        perf_path = os.path.join(save_dir, 'class_performance.png')
        plt.savefig(perf_path, dpi=300, bbox_inches='tight')
        plt.savefig(os.path.join(save_dir, 'class_performance.pdf'), bbox_inches='tight')
        plt.close(fig)  # å…³é—­å›¾å½¢
        
        print(f"âœ… ç±»åˆ«æ€§èƒ½å›¾å·²ä¿å­˜: {perf_path}")
        print(f"ğŸ“ ä½¿ç”¨å­—ä½“: AR PL UMing CN")
        
        return perf_path
    
    def plot_summary_chart(self, save_dir='./results', figsize=(10, 6)):
        """ç»˜åˆ¶æ€»ä½“æ€§èƒ½æ€»ç»“å›¾"""
        if not self.results:
            print("âš ï¸  è¯·å…ˆè¿è¡Œevaluate()æ–¹æ³•")
            return
        
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ“Š ç»˜åˆ¶æ€»ä½“æ€§èƒ½æ€»ç»“å›¾...")
        
        # è®¾ç½®å­—ä½“
        plt.rcParams['font.family'] = 'AR PL UMing CN'
        plt.rcParams['axes.unicode_minus'] = False
        
        class_report = self.results['class_report']
        overall_acc = self.results['overall_accuracy'] * 100
        
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=figsize)
        
        # æ•°æ®
        categories = ['ç²¾ç¡®ç‡\n(Precision)', 'å¬å›ç‡\n(Recall)', 'F1åˆ†æ•°\n(F1-Score)']
        macro_avg = [
            class_report['macro avg']['precision'] * 100,
            class_report['macro avg']['recall'] * 100,
            class_report['macro avg']['f1-score'] * 100
        ]
        weighted_avg = [
            class_report['weighted avg']['precision'] * 100,
            class_report['weighted avg']['recall'] * 100,
            class_report['weighted avg']['f1-score'] * 100
        ]
        
        x = np.arange(len(categories))
        width = 0.35
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        bars1 = ax.bar(x - width/2, macro_avg, width, label='å®å¹³å‡', color='#2E86AB', alpha=0.8)
        bars2 = ax.bar(x + width/2, weighted_avg, width, label='åŠ æƒå¹³å‡', color='#A23B72', alpha=0.8)
        
        # æ·»åŠ æ€»ä½“å‡†ç¡®ç‡çº¿
        ax.axhline(y=overall_acc, color='#F18F01', linestyle='--', linewidth=2, 
                  label=f'æ€»ä½“å‡†ç¡®ç‡ ({overall_acc:.2f}%)')
        
        ax.set_xlabel('æ€§èƒ½æŒ‡æ ‡', fontsize=12)
        ax.set_ylabel('ç™¾åˆ†æ¯” (%)', fontsize=12)
        ax.set_title('æ¨¡å‹æ€»ä½“æ€§èƒ½æ€»ç»“', fontsize=14, pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 100)
        ax.legend(loc='lower right')
        ax.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{height:.1f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        summary_path = os.path.join(save_dir, 'performance_summary.png')
        plt.savefig(summary_path, dpi=300, bbox_inches='tight')
        plt.close(fig)
        
        print(f"âœ… æ€»ä½“æ€§èƒ½æ€»ç»“å›¾å·²ä¿å­˜: {summary_path}")
        return summary_path
    
    def generate_latex_table(self, save_dir='./results'):
        """ç”ŸæˆLaTeXè¡¨æ ¼"""
        if not self.results:
            print("âš ï¸  è¯·å…ˆè¿è¡Œevaluate()æ–¹æ³•")
            return
        
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ“‹ ç”ŸæˆLaTeXè¡¨æ ¼...")
        
        class_report = self.results['class_report']
        class_names = self.results['class_names']
        overall_acc = self.results['overall_accuracy'] * 100
        total_samples = self.results['total_samples']
        
        # åˆ›å»ºLaTeXè¡¨æ ¼ - ä¿®å¤æ ¼å¼åŒ–é—®é¢˜
        latex_table = f"""\\begin{{table}}[htbp]
\\centering
\\caption{{åœ¨æµ‹è¯•é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½ (æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.2f}\\%)}}
\\label{{tab:test_results}}
\\begin{{tabular}}{{lcccc}}
\\toprule
\\textbf{{ç±»åˆ«}} & \\textbf{{ç²¾ç¡®ç‡}} & \\textbf{{å¬å›ç‡}} & \\textbf{{F1åˆ†æ•°}} & \\textbf{{æ”¯æŒåº¦}} \\\\
\\midrule
"""
        
        # æ·»åŠ æ¯ä¸ªç±»åˆ«çš„æ•°æ®
        for cls in class_names:
            p = class_report[cls]['precision'] * 100
            r = class_report[cls]['recall'] * 100
            f = class_report[cls]['f1-score'] * 100
            s = class_report[cls]['support']
            
            latex_table += f"{cls} & {p:.2f}\\% & {r:.2f}\\% & {f:.2f}\\% & {s} \\\\\n"
        
        # æ·»åŠ å¹³å‡å€¼
        macro_precision = class_report['macro avg']['precision'] * 100
        macro_recall = class_report['macro avg']['recall'] * 100
        macro_f1 = class_report['macro avg']['f1-score'] * 100
        macro_support = class_report['macro avg']['support']
        
        weighted_precision = class_report['weighted avg']['precision'] * 100
        weighted_recall = class_report['weighted avg']['recall'] * 100
        weighted_f1 = class_report['weighted avg']['f1-score'] * 100
        weighted_support = class_report['weighted avg']['support']
        
        latex_table += "\\midrule\n"
        latex_table += f"å®å¹³å‡ & {macro_precision:.2f}\\% & {macro_recall:.2f}\\% & {macro_f1:.2f}\\% & {macro_support} \\\\\n"
        latex_table += f"åŠ æƒå¹³å‡ & {weighted_precision:.2f}\\% & {weighted_recall:.2f}\\% & {weighted_f1:.2f}\\% & {weighted_support} \\\\\n"
        
        latex_table += """\\bottomrule
\\end{tabular}
\\end{table}"""
        
        # ä¿å­˜LaTeXè¡¨æ ¼
        latex_path = os.path.join(save_dir, 'results_latex.tex')
        with open(latex_path, 'w', encoding='utf-8') as f:
            f.write(latex_table)
        
        print(f"âœ… LaTeXè¡¨æ ¼å·²ä¿å­˜: {latex_path}")
        
        # ç®€å•ç‰ˆæœ¬
        simple_latex = f"""\\begin{{tabular}}{{lc}}
\\hline
\\textbf{{æŒ‡æ ‡}} & \\textbf{{æ•°å€¼}} \\\\ \\hline
æ€»ä½“å‡†ç¡®ç‡ & {overall_acc:.2f}\\% \\\\
æ€»æ ·æœ¬æ•° & {total_samples:,} \\\\
å®å¹³å‡F1 & {macro_f1:.2f}\\% \\\\
åŠ æƒå¹³å‡F1 & {weighted_f1:.2f}\\% \\\\ \\hline
\\end{{tabular}}"""
        
        simple_path = os.path.join(save_dir, 'simple_results.tex')
        with open(simple_path, 'w', encoding='utf-8') as f:
            f.write(simple_latex)
        
        print(f"âœ… ç®€åŒ–ç‰ˆLaTeXå·²ä¿å­˜: {simple_path}")
        
        # åŒæ—¶ç”Ÿæˆä¸€ä¸ªçº¯æ–‡æœ¬çš„Markdownè¡¨æ ¼ï¼Œæ–¹ä¾¿æŸ¥çœ‹
        markdown_table = f"""# æ¨¡å‹æ€§èƒ½æŠ¥å‘Š

## æ€»ä½“æ€§èƒ½
- **æ€»ä½“å‡†ç¡®ç‡**: {overall_acc:.4f}%
- **æ€»æ ·æœ¬æ•°**: {total_samples:,}
- **æµ‹è¯•æ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

## è¯¦ç»†æ€§èƒ½æŒ‡æ ‡

| ç±»åˆ« | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1åˆ†æ•° | æ”¯æŒåº¦ |
|------|--------|--------|--------|--------|
"""
        
        for cls in class_names:
            p = class_report[cls]['precision'] * 100
            r = class_report[cls]['recall'] * 100
            f = class_report[cls]['f1-score'] * 100
            s = class_report[cls]['support']
            markdown_table += f"| {cls} | {p:.2f}% | {r:.2f}% | {f:.2f}% | {s} |\n"
        
        markdown_table += f"""| **å®å¹³å‡** | {macro_precision:.2f}% | {macro_recall:.2f}% | {macro_f1:.2f}% | {macro_support} |
| **åŠ æƒå¹³å‡** | {weighted_precision:.2f}% | {weighted_recall:.2f}% | {weighted_f1:.2f}% | {weighted_support} |

## æ€§èƒ½æ€»ç»“
- æœ€ä½³è¡¨ç°ç±»åˆ«: **happy** (ç²¾ç¡®ç‡: {class_report['happy']['precision']*100:.2f}%, å¬å›ç‡: {class_report['happy']['recall']*100:.2f}%)
- æœ€å·®è¡¨ç°ç±»åˆ«: **fear** (ç²¾ç¡®ç‡: {class_report['fear']['precision']*100:.2f}%, å¬å›ç‡: {class_report['fear']['recall']*100:.2f}%)
- ç±»åˆ«ä¸å¹³è¡¡: disgustç±»åˆ«æ ·æœ¬æœ€å°‘({class_report['disgust']['support']})ï¼Œä½†è¡¨ç°ä¸é”™(ç²¾ç¡®ç‡: {class_report['disgust']['precision']*100:.2f}%)
"""
        
        markdown_path = os.path.join(save_dir, 'performance_report.md')
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_table)
        
        print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {markdown_path}")
        
        return latex_table

def run_full_evaluation(test_dir='./data/test', model_path='./best_model_3090.pth'):
    """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
    print("=" * 70)
    print("ğŸ¯ Vision Transformer åœ¨æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°")
    print("=" * 70)
    
    evaluator = PrivateTestEvaluator()
    
    try:
        # 1. è¯„ä¼°æ¨¡å‹
        print("\n1ï¸âƒ£ æ¨¡å‹è¯„ä¼°...")
        results = evaluator.evaluate(test_dir=test_dir, model_path=model_path)
        
        # 2. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        print("\n2ï¸âƒ£ ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š...")
        evaluator.generate_report('./results')
        
        # 3. ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨
        print("\n3ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        try:
            cm_path = evaluator.plot_confusion_matrix('./results')
            print(f"  æ··æ·†çŸ©é˜µ: {cm_path}")
        except Exception as e:
            print(f"  âš ï¸  æ··æ·†çŸ©é˜µç”Ÿæˆå¤±è´¥: {e}")
        
        try:
            perf_path = evaluator.plot_class_performance('./results')
            print(f"  ç±»åˆ«æ€§èƒ½å›¾: {perf_path}")
        except Exception as e:
            print(f"  âš ï¸  ç±»åˆ«æ€§èƒ½å›¾ç”Ÿæˆå¤±è´¥: {e}")
        
        try:
            summary_path = evaluator.plot_summary_chart('./results')
            print(f"  æ€§èƒ½æ€»ç»“å›¾: {summary_path}")
        except Exception as e:
            print(f"  âš ï¸  æ€§èƒ½æ€»ç»“å›¾ç”Ÿæˆå¤±è´¥: {e}")
        
        # 4. ç”ŸæˆLaTeXè¡¨æ ¼
        print("\n4ï¸âƒ£ ç”ŸæˆLaTeXè¡¨æ ¼...")
        try:
            evaluator.generate_latex_table('./results')
        except Exception as e:
            print(f"  âš ï¸  LaTeXè¡¨æ ¼ç”Ÿæˆå¤±è´¥: {e}")
            print("  â„¹ï¸  æ­£åœ¨ç”Ÿæˆç®€åŒ–ç‰ˆæœ¬...")
            # ç”Ÿæˆä¸€ä¸ªç®€å•çš„æ–‡æœ¬ç‰ˆæœ¬
            simple_report = f"""
æ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']*100:.4f}%
æ€»æ ·æœ¬æ•°: {results['total_samples']:,}
å®å¹³å‡F1: {results['class_report']['macro avg']['f1-score']*100:.2f}%
åŠ æƒå¹³å‡F1: {results['class_report']['weighted avg']['f1-score']*100:.2f}%
            """
            with open('./results/simple_report.txt', 'w') as f:
                f.write(simple_report)
        
        # 5. æœ€ç»ˆæ€»ç»“
        print("\n" + "=" * 70)
        print("âœ… è¯„ä¼°å®Œæˆï¼")
        print("=" * 70)
        print(f"ğŸ“Š æ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']*100:.4f}%")
        print(f"ğŸ“ˆ æ€»æ ·æœ¬æ•°: {results['total_samples']:,}")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: ./results/")
        
        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶
        if os.path.exists('./results'):
            print("\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶:")
            files = os.listdir('./results')
            if files:
                for file in sorted(files):
                    if file.endswith(('.png', '.pdf', '.json', '.csv', '.tex', '.md', '.txt')):
                        full_path = f'./results/{file}'
                        size = os.path.getsize(full_path) / 1024
                        print(f"  â€¢ {file:25s} ({size:.1f} KB)")
            else:
                print("  (ç©ºç›®å½•)")
        
        print("=" * 70)
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def quick_test(test_dir='./data/test', model_path='./best_model_3090.pth'):
    """å¿«é€Ÿæµ‹è¯•"""
    print("ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print("=" * 70)
    
    evaluator = PrivateTestEvaluator()
    
    try:
        results = evaluator.evaluate(test_dir=test_dir, model_path=model_path)
        
        print(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']*100:.4f}%")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {results['total_samples']:,}")
        
        # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        print("\nğŸ“‹ å„ç±»åˆ«å‡†ç¡®ç‡:")
        class_report = results['class_report']
        for cls in results['class_names']:
            acc = class_report[cls]['recall'] * 100
            print(f"  {cls:10s}: {acc:6.2f}%")
        
        print(f"\nğŸ“ˆ å®å¹³å‡F1: {class_report['macro avg']['f1-score']*100:.2f}%")
        print(f"ğŸ“ˆ åŠ æƒå¹³å‡F1: {class_report['weighted avg']['f1-score']*100:.2f}%")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹')
    parser.add_argument('--mode', type=str, default='full',
                       choices=['full', 'quick', 'font'],
                       help='è¯„ä¼°æ¨¡å¼: full(å®Œæ•´è¯„ä¼°), quick(å¿«é€Ÿæµ‹è¯•), font(å­—ä½“æµ‹è¯•)')
    parser.add_argument('--test_dir', type=str, default='./data/test',
                       help='æµ‹è¯•é›†è·¯å¾„')
    parser.add_argument('--model_path', type=str, default='./best_model_3090.pth',
                       help='æ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.mode == 'font':
        # å­—ä½“æµ‹è¯•
        print("ğŸ” å­—ä½“æµ‹è¯•æ¨¡å¼")
        print(f"å½“å‰å­—ä½“: {matplotlib.rcParams.get('font.family', 'æœªçŸ¥')}")
        
        # åˆ›å»ºæµ‹è¯•å›¾
        plt.figure(figsize=(8, 4))
        plt.text(0.5, 0.7, 'ä¸­æ–‡æµ‹è¯•: AR PL UMing CN', fontsize=16, ha='center', va='center')
        plt.text(0.5, 0.5, 'é¢„æµ‹æ ‡ç­¾ - çœŸå®æ ‡ç­¾', fontsize=12, ha='center', va='center')
        plt.text(0.5, 0.3, 'ç²¾ç¡®ç‡: 85.6% å¬å›ç‡: 92.1%', fontsize=12, ha='center', va='center')
        plt.title('å­—ä½“æµ‹è¯• - AR PL UMing CN', fontsize=14)
        plt.axis('off')
        plt.savefig('final_font_test.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… å­—ä½“æµ‹è¯•å›¾å·²ä¿å­˜: final_font_test.png")
        
    elif args.mode == 'quick':
        quick_test(args.test_dir, args.model_path)
    else:
        run_full_evaluation(args.test_dir, args.model_path)